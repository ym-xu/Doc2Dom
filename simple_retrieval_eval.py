#!/usr/bin/env python3
"""
ç®€åŒ–æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ - è¯„ä¼°å‰10ä¸ªèŠ‚ç‚¹ä¸­æ˜¯å¦åŒ…å«è¯æ®é¡µé¢
ä½¿ç”¨CLIPæ¨¡å‹ï¼Œæ”¯æŒGPUåŠ é€Ÿï¼Œæµ‹è¯•æ‰€æœ‰æ–‡æ¡£æ‰€æœ‰é—®é¢˜ï¼Œä¿å­˜ç»“æœ
"""

import json
import os
import sys
import numpy as np
import time
from typing import Dict, List, Any
from collections import defaultdict
import ast
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

class SimpleRetrievalEvaluator:
    """ç®€åŒ–æ£€ç´¢è¯„ä¼°å™¨"""
    
    def __init__(self, samples_path: str = "samples.json", 
                 data_dir: str = "/data/users/yiming/dox2dom/dom/MMLongBench-Doc_qwen2vl-25-512"):
        self.samples_path = samples_path
        self.data_dir = data_dir
        self.samples = []
        self.embedder = None
        
        # åŠ è½½æ ·æœ¬
        self._load_samples()
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'overall_stats': {},
            'per_document_stats': {},
            'per_question_results': [],
            'evaluation_time': 0
        }
    
    def _load_samples(self):
        """åŠ è½½æ ·æœ¬æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ ·æœ¬æ•°æ®: {self.samples_path}")
        with open(self.samples_path, 'r', encoding='utf-8') as f:
            self.samples = json.load(f)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def initialize_embedder(self):
        """åˆå§‹åŒ–CLIPåµŒå…¥å™¨"""
        print("ğŸš€ åˆå§‹åŒ–CLIPåµŒå…¥å™¨...")
        from subtree_embedding import SubtreeEmbedder
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä½¿ç”¨CLIPæ¨¡å‹
        self.embedder = SubtreeEmbedder(
            model_name="openai/clip-vit-large-patch14",  # ä½¿ç”¨CLIP
            aggregator="attention",
            embedding_dim=768,  # CLIPçš„ç»´åº¦
            data_dir=self.data_dir,
            device=device  # ä¼ é€’è®¾å¤‡ä¿¡æ¯
        )
        print(f"âœ… åµŒå…¥å™¨åˆå§‹åŒ–å®Œæˆ: {self.embedder.model_name}")
    
    def _load_document(self, doc_id: str) -> Dict:
        """åŠ è½½æ–‡æ¡£DOM JSON"""
        doc_name = doc_id.replace('.pdf', '.json')
        doc_path = os.path.join(self.data_dir, doc_name)
        
        if not os.path.exists(doc_path):
            return None
        
        try:
            with open(doc_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {doc_name}: {e}")
            return None
    
    def _collect_document_nodes(self, root_node) -> List[Dict]:
        """æ”¶é›†æ–‡æ¡£ä¸­çš„æ‰€æœ‰æœ‰æ•ˆèŠ‚ç‚¹"""
        nodes = []
        
        def dfs_collect(node):
            if isinstance(node, dict):
                metadata = node.get('metadata', {})
                if (metadata.get('global_id') and 
                    metadata.get('page_number') is not None):
                    nodes.append(node)
                
                # é€’å½’å¤„ç†å­èŠ‚ç‚¹
                for child in node.get('children', []):
                    dfs_collect(child)
        
        dfs_collect(root_node)
        return nodes
    
    def _embed_nodes_batch(self, nodes: List[Dict], batch_size: int = 50) -> Dict[str, Dict]:
        """æ‰¹é‡ç¼–ç èŠ‚ç‚¹"""
        node_embeddings = {}
        total_nodes = len(nodes)
        
        for i in range(0, total_nodes, batch_size):
            batch = nodes[i:i+batch_size]
            print(f"   ç¼–ç èŠ‚ç‚¹æ‰¹æ¬¡: {i+1}-{min(i+batch_size, total_nodes)}/{total_nodes}")
            
            for node in batch:
                try:
                    metadata = node.get('metadata', {})
                    node_id = metadata.get('global_id')
                    page_number = metadata.get('page_number', 0)
                    element_type = metadata.get('element_type', 'unknown')
                    
                    # ç¼–ç èŠ‚ç‚¹
                    result = self.embedder.encode_subtree(node, max_depth=2)
                    
                    # æå–å®Œæ•´å­æ ‘å†…å®¹ï¼ˆä¸æˆªæ–­ï¼‰
                    full_text_content = self._collect_subtree_text(node)
                    
                    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™100å­—ç¬¦é¢„è§ˆ
                    content_preview = full_text_content[:100] if full_text_content else f"[{element_type}]"
                    
                    node_embeddings[node_id] = {
                        'embedding': result['embedding'],
                        'page_number': page_number,
                        'element_type': element_type,
                        'content_preview': content_preview,
                        'full_text_content': full_text_content,  # å®Œæ•´å­æ ‘æ–‡æœ¬å†…å®¹
                        'original_node': node,  # æ–°å¢ï¼šä¿å­˜åŸå§‹èŠ‚ç‚¹å®Œæ•´ä¿¡æ¯
                        'metadata': metadata
                    }
                    
                except Exception as e:
                    # é™é»˜å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                    continue
        
        return node_embeddings
    
    def _retrieve_top_k(self, query: str, node_embeddings: Dict[str, Dict], k: int = 10) -> List[Dict]:
        """æ£€ç´¢top-kç›¸å…³èŠ‚ç‚¹"""
        if not node_embeddings:
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedder.text_encoder.encode(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for node_id, node_data in node_embeddings.items():
            try:
                node_embedding = node_data['embedding']
                similarity = np.dot(query_embedding, node_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding)
                )
                similarities.append({
                    'node_id': node_id,
                    'score': float(similarity),
                    'page_number': node_data['page_number'],
                    'element_type': node_data['element_type'],
                    'content_preview': node_data['content_preview']
                })
            except Exception:
                continue
        
        # æ’åºå¹¶è¿”å›top-k
        similarities.sort(key=lambda x: x['score'], reverse=True)
        return similarities[:k]
    
    def _extract_nodes_content(self, top_k_results: List[Dict], node_embeddings: Dict[str, Dict]) -> List[Dict]:
        """æå–æ£€ç´¢åˆ°çš„nodesçš„åŸå§‹å†…å®¹ï¼Œä¾›agentsç³»ç»Ÿä½¿ç”¨"""
        nodes_content = []
        
        for result in top_k_results:
            node_id = result['node_id']
            if node_id not in node_embeddings:
                continue
                
            node_data = node_embeddings[node_id]
            metadata = node_data.get('metadata', {})
            original_node = node_data.get('original_node', {})
            
            # æ„å»ºagentså¯ç”¨çš„èŠ‚ç‚¹å†…å®¹
            node_content = {
                'node_id': node_id,
                'rank': len(nodes_content) + 1,
                'similarity_score': result['score'],
                'page_number': result['page_number'],
                'element_type': result['element_type'],
                
                # æ–‡æœ¬å†…å®¹
                'text_content': node_data.get('full_text_content', ''),
                
                # å®Œæ•´å­æ ‘ç»“æ„ä¿¡æ¯
                'subtree_structure': self._extract_subtree_structure(original_node),
                
                # ä½ç½®ä¿¡æ¯
                'bbox': metadata.get('bbox'),
                'dom_path': self._construct_dom_path(metadata),
                
                # ç»“æ„ä¿¡æ¯
                'heading_level': metadata.get('heading_level'),
                'is_chapter_title': metadata.get('is_chapter_title', False),
                'parent_chapter': metadata.get('parent_chapter'),
                'depth': metadata.get('depth', 0),
                
                # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œæå–å›¾ç‰‡ç›¸å…³å†…å®¹ï¼ˆåŒ…æ‹¬é“¾æ¥ï¼‰
                'image_info': self._extract_enhanced_image_content(original_node, metadata) if result['element_type'] == 'figure' else None,
                
                # å¦‚æœæ˜¯è¡¨æ ¼ï¼Œæå–è¡¨æ ¼å†…å®¹ï¼ˆåŒ…æ‹¬å®Œæ•´è¡¨æ ¼æ•°æ®ï¼‰
                'table_info': self._extract_enhanced_table_content(original_node, metadata) if result['element_type'] == 'table' else None,
            }
            
            nodes_content.append(node_content)
        
        return nodes_content
    
    def _collect_subtree_text(self, node: Dict) -> str:
        """é€’å½’æ”¶é›†å­æ ‘çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
        if not isinstance(node, dict):
            return ""
        
        text_parts = []
        
        # æ·»åŠ å½“å‰èŠ‚ç‚¹çš„æ–‡æœ¬
        node_text = node.get('text', '').strip()
        if node_text:
            text_parts.append(node_text)
        
        # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œæ·»åŠ AIæè¿°
        metadata = node.get('metadata', {})
        if metadata.get('element_type') == 'figure':
            ai_desc = metadata.get('ai_description', '')
            if ai_desc and ai_desc not in ['Skipped (disabled or too small)', 'Failed']:
                text_parts.append(f"[å›¾ç‰‡æè¿°: {ai_desc}]")
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        children = node.get('children', [])
        for child in children:
            child_text = self._collect_subtree_text(child)
            if child_text:
                text_parts.append(child_text)
        
        return " ".join(text_parts)
    
    def _construct_dom_path(self, metadata: Dict) -> str:
        """æ„å»ºDOMè·¯å¾„ä¿¡æ¯"""
        page = metadata.get('page_number', '?')
        if page != '?' and page is not None:
            page += 1  # è½¬æ¢ä¸º1-basedç´¢å¼•
        
        element_type = metadata.get('element_type', 'unknown')
        node_id = metadata.get('global_id', 'unknown')
        
        return f"Page {page} > {element_type}[{node_id}]"
    
    def _extract_image_content(self, metadata: Dict) -> Dict:
        """æå–å›¾ç‰‡ç›¸å…³å†…å®¹"""
        return {
            'ai_description': metadata.get('ai_description', ''),
            'description_method': metadata.get('description_method', ''),
            'image_extracted': metadata.get('image_extracted', False),
        }
    
    def _extract_table_content(self, metadata: Dict) -> Dict:
        """æå–è¡¨æ ¼ç›¸å…³å†…å®¹"""
        return {
            'table_image_extracted': metadata.get('table_image_extracted', False),
            'row_count': metadata.get('row_count'),
            'col_count': metadata.get('col_count'),
        }
    
    def _extract_subtree_structure(self, node: Dict) -> Dict:
        """æå–å­æ ‘ç»“æ„ä¿¡æ¯"""
        if not isinstance(node, dict):
            return {}
        
        def extract_node_info(n):
            if not isinstance(n, dict):
                return None
            
            info = {
                'text': n.get('text', '').strip(),
                'metadata': n.get('metadata', {}),
                'children': []
            }
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            children = n.get('children', [])
            for child in children:
                child_info = extract_node_info(child)
                if child_info:
                    info['children'].append(child_info)
            
            return info
        
        return extract_node_info(node)
    
    def _extract_enhanced_image_content(self, node: Dict, metadata: Dict) -> Dict:
        """æå–å¢å¼ºçš„å›¾ç‰‡å†…å®¹ä¿¡æ¯"""
        image_info = {
            'ai_description': metadata.get('ai_description', ''),
            'description_method': metadata.get('description_method', ''),
            'image_extracted': metadata.get('image_extracted', False),
            'image_links': [],
            'image_path': '',
            'image_src': '',
        }
        
        # æå–å›¾ç‰‡srcè·¯å¾„
        image_src = node.get('src', '')
        if image_src:
            image_info['image_src'] = image_src
            image_info['image_path'] = image_src  # å‘åå…¼å®¹
        
        # ä»å­æ ‘ä¸­æå–å›¾ç‰‡é“¾æ¥å’Œsrcä¿¡æ¯
        def find_image_info(n):
            links = []
            src_paths = []
            
            if isinstance(n, dict):
                # æ£€æŸ¥imgæ ‡ç­¾çš„src
                if n.get('tag') == 'img' and n.get('src'):
                    src_paths.append(n['src'])
                
                # æ£€æŸ¥èŠ‚ç‚¹è‡ªèº«çš„src
                if n.get('src'):
                    src_paths.append(n['src'])
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é“¾æ¥ä¿¡æ¯
                if n.get('metadata', {}).get('href'):
                    links.append({
                        'href': n['metadata']['href'],
                        'text': n.get('text', '').strip()
                    })
                
                # é€’å½’å­èŠ‚ç‚¹
                for child in n.get('children', []):
                    child_links, child_srcs = find_image_info(child)
                    links.extend(child_links)
                    src_paths.extend(child_srcs)
            
            return links, src_paths
        
        links, src_paths = find_image_info(node)
        image_info['image_links'] = links
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥çš„srcï¼Œå°è¯•ä»å­èŠ‚ç‚¹è·å–
        if not image_info['image_src'] and src_paths:
            image_info['image_src'] = src_paths[0]  # å–ç¬¬ä¸€ä¸ª
            image_info['image_path'] = src_paths[0]
            
        # ä¿å­˜æ‰€æœ‰æ‰¾åˆ°çš„srcè·¯å¾„
        image_info['all_image_sources'] = src_paths
        
        return image_info
    
    def _extract_enhanced_table_content(self, node: Dict, metadata: Dict) -> Dict:
        """æå–å¢å¼ºçš„è¡¨æ ¼å†…å®¹ä¿¡æ¯"""
        table_info = {
            'table_image_extracted': metadata.get('table_image_extracted', False),
            'row_count': metadata.get('row_count'),
            'col_count': metadata.get('col_count'),
            'table_data': [],
            'table_text': '',
            'table_src': '',
            'table_links': [],
        }
        
        # æå–è¡¨æ ¼srcè·¯å¾„
        table_src = node.get('src', '')
        if table_src:
            table_info['table_src'] = table_src
        
        # ä»å­æ ‘ä¸­æå–è¡¨æ ¼æ•°æ®å’Œé“¾æ¥
        def extract_table_info(n):
            if not isinstance(n, dict):
                return [], [], []
            
            rows = []
            table_text_parts = []
            src_paths = []
            
            # æ£€æŸ¥srcä¿¡æ¯
            if n.get('src'):
                src_paths.append(n['src'])
            
            # å¦‚æœæ˜¯è¡¨æ ¼è¡Œ
            if n.get('metadata', {}).get('element_type') == 'table-row':
                row_cells = []
                for child in n.get('children', []):
                    if child.get('metadata', {}).get('element_type') == 'table-cell':
                        cell_text = self._collect_subtree_text(child)
                        row_cells.append(cell_text)
                        table_text_parts.append(cell_text)
                if row_cells:
                    rows.append(row_cells)
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            for child in n.get('children', []):
                child_rows, child_text, child_srcs = extract_table_info(child)
                rows.extend(child_rows)
                table_text_parts.extend(child_text)
                src_paths.extend(child_srcs)
            
            return rows, table_text_parts, src_paths
        
        table_data, text_parts, src_paths = extract_table_info(node)
        table_info['table_data'] = table_data
        table_info['table_text'] = ' | '.join(text_parts)
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥çš„srcï¼Œå°è¯•ä»å­èŠ‚ç‚¹è·å–
        if not table_info['table_src'] and src_paths:
            table_info['table_src'] = src_paths[0]
            
        # ä¿å­˜æ‰€æœ‰æ‰¾åˆ°çš„srcè·¯å¾„
        table_info['all_table_sources'] = src_paths
        
        return table_info
    
    def _save_retrieved_nodes_for_agents(self, doc_id: str, question: str, nodes_content: List[Dict]):
        """ä¿å­˜æ£€ç´¢åˆ°çš„nodeså†…å®¹ä¾›agentsç³»ç»Ÿä½¿ç”¨"""
        # åˆ›å»ºä¿å­˜ç›®å½•
        agents_dir = "/data/users/yiming/dox2dom/retrieved_nodes/" + self.data_dir.split("/")[-1]
        os.makedirs(agents_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŸºäºæ–‡æ¡£IDå’Œé—®é¢˜hashï¼‰
        import hashlib
        question_hash = hashlib.md5(question.encode()).hexdigest()[:8]
        filename = f"{doc_id.replace('.pdf', '')}_{question_hash}.json"
        filepath = os.path.join(agents_dir, filename)
        
        # æ„å»ºä¿å­˜çš„æ•°æ®ç»“æ„
        agents_data = {
            'doc_id': doc_id,
            'question': question,
            'retrieval_timestamp': time.time(),
            'total_nodes': len(nodes_content),
            'nodes_content': nodes_content
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(agents_data, f, indent=2, ensure_ascii=False)
        
        print(f"   ğŸ’¾ ä¿å­˜æ£€ç´¢å†…å®¹ä¾›agentsä½¿ç”¨: {filepath}")
        return filepath
    
    def _get_retrieval_metric(self, gt_pages: List[int], pred_pages: List[int]) -> float:
        """è®¡ç®—æ£€ç´¢æŒ‡æ ‡ - MMLongBenché£æ ¼çš„æ£€ç´¢ç²¾åº¦"""
        if not gt_pages:
            return 0.0
        
        # å¯¹äºæ¯ä¸ªground truthé¡µé¢ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨é¢„æµ‹é¡µé¢ä¸­
        retrieval_precision_scores = []
        for gt_page in gt_pages:
            if gt_page in pred_pages:
                retrieval_precision_scores.append(1.0)
            else:
                retrieval_precision_scores.append(0.0)
        
        # è¿”å›å¹³å‡å¾—åˆ†
        return sum(retrieval_precision_scores) / len(retrieval_precision_scores) if retrieval_precision_scores else 0.0
    
    def _get_similarity_score(self, chunk: str, answer: str) -> float:
        """è®¡ç®—chunkå’Œç­”æ¡ˆä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        chunk_lower = chunk.lower()
        answer_lower = answer.lower()
        
        # ç²¾ç¡®åŒ¹é…
        if answer_lower in chunk_lower:
            return 1.0
        
        # è¯æ±‡é‡å åˆ†æ•°
        chunk_words = set(chunk_lower.split())
        answer_words = set(answer_lower.split())
        
        if not answer_words:
            return 0.0
        
        overlap = len(chunk_words & answer_words)
        return overlap / len(answer_words)
    
    def _eval_retrieval(self, gt_answers: List[str], retrieved_chunks: List[str]) -> Dict[str, float]:
        """ä½¿ç”¨chunkåˆ†æ•°è¯„ä¼°æ£€ç´¢ç»“æœ"""
        if not retrieved_chunks:
            return {"chunk_score": 0.0}
        
        scores = []
        for ans in gt_answers:
            ans_scores = [self._get_similarity_score(chunk, ans) for chunk in retrieved_chunks]
            best_score = max(ans_scores + [0])
            scores.append(np.log(best_score + 1) / np.log(2))
        
        return {
            "chunk_score": max(scores) if scores else 0.0
        }
    
    def _calculate_chunk_score(self, question_answer: str, top_10_results: List[Dict], node_embeddings: Dict[str, Dict]) -> float:
        """è®¡ç®—chunkåˆ†æ•° - åŸºäºæ–‡æœ¬åŒ¹é…çš„MMLongBenché£æ ¼è¯„ä¼°"""
        if not top_10_results or not question_answer or question_answer == "Not answerable":
            return 0.0
        
        # æå–æ£€ç´¢åˆ°çš„chunksçš„æ–‡æœ¬å†…å®¹
        retrieved_chunks = []
        for result in top_10_results:
            node_id = result['node_id']
            if node_id in node_embeddings:
                node_data = node_embeddings[node_id]
                chunk_text = node_data.get('full_text_content', '')
                if chunk_text.strip():
                    retrieved_chunks.append(chunk_text)
        
        # ä½¿ç”¨ground truthç­”æ¡ˆè¯„ä¼°
        gt_answers = [question_answer]
        chunk_metrics = self._eval_retrieval(gt_answers, retrieved_chunks)
        
        return chunk_metrics["chunk_score"]
    
    def _evaluate_question(self, sample: Dict, node_embeddings: Dict[str, Dict]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        # è§£æè¯æ®é¡µé¢
        try:
            evidence_pages = ast.literal_eval(sample['evidence_pages'])
            if not isinstance(evidence_pages, list):
                evidence_pages = [evidence_pages]
        except:
            evidence_pages = []
        
        # è§£æè¯æ®æ¥æº
        try:
            evidence_sources = ast.literal_eval(sample.get('evidence_sources', '[]'))
            if not isinstance(evidence_sources, list):
                evidence_sources = [evidence_sources]
        except:
            evidence_sources = []
        
        # æ‰§è¡Œæ£€ç´¢ï¼ˆå³ä½¿æ²¡æœ‰è¯æ®é¡µé¢ï¼Œä¹Ÿè¦æ£€ç´¢ä¾›é—®ç­”ç³»ç»Ÿä½¿ç”¨ï¼‰
        query = sample['question']
        top_10_results = self._retrieve_top_k(query, node_embeddings, k=10)
        
        # æ£€æŸ¥å‰10ä¸ªèŠ‚ç‚¹ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰è¯æ®é¡µé¢
        retrieved_pages = set()
        retrieved_sources = set()
        for result in top_10_results:
            page_num = result['page_number']
            # æ·»åŠ 0-basedå’Œ1-basedé¡µé¢ï¼ˆå…¼å®¹æ€§æ£€æŸ¥ï¼‰
            retrieved_pages.add(page_num + 1)
            
            # æ ¹æ®element_typeåˆ†ç±»source
            element_type = result['element_type']
            if element_type == 'figure':
                retrieved_sources.add('Figure')
            elif element_type == 'table':
                retrieved_sources.add('Table')
            elif element_type in ['paragraph', 'text_block']:
                retrieved_sources.add('Pure-text (Plain-text)')
            elif element_type in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'heading']:
                retrieved_sources.add('Generalized-text (Layout)')
            else:
                retrieved_sources.add('Pure-text (Plain-text)')
        
        # è®¡ç®—é¡µé¢è¦†ç›–æƒ…å†µ
        evidence_pages_set = set(evidence_pages)
        covered_pages = evidence_pages_set.intersection(retrieved_pages)
        
        # è®¡ç®—é¡µé¢çº§åˆ«çš„ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°
        if evidence_pages_set and retrieved_pages:
            page_precision = len(covered_pages) / len(retrieved_pages)
            page_recall = len(covered_pages) / len(evidence_pages_set)
            page_f1 = 2 * page_precision * page_recall / (page_precision + page_recall) if (page_precision + page_recall) > 0 else 0.0
        else:
            page_precision = page_recall = page_f1 = 0.0
        
        # è®¡ç®—æºç±»å‹è¦†ç›–æƒ…å†µ
        evidence_sources_set = set(evidence_sources)
        covered_sources = evidence_sources_set.intersection(retrieved_sources)
        
        # å¦‚æœæ²¡æœ‰è¯æ®é¡µé¢ï¼Œæ ‡è®°ä¸ºä¸å‚ä¸ç»Ÿè®¡ï¼Œä½†ä»ç„¶æä¾›æ£€ç´¢ç»“æœ
        has_evidence = len(evidence_pages_set) > 0
        coverage_ratio = len(covered_pages) / len(evidence_pages_set) if has_evidence else 0.0
        all_pages_covered = coverage_ratio == 1.0 if has_evidence else False
        
        # è®¡ç®—æºç±»å‹æŒ‡æ ‡
        if evidence_sources_set:
            source_precision = len(covered_sources) / len(retrieved_sources) if retrieved_sources else 0
            source_recall = len(covered_sources) / len(evidence_sources_set)
            source_f1 = 2 * source_precision * source_recall / (source_precision + source_recall) if (source_precision + source_recall) > 0 else 0
        else:
            source_precision = source_recall = source_f1 = 0
        
        # è®¡ç®—Hit@1, Hit@3, Hit@5 (åªæœ‰æœ‰è¯æ®é¡µé¢æ—¶æ‰è®¡ç®—)
        hit_at_1 = False
        hit_at_3 = False  
        hit_at_5 = False
        
        if has_evidence and top_10_results:
            # æ£€æŸ¥å‰1ä¸ª
            page_1 = top_10_results[0]['page_number']
            hit_at_1 = page_1 in evidence_pages or (page_1 + 1) in evidence_pages
            
            # æ£€æŸ¥å‰3ä¸ª
            pages_3 = [r['page_number'] for r in top_10_results[:3]]
            hit_at_3 = any(p in evidence_pages or (p + 1) in evidence_pages for p in pages_3)
            
            # æ£€æŸ¥å‰5ä¸ª
            pages_5 = [r['page_number'] for r in top_10_results[:5]]
            hit_at_5 = any(p in evidence_pages or (p + 1) in evidence_pages for p in pages_5)
        
        # è®¡ç®—æ–°å¢æŒ‡æ ‡
        # 1. Page hit rate: æ˜¯å¦æœ‰ä»»ä½•è¯æ®é¡µé¢è¢«æ£€ç´¢åˆ°
        page_hit_rate = 1.0 if (evidence_pages_set & retrieved_pages) else 0.0 if has_evidence else 0.0
        
        # 2. Retrieval precision (page-based)
        gt_pages = list(evidence_pages_set)
        pred_pages = list(retrieved_pages)
        retrieval_precision = self._get_retrieval_metric(gt_pages, pred_pages) if gt_pages and pred_pages else 0
        
        # 3. Chunk score
        chunk_score = self._calculate_chunk_score(sample['answer'], top_10_results, node_embeddings)
        
        # 4. Top score (æœ€é«˜ç›¸ä¼¼åº¦åˆ†æ•°)
        top_score = top_10_results[0]['score'] if top_10_results else 0
        
        # æå–å¹¶ä¿å­˜æ£€ç´¢åˆ°çš„nodeså†…å®¹ä¾›agentsä½¿ç”¨ï¼ˆåŒ…å«å®Œæ•´å­æ ‘å†…å®¹ï¼‰
        nodes_content = self._extract_nodes_content(top_10_results, node_embeddings)
        agents_file = self._save_retrieved_nodes_for_agents(sample['doc_id'], sample['question'], nodes_content)
        
        return {
            'doc_id': sample['doc_id'],
            'question': sample['question'],
            'answer': sample['answer'],
            'evidence_pages': evidence_pages,
            'evidence_sources': evidence_sources,
            'retrieved_pages': list(retrieved_pages),
            'retrieved_sources': list(retrieved_sources),
            'coverage_ratio': coverage_ratio,
            'all_pages_covered': all_pages_covered,
            'hit_at_1': hit_at_1,
            'hit_at_3': hit_at_3,
            'hit_at_5': hit_at_5,
            # é¡µé¢çº§åˆ«æŒ‡æ ‡
            'page_precision': page_precision,
            'page_recall': page_recall,
            'page_f1': page_f1,
            # æºç±»å‹æŒ‡æ ‡
            'source_precision': source_precision,
            'source_recall': source_recall,
            'source_f1': source_f1,
            # å…¶ä»–æŒ‡æ ‡
            'page_hit_rate': page_hit_rate,
            'retrieval_precision': retrieval_precision,
            'chunk_score': chunk_score,
            'top_score': top_score,
            'top_10_results': top_10_results,
            'agents_file': agents_file,  # æ–°å¢ï¼šä¿å­˜çš„agentsæ–‡ä»¶è·¯å¾„
            'has_evidence': has_evidence  # æ–°å¢ï¼šæ˜¯å¦æœ‰è¯æ®é¡µé¢ï¼ˆç”¨äºç»Ÿè®¡ç­›é€‰ï¼‰
        }
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼šæ‰€æœ‰æ–‡æ¡£ï¼Œæ‰€æœ‰é—®é¢˜"""
        if not self.embedder:
            print("âŒ è¯·å…ˆåˆå§‹åŒ–åµŒå…¥å™¨")
            return
        
        start_time = time.time()
        print(f"\nğŸ¯ å¼€å§‹å®Œæ•´æ£€ç´¢è¯„ä¼°")
        print(f"æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        
        # æŒ‰æ–‡æ¡£åˆ†ç»„æ ·æœ¬
        samples_by_doc = defaultdict(list)
        for sample in self.samples:
            samples_by_doc[sample['doc_id']].append(sample)
        
        print(f"æ–‡æ¡£æ•°: {len(samples_by_doc)}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_questions = 0
        total_coverage = 0
        total_all_covered = 0
        total_hit_1 = 0
        total_hit_3 = 0
        total_hit_5 = 0
        total_skipped = 0  # è·³è¿‡çš„æ— è¯æ®é¡µé¢æ ·æœ¬æ•°
        
        # æ–°å¢æŒ‡æ ‡çš„æ€»è®¡
        total_page_precision = 0
        total_page_recall = 0
        total_page_f1 = 0
        total_source_precision = 0
        total_source_recall = 0
        total_source_f1 = 0
        total_page_hit_rate = 0
        total_retrieval_precision = 0
        total_chunk_score = 0
        total_top_score = 0
        
        # é€æ–‡æ¡£å¤„ç†
        for doc_idx, (doc_id, doc_samples) in enumerate(samples_by_doc.items()):
            print(f"\nğŸ“– [{doc_idx+1}/{len(samples_by_doc)}] å¤„ç†æ–‡æ¡£: {doc_id}")
            print(f"   é—®é¢˜æ•°: {len(doc_samples)}")
            
            # åŠ è½½æ–‡æ¡£
            document = self._load_document(doc_id)
            if not document:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆåŠ è½½å¤±è´¥ï¼‰")
                continue
            
            # æ”¶é›†èŠ‚ç‚¹
            nodes = self._collect_document_nodes(document)
            if not nodes:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆæ— æœ‰æ•ˆèŠ‚ç‚¹ï¼‰")
                continue
            
            print(f"   èŠ‚ç‚¹æ•°: {len(nodes)}")
            
            # ç¼–ç èŠ‚ç‚¹
            try:
                node_embeddings = self._embed_nodes_batch(nodes)
                if not node_embeddings:
                    print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆç¼–ç å¤±è´¥ï¼‰")
                    continue
                print(f"   âœ… æˆåŠŸç¼–ç : {len(node_embeddings)} ä¸ªèŠ‚ç‚¹")
            except Exception as e:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆç¼–ç å¼‚å¸¸ï¼‰: {e}")
                continue
            
            # è¯„ä¼°è¯¥æ–‡æ¡£çš„æ‰€æœ‰é—®é¢˜
            doc_results = []
            doc_coverage = 0
            doc_all_covered = 0
            doc_hit_1 = 0
            doc_hit_3 = 0
            doc_hit_5 = 0
            
            # æ–‡æ¡£çº§åˆ«çš„æ–°å¢æŒ‡æ ‡
            doc_page_precision = 0
            doc_page_recall = 0
            doc_page_f1 = 0
            doc_source_precision = 0
            doc_source_recall = 0
            doc_source_f1 = 0
            doc_page_hit_rate = 0
            doc_retrieval_precision = 0
            doc_chunk_score = 0
            doc_top_score = 0
            
            for sample_idx, sample in enumerate(doc_samples):
                if sample_idx % 10 == 0:
                    print(f"   è¯„ä¼°è¿›åº¦: {sample_idx+1}/{len(doc_samples)}")
                
                try:
                    result = self._evaluate_question(sample, node_embeddings)
                    
                    if result is None:
                        continue
                    
                    doc_results.append(result)
                    self.results['per_question_results'].append(result)
                    
                    # åªæœ‰æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬æ‰å‚ä¸ç»Ÿè®¡è®¡ç®—
                    if result['has_evidence']:
                        doc_coverage += result['coverage_ratio']
                        if result['all_pages_covered']:
                            doc_all_covered += 1
                        if result['hit_at_1']:
                            doc_hit_1 += 1
                        if result['hit_at_3']:
                            doc_hit_3 += 1
                        if result['hit_at_5']:
                            doc_hit_5 += 1
                            
                        # ç´¯è®¡æ–°å¢æŒ‡æ ‡
                        doc_page_precision += result['page_precision']
                        doc_page_recall += result['page_recall']
                        doc_page_f1 += result['page_f1']
                        doc_source_precision += result['source_precision']
                        doc_source_recall += result['source_recall']
                        doc_source_f1 += result['source_f1']
                        doc_page_hit_rate += result['page_hit_rate']
                        doc_retrieval_precision += result['retrieval_precision']
                        doc_chunk_score += result['chunk_score']
                        doc_top_score += result['top_score']
                    else:
                        total_skipped += 1
                        print(f"   âš ï¸  è·³è¿‡ç»Ÿè®¡ï¼ˆæ— è¯æ®é¡µé¢ï¼‰: {sample['question'][:50]}...")
                    
                except Exception as e:
                    print(f"   âŒ é—®é¢˜è¯„ä¼°å¤±è´¥: {e}")
                    continue
            
            # æ–‡æ¡£çº§åˆ«ç»Ÿè®¡
            if doc_results:
                # è®¡ç®—æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬æ•°é‡
                doc_valid_count = sum(1 for r in doc_results if r['has_evidence'])
                
                doc_stats = {
                    'questions_count': len(doc_results),
                    'valid_questions_count': doc_valid_count,
                    'skipped_questions_count': len(doc_results) - doc_valid_count,
                    'avg_coverage': doc_coverage / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'all_pages_covered_rate': doc_all_covered / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'hit_at_1_rate': doc_hit_1 / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'hit_at_3_rate': doc_hit_3 / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'hit_at_5_rate': doc_hit_5 / doc_valid_count if doc_valid_count > 0 else 0.0,
                    # é¡µé¢çº§åˆ«æŒ‡æ ‡
                    'avg_page_precision': doc_page_precision / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'avg_page_recall': doc_page_recall / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'avg_page_f1': doc_page_f1 / doc_valid_count if doc_valid_count > 0 else 0.0,
                    # æºç±»å‹æŒ‡æ ‡
                    'avg_source_precision': doc_source_precision / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'avg_source_recall': doc_source_recall / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'avg_source_f1': doc_source_f1 / doc_valid_count if doc_valid_count > 0 else 0.0,
                    # å…¶ä»–æŒ‡æ ‡
                    'avg_page_hit_rate': doc_page_hit_rate / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'avg_retrieval_precision': doc_retrieval_precision / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'avg_chunk_score': doc_chunk_score / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'avg_top_score': doc_top_score / doc_valid_count if doc_valid_count > 0 else 0.0
                }
                self.results['per_document_stats'][doc_id] = doc_stats
                
                # ç´¯è®¡åˆ°æ€»ä½“ç»Ÿè®¡ï¼ˆåªè®¡ç®—æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬ï¼‰
                total_questions += doc_valid_count
                total_coverage += doc_coverage
                total_all_covered += doc_all_covered
                total_hit_1 += doc_hit_1
                total_hit_3 += doc_hit_3
                total_hit_5 += doc_hit_5
                
                # ç´¯è®¡æ–°å¢æŒ‡æ ‡
                total_page_precision += doc_page_precision
                total_page_recall += doc_page_recall
                total_page_f1 += doc_page_f1
                total_source_precision += doc_source_precision
                total_source_recall += doc_source_recall
                total_source_f1 += doc_source_f1
                total_page_hit_rate += doc_page_hit_rate
                total_retrieval_precision += doc_retrieval_precision
                total_chunk_score += doc_chunk_score
                total_top_score += doc_top_score
                
                print(f"   ğŸ“Š æ–‡æ¡£ç»Ÿè®¡: è¦†ç›–ç‡={doc_stats['avg_coverage']:.3f}, "
                      f"å®Œå…¨è¦†ç›–={doc_stats['all_pages_covered_rate']:.3f}, "
                      f"Hit@1={doc_stats['hit_at_1_rate']:.3f}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if total_questions > 0:
            self.results['overall_stats'] = {
                'total_questions': total_questions,
                'total_documents': len(self.results['per_document_stats']),
                'total_skipped': total_skipped,
                'total_samples': len(self.samples),
                'avg_coverage': total_coverage / total_questions,
                'all_pages_covered_rate': total_all_covered / total_questions,
                'hit_at_1_rate': total_hit_1 / total_questions,
                'hit_at_3_rate': total_hit_3 / total_questions,
                'hit_at_5_rate': total_hit_5 / total_questions,
                # é¡µé¢çº§åˆ«æŒ‡æ ‡
                'avg_page_precision': total_page_precision / total_questions,
                'avg_page_recall': total_page_recall / total_questions,
                'avg_page_f1': total_page_f1 / total_questions,
                # æºç±»å‹æŒ‡æ ‡
                'avg_source_precision': total_source_precision / total_questions,
                'avg_source_recall': total_source_recall / total_questions,
                'avg_source_f1': total_source_f1 / total_questions,
                # å…¶ä»–æŒ‡æ ‡
                'avg_page_hit_rate': total_page_hit_rate / total_questions,
                'avg_retrieval_precision': total_retrieval_precision / total_questions,
                'avg_chunk_score': total_chunk_score / total_questions,
                'avg_top_score': total_top_score / total_questions
            }
        
        # è®°å½•è¯„ä¼°æ—¶é—´
        self.results['evaluation_time'] = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        self._display_results()
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        # æ˜¾ç¤ºagentsæ–‡ä»¶ä¿å­˜ä¿¡æ¯
        agents_files_count = sum(1 for r in self.results['per_question_results'] if r.get('agents_file'))
        valid_agents_count = sum(1 for r in self.results['per_question_results'] if r.get('agents_file') and r['has_evidence'])
        no_evidence_agents_count = sum(1 for r in self.results['per_question_results'] if r.get('agents_file') and not r['has_evidence'])
        
        print(f"\nğŸ’¾ å·²ä¸º {agents_files_count} ä¸ªé—®é¢˜ä¿å­˜æ£€ç´¢å†…å®¹åˆ° ./retrieved_nodes_for_agents/ ç›®å½•")
        print(f"   - æœ‰è¯æ®é¡µé¢: {valid_agents_count} ä¸ª")
        print(f"   - æ— è¯æ®é¡µé¢: {no_evidence_agents_count} ä¸ª")
        print("   æ‰€æœ‰æ–‡ä»¶éƒ½å¯ä¾›agentsç³»ç»Ÿä½¿ç”¨ï¼ˆé—®ç­”ç³»ç»Ÿå¯æ®æ­¤åˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”ï¼‰")
    
    def _display_results(self):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
        stats = self.results['overall_stats']
        if not stats:
            print("âŒ æ— è¯„ä¼°ç»“æœ")
            return
        
        print("\n" + "="*80)
        print("ğŸ¯ å®Œæ•´æ£€ç´¢è¯„ä¼°ç»“æœ")
        print("="*80)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"   æœ‰æ•ˆé—®é¢˜æ•°: {stats['total_questions']} (æœ‰è¯æ®é¡µé¢)")
        print(f"   è·³è¿‡æ ·æœ¬æ•°: {stats['total_skipped']} (æ— è¯æ®é¡µé¢ï¼Œä½†å·²æ£€ç´¢)")
        print(f"   å¤„ç†æ–‡æ¡£æ•°: {stats['total_documents']}")
        print(f"   è¯„ä¼°æ—¶é—´: {self.results['evaluation_time']:.1f}ç§’")
        
        print(f"\nğŸ“ˆ æ£€ç´¢æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å¹³å‡é¡µé¢è¦†ç›–ç‡: {stats['avg_coverage']:.3f}")
        print(f"   å®Œå…¨è¦†ç›–ç‡: {stats['all_pages_covered_rate']:.3f}")
        print(f"   Hit@1: {stats['hit_at_1_rate']:.3f}")
        print(f"   Hit@3: {stats['hit_at_3_rate']:.3f}")
        print(f"   Hit@5: {stats['hit_at_5_rate']:.3f}")
        
        print(f"\nğŸ“Š é¡µé¢çº§åˆ«æŒ‡æ ‡:")
        print(f"   é¡µé¢ç²¾ç¡®åº¦: {stats['avg_page_precision']:.3f}")
        print(f"   é¡µé¢å¬å›ç‡: {stats['avg_page_recall']:.3f}")
        print(f"   é¡µé¢F1åˆ†æ•°: {stats['avg_page_f1']:.3f}")
        print(f"   é¡µé¢å‘½ä¸­ç‡: {stats['avg_page_hit_rate']:.3f}")
        
        print(f"\nğŸ“Š æºç±»å‹æŒ‡æ ‡:")
        print(f"   æºç±»å‹ç²¾åº¦: {stats['avg_source_precision']:.3f}")
        print(f"   æºç±»å‹å¬å›: {stats['avg_source_recall']:.3f}")
        print(f"   æºç±»å‹F1: {stats['avg_source_f1']:.3f}")
        
        print(f"\nğŸ“Š å…¶ä»–è¯„ä¼°æŒ‡æ ‡:")
        print(f"   æ£€ç´¢ç²¾åº¦: {stats['avg_retrieval_precision']:.3f}")
        print(f"   å—å¾—åˆ†: {stats['avg_chunk_score']:.3f}")
        print(f"   æœ€é«˜å¾—åˆ†: {stats['avg_top_score']:.3f}")
        
        # æ˜¾ç¤ºä¸€äº›æˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹ï¼ˆåªè€ƒè™‘æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬ï¼‰
        valid_results = [r for r in self.results['per_question_results'] if r['has_evidence']]
        success_cases = [r for r in valid_results if r['all_pages_covered']]
        fail_cases = [r for r in valid_results if not r['all_pages_covered']]
        no_evidence_cases = [r for r in self.results['per_question_results'] if not r['has_evidence']]
        
        if success_cases:
            print(f"\nâœ… æˆåŠŸæ¡ˆä¾‹ ({len(success_cases)}ä¸ª):")
            for case in success_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   è¯æ®é¡µé¢: {case['evidence_pages']}")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print()
        
        if fail_cases:
            print(f"\nâŒ å¤±è´¥æ¡ˆä¾‹ ({len(fail_cases)}ä¸ª):")
            for case in fail_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   è¯æ®é¡µé¢: {case['evidence_pages']}")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print(f"   è¦†ç›–ç‡: {case['coverage_ratio']:.3f}")
                print()
        
        if no_evidence_cases:
            print(f"\nâš ï¸  æ— è¯æ®é¡µé¢æ¡ˆä¾‹ ({len(no_evidence_cases)}ä¸ªï¼Œå·²æ£€ç´¢ä½†æœªå‚ä¸ç»Ÿè®¡):")
            for case in no_evidence_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print(f"   agentsæ–‡ä»¶: {case.get('agents_file', 'N/A')}")
                print()
    
    def _save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_file = "/data/users/yiming/dox2dom/retrieved_nodes/" + self.data_dir.split("/")[-1] + "/domretrieval_evaluation_results.json"
        
        # ä¸ºäº†JSONåºåˆ—åŒ–ï¼Œè½¬æ¢numpyæ•°ç»„ç­‰
        serializable_results = {
            'overall_stats': self.results['overall_stats'],
            'per_document_stats': self.results['per_document_stats'],
            'evaluation_time': self.results['evaluation_time'],
            'model_info': {
                'model_name': self.embedder.model_name if self.embedder else 'unknown',
                'embedding_dim': self.embedder.text_encoder.target_dim if self.embedder else 'unknown'
            },
            'evaluation_config': {
                'top_k': 10,
                'max_depth': 2,
                'data_dir': self.data_dir
            }
        }
        
        # ä¿å­˜ç®€åŒ–ç‰ˆçš„é—®é¢˜ç»“æœï¼ˆå»æ‰embeddingç­‰å¤§å¯¹è±¡ï¼‰
        simplified_question_results = []
        for result in self.results['per_question_results']:
            simplified_result = {
                'doc_id': result['doc_id'],
                'question': result['question'][:200],  # æˆªæ–­é—®é¢˜
                'evidence_pages': result['evidence_pages'],
                'coverage_ratio': result['coverage_ratio'],
                'all_pages_covered': result['all_pages_covered'],
                'hit_at_1': result['hit_at_1'],
                'hit_at_3': result['hit_at_3'],
                'hit_at_5': result['hit_at_5'],
                'top_3_results': [
                    {
                        'page_number': r['page_number'],
                        'score': r['score'],
                        'element_type': r['element_type']
                    }
                    for r in result['top_10_results'][:3]
                ]
            }
            simplified_question_results.append(simplified_result)
        
        serializable_results['per_question_results'] = simplified_question_results
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ç®€åŒ–æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SimpleRetrievalEvaluator()
    
    # åˆå§‹åŒ–åµŒå…¥å™¨
    evaluator.initialize_embedder()
    
    # è¿è¡Œå®Œæ•´è¯„ä¼°
    evaluator.run_full_evaluation()
    
    print("\nğŸ‰ è¯„ä¼°å®Œæˆ!")

if __name__ == "__main__":
    main()