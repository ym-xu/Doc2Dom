#!/usr/bin/env python3
"""
ç®€åŒ–æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ - è¯„ä¼°å‰10ä¸ªèŠ‚ç‚¹ä¸­æ˜¯å¦åŒ…å«è¯æ®é¡µé¢
ä½¿ç”¨CLIPæ¨¡å‹ï¼Œæ”¯æŒGPUåŠ é€Ÿï¼Œæµ‹è¯•æ‰€æœ‰æ–‡æ¡£æ‰€æœ‰é—®é¢˜ï¼Œä¿å­˜ç»“æœ
"""

import json
import os
import sys
import numpy as np
import time
from typing import Dict, List, Any
from collections import defaultdict
import ast
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

class SimpleRetrievalEvaluator:
    """ç®€åŒ–æ£€ç´¢è¯„ä¼°å™¨"""
    
    def __init__(self, samples_path: str = "samples.json", 
                 data_dir: str = "./data/dom/MMLongBench-Doc-Best"):
        self.samples_path = samples_path
        self.data_dir = data_dir
        self.samples = []
        self.embedder = None
        
        # åŠ è½½æ ·æœ¬
        self._load_samples()
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'overall_stats': {},
            'per_document_stats': {},
            'per_question_results': [],
            'evaluation_time': 0
        }
    
    def _load_samples(self):
        """åŠ è½½æ ·æœ¬æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ ·æœ¬æ•°æ®: {self.samples_path}")
        with open(self.samples_path, 'r', encoding='utf-8') as f:
            self.samples = json.load(f)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def initialize_embedder(self):
        """åˆå§‹åŒ–CLIPåµŒå…¥å™¨"""
        print("ğŸš€ åˆå§‹åŒ–CLIPåµŒå…¥å™¨...")
        from subtree_embedding import SubtreeEmbedder
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä½¿ç”¨CLIPæ¨¡å‹
        self.embedder = SubtreeEmbedder(
            model_name="openai/clip-vit-large-patch14",  # ä½¿ç”¨CLIP
            aggregator="attention",
            embedding_dim=768,  # CLIPçš„ç»´åº¦
            data_dir=self.data_dir,
            device=device  # ä¼ é€’è®¾å¤‡ä¿¡æ¯
        )
        print(f"âœ… åµŒå…¥å™¨åˆå§‹åŒ–å®Œæˆ: {self.embedder.model_name}")
    
    def _load_document(self, doc_id: str) -> Dict:
        """åŠ è½½æ–‡æ¡£DOM JSON"""
        doc_name = doc_id.replace('.pdf', '.json')
        doc_path = os.path.join(self.data_dir, doc_name)
        
        if not os.path.exists(doc_path):
            return None
        
        try:
            with open(doc_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {doc_name}: {e}")
            return None
    
    def _collect_document_nodes(self, root_node) -> List[Dict]:
        """æ”¶é›†æ–‡æ¡£ä¸­çš„æ‰€æœ‰æœ‰æ•ˆèŠ‚ç‚¹"""
        nodes = []
        
        def dfs_collect(node):
            if isinstance(node, dict):
                metadata = node.get('metadata', {})
                if (metadata.get('global_id') and 
                    metadata.get('page_number') is not None):
                    nodes.append(node)
                
                # é€’å½’å¤„ç†å­èŠ‚ç‚¹
                for child in node.get('children', []):
                    dfs_collect(child)
        
        dfs_collect(root_node)
        return nodes
    
    def _embed_nodes_batch(self, nodes: List[Dict], batch_size: int = 50) -> Dict[str, Dict]:
        """æ‰¹é‡ç¼–ç èŠ‚ç‚¹"""
        node_embeddings = {}
        total_nodes = len(nodes)
        
        for i in range(0, total_nodes, batch_size):
            batch = nodes[i:i+batch_size]
            print(f"   ç¼–ç èŠ‚ç‚¹æ‰¹æ¬¡: {i+1}-{min(i+batch_size, total_nodes)}/{total_nodes}")
            
            for node in batch:
                try:
                    metadata = node.get('metadata', {})
                    node_id = metadata.get('global_id')
                    page_number = metadata.get('page_number', 0)
                    element_type = metadata.get('element_type', 'unknown')
                    
                    # ç¼–ç èŠ‚ç‚¹
                    result = self.embedder.encode_subtree(node, max_depth=2)
                    
                    # æå–å†…å®¹é¢„è§ˆ
                    text = node.get('text', '').strip()
                    ai_desc = metadata.get('ai_description', '')
                    content_preview = text[:100] if text else f"[{element_type}]"
                    
                    node_embeddings[node_id] = {
                        'embedding': result['embedding'],
                        'page_number': page_number,
                        'element_type': element_type,
                        'content_preview': content_preview,
                        'metadata': metadata
                    }
                    
                except Exception as e:
                    # é™é»˜å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                    continue
        
        return node_embeddings
    
    def _retrieve_top_k(self, query: str, node_embeddings: Dict[str, Dict], k: int = 10) -> List[Dict]:
        """æ£€ç´¢top-kç›¸å…³èŠ‚ç‚¹"""
        if not node_embeddings:
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedder.text_encoder.encode(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for node_id, node_data in node_embeddings.items():
            try:
                node_embedding = node_data['embedding']
                similarity = np.dot(query_embedding, node_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding)
                )
                similarities.append({
                    'node_id': node_id,
                    'score': float(similarity),
                    'page_number': node_data['page_number'],
                    'element_type': node_data['element_type'],
                    'content_preview': node_data['content_preview']
                })
            except Exception:
                continue
        
        # æ’åºå¹¶è¿”å›top-k
        similarities.sort(key=lambda x: x['score'], reverse=True)
        return similarities[:k]
    
    def _extract_nodes_content(self, top_k_results: List[Dict], node_embeddings: Dict[str, Dict], original_nodes: List[Dict] = None) -> List[Dict]:
        """æå–æ£€ç´¢åˆ°çš„nodesçš„åŸå§‹å†…å®¹ï¼Œä¾›agentsç³»ç»Ÿä½¿ç”¨"""
        nodes_content = []
        
        for result in top_k_results:
            node_id = result['node_id']
            if node_id not in node_embeddings:
                continue
                
            node_data = node_embeddings[node_id]
            metadata = node_data.get('metadata', {})
            
            # æ„å»ºagentså¯ç”¨çš„èŠ‚ç‚¹å†…å®¹
            node_content = {
                'node_id': node_id,
                'rank': len(nodes_content) + 1,
                'similarity_score': result['score'],
                'page_number': result['page_number'],
                'element_type': result['element_type'],
                
                # åŸå§‹å­æ ‘æ–‡æœ¬å†…å®¹ï¼ˆå®Œæ•´ç‰ˆï¼‰
                'text_content': self._get_subtree_full_content(node_id, original_nodes) if original_nodes else self._get_node_text_content(node_data, metadata),
                
                # ä½ç½®ä¿¡æ¯
                'bbox': metadata.get('bbox'),
                'dom_path': self._construct_dom_path(metadata),
                
                # ç»“æ„ä¿¡æ¯
                'heading_level': metadata.get('heading_level'),
                'is_chapter_title': metadata.get('is_chapter_title', False),
                'parent_chapter': metadata.get('parent_chapter'),
                'depth': metadata.get('depth', 0),
                
                # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œæå–å›¾ç‰‡ç›¸å…³å†…å®¹
                'image_info': self._extract_image_content(metadata) if result['element_type'] == 'figure' else None,
                
                # å¦‚æœæ˜¯è¡¨æ ¼ï¼Œæå–è¡¨æ ¼å†…å®¹
                'table_info': self._extract_table_content(metadata) if result['element_type'] == 'table' else None,
            }
            
            nodes_content.append(node_content)
        
        return nodes_content
    
    def _get_node_text_content(self, node_data: Dict, metadata: Dict) -> str:
        """è·å–èŠ‚ç‚¹çš„å®Œæ•´å­æ ‘æ–‡æœ¬å†…å®¹"""
        # å°è¯•ä»åŸå§‹node_embeddingsä¸­è·å–å®Œæ•´çš„å­æ ‘å†…å®¹
        # è¿™é‡Œçš„node_dataæ¥è‡ªäº_embed_nodes_batchï¼Œå¯èƒ½åªåŒ…å«é¢„è§ˆ
        text_content = node_data.get('content_preview', '') or metadata.get('text', '')
        return text_content.strip()
    
    def _get_subtree_full_content(self, node_id: str, all_nodes: List[Dict]) -> str:
        """è·å–å­æ ‘çš„å®Œæ•´å†…å®¹ï¼ˆåŒ…æ‹¬å­èŠ‚ç‚¹ï¼‰"""
        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹DOMèŠ‚ç‚¹
        target_node = None
        for node in all_nodes:
            if isinstance(node, dict) and node.get('metadata', {}).get('global_id') == node_id:
                target_node = node
                break
        
        if not target_node:
            return ""
        
        # é€’å½’æ”¶é›†å­æ ‘çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹
        return self._collect_subtree_text(target_node)
    
    def _collect_subtree_text(self, node: Dict) -> str:
        """é€’å½’æ”¶é›†å­æ ‘çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
        if not isinstance(node, dict):
            return ""
        
        text_parts = []
        
        # æ·»åŠ å½“å‰èŠ‚ç‚¹çš„æ–‡æœ¬
        node_text = node.get('text', '').strip()
        if node_text:
            text_parts.append(node_text)
        
        # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œæ·»åŠ AIæè¿°
        metadata = node.get('metadata', {})
        if metadata.get('element_type') == 'figure':
            ai_desc = metadata.get('ai_description', '')
            if ai_desc and ai_desc not in ['Skipped (disabled or too small)', 'Failed']:
                text_parts.append(f"[å›¾ç‰‡æè¿°: {ai_desc}]")
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        children = node.get('children', [])
        for child in children:
            child_text = self._collect_subtree_text(child)
            if child_text:
                text_parts.append(child_text)
        
        return " ".join(text_parts)
    
    def _construct_dom_path(self, metadata: Dict) -> str:
        """æ„å»ºDOMè·¯å¾„ä¿¡æ¯"""
        page = metadata.get('page_number', '?')
        if page != '?' and page is not None:
            page += 1  # è½¬æ¢ä¸º1-basedç´¢å¼•
        
        element_type = metadata.get('element_type', 'unknown')
        node_id = metadata.get('global_id', 'unknown')
        
        return f"Page {page} > {element_type}[{node_id}]"
    
    def _extract_image_content(self, metadata: Dict) -> Dict:
        """æå–å›¾ç‰‡ç›¸å…³å†…å®¹"""
        return {
            'ai_description': metadata.get('ai_description', ''),
            'description_method': metadata.get('description_method', ''),
            'image_extracted': metadata.get('image_extracted', False),
        }
    
    def _extract_table_content(self, metadata: Dict) -> Dict:
        """æå–è¡¨æ ¼ç›¸å…³å†…å®¹"""
        return {
            'table_image_extracted': metadata.get('table_image_extracted', False),
            'row_count': metadata.get('row_count'),
            'col_count': metadata.get('col_count'),
        }
    
    def _save_retrieved_nodes_for_agents(self, doc_id: str, question: str, nodes_content: List[Dict]):
        """ä¿å­˜æ£€ç´¢åˆ°çš„nodeså†…å®¹ä¾›agentsç³»ç»Ÿä½¿ç”¨"""
        # åˆ›å»ºä¿å­˜ç›®å½•
        agents_dir = "./retrieved_nodes_for_agents"
        os.makedirs(agents_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŸºäºæ–‡æ¡£IDå’Œé—®é¢˜hashï¼‰
        import hashlib
        question_hash = hashlib.md5(question.encode()).hexdigest()[:8]
        filename = f"{doc_id.replace('.pdf', '')}_{question_hash}.json"
        filepath = os.path.join(agents_dir, filename)
        
        # æ„å»ºä¿å­˜çš„æ•°æ®ç»“æ„
        agents_data = {
            'doc_id': doc_id,
            'question': question,
            'retrieval_timestamp': time.time(),
            'total_nodes': len(nodes_content),
            'nodes_content': nodes_content
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(agents_data, f, indent=2, ensure_ascii=False)
        
        print(f"   ğŸ’¾ ä¿å­˜æ£€ç´¢å†…å®¹ä¾›agentsä½¿ç”¨: {filepath}")
        return filepath
    
    def _evaluate_question(self, sample: Dict, node_embeddings: Dict[str, Dict], original_nodes: List[Dict] = None) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        # è§£æè¯æ®é¡µé¢
        try:
            evidence_pages = ast.literal_eval(sample['evidence_pages'])
            if not isinstance(evidence_pages, list):
                evidence_pages = [evidence_pages]
        except:
            evidence_pages = []
        
        # æ‰§è¡Œæ£€ç´¢ï¼ˆå³ä½¿æ²¡æœ‰è¯æ®é¡µé¢ï¼Œä¹Ÿè¦æ£€ç´¢ä¾›é—®ç­”ç³»ç»Ÿä½¿ç”¨ï¼‰
        
        # æ‰§è¡Œæ£€ç´¢
        query = sample['question']
        top_10_results = self._retrieve_top_k(query, node_embeddings, k=10)
        
        # æ£€æŸ¥å‰10ä¸ªèŠ‚ç‚¹ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰è¯æ®é¡µé¢
        retrieved_pages = set()
        for result in top_10_results:
            page_num = result['page_number']
            # æ·»åŠ 0-basedå’Œ1-basedé¡µé¢ï¼ˆå…¼å®¹æ€§æ£€æŸ¥ï¼‰
            # retrieved_pages.add(page_num)
            retrieved_pages.add(page_num + 1)
        
        # è®¡ç®—è¦†ç›–æƒ…å†µ
        evidence_pages_set = set(evidence_pages)
        covered_pages = evidence_pages_set.intersection(retrieved_pages)
        
        # å¦‚æœæ²¡æœ‰è¯æ®é¡µé¢ï¼Œæ ‡è®°ä¸ºä¸å‚ä¸ç»Ÿè®¡ï¼Œä½†ä»ç„¶æä¾›æ£€ç´¢ç»“æœ
        has_evidence = len(evidence_pages_set) > 0
        coverage_ratio = len(covered_pages) / len(evidence_pages_set) if has_evidence else 0.0
        all_pages_covered = coverage_ratio == 1.0 if has_evidence else False
        
        # è®¡ç®—Hit@1, Hit@3, Hit@5 (åªæœ‰æœ‰è¯æ®é¡µé¢æ—¶æ‰è®¡ç®—)
        hit_at_1 = False
        hit_at_3 = False  
        hit_at_5 = False
        
        if has_evidence and top_10_results:
            # æ£€æŸ¥å‰1ä¸ª
            page_1 = top_10_results[0]['page_number']
            hit_at_1 = page_1 in evidence_pages or (page_1 + 1) in evidence_pages
            
            # æ£€æŸ¥å‰3ä¸ª
            pages_3 = [r['page_number'] for r in top_10_results[:3]]
            hit_at_3 = any(p in evidence_pages or (p + 1) in evidence_pages for p in pages_3)
            
            # æ£€æŸ¥å‰5ä¸ª
            pages_5 = [r['page_number'] for r in top_10_results[:5]]
            hit_at_5 = any(p in evidence_pages or (p + 1) in evidence_pages for p in pages_5)
        
        # æå–å¹¶ä¿å­˜æ£€ç´¢åˆ°çš„nodeså†…å®¹ä¾›agentsä½¿ç”¨ï¼ˆåŒ…å«å®Œæ•´å­æ ‘å†…å®¹ï¼‰
        nodes_content = self._extract_nodes_content(top_10_results, node_embeddings, original_nodes)
        agents_file = self._save_retrieved_nodes_for_agents(sample['doc_id'], sample['question'], nodes_content)
        
        return {
            'doc_id': sample['doc_id'],
            'question': sample['question'],
            'answer': sample['answer'],
            'evidence_pages': evidence_pages,
            'evidence_sources': sample.get('evidence_sources', ''),
            'retrieved_pages': list(retrieved_pages),
            'coverage_ratio': coverage_ratio,
            'all_pages_covered': all_pages_covered,
            'hit_at_1': hit_at_1,
            'hit_at_3': hit_at_3,
            'hit_at_5': hit_at_5,
            'top_10_results': top_10_results,
            'agents_file': agents_file,  # æ–°å¢ï¼šä¿å­˜çš„agentsæ–‡ä»¶è·¯å¾„
            'has_evidence': has_evidence  # æ–°å¢ï¼šæ˜¯å¦æœ‰è¯æ®é¡µé¢ï¼ˆç”¨äºç»Ÿè®¡ç­›é€‰ï¼‰
        }
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼šæ‰€æœ‰æ–‡æ¡£ï¼Œæ‰€æœ‰é—®é¢˜"""
        if not self.embedder:
            print("âŒ è¯·å…ˆåˆå§‹åŒ–åµŒå…¥å™¨")
            return
        
        start_time = time.time()
        print(f"\nğŸ¯ å¼€å§‹å®Œæ•´æ£€ç´¢è¯„ä¼°")
        print(f"æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        
        # æŒ‰æ–‡æ¡£åˆ†ç»„æ ·æœ¬
        samples_by_doc = defaultdict(list)
        for sample in self.samples:
            samples_by_doc[sample['doc_id']].append(sample)
        
        print(f"æ–‡æ¡£æ•°: {len(samples_by_doc)}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_questions = 0
        total_coverage = 0
        total_all_covered = 0
        total_hit_1 = 0
        total_hit_3 = 0
        total_hit_5 = 0
        total_skipped = 0  # è·³è¿‡çš„æ— è¯æ®é¡µé¢æ ·æœ¬æ•°
        
        # é€æ–‡æ¡£å¤„ç†
        for doc_idx, (doc_id, doc_samples) in enumerate(samples_by_doc.items()):
            print(f"\nğŸ“– [{doc_idx+1}/{len(samples_by_doc)}] å¤„ç†æ–‡æ¡£: {doc_id}")
            print(f"   é—®é¢˜æ•°: {len(doc_samples)}")
            
            # åŠ è½½æ–‡æ¡£
            document = self._load_document(doc_id)
            if not document:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆåŠ è½½å¤±è´¥ï¼‰")
                continue
            
            # æ”¶é›†èŠ‚ç‚¹
            nodes = self._collect_document_nodes(document)
            if not nodes:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆæ— æœ‰æ•ˆèŠ‚ç‚¹ï¼‰")
                continue
            
            print(f"   èŠ‚ç‚¹æ•°: {len(nodes)}")
            
            # ç¼–ç èŠ‚ç‚¹
            try:
                node_embeddings = self._embed_nodes_batch(nodes)
                if not node_embeddings:
                    print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆç¼–ç å¤±è´¥ï¼‰")
                    continue
                print(f"   âœ… æˆåŠŸç¼–ç : {len(node_embeddings)} ä¸ªèŠ‚ç‚¹")
            except Exception as e:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆç¼–ç å¼‚å¸¸ï¼‰: {e}")
                continue
            
            # è¯„ä¼°è¯¥æ–‡æ¡£çš„æ‰€æœ‰é—®é¢˜
            doc_results = []
            doc_coverage = 0
            doc_all_covered = 0
            doc_hit_1 = 0
            doc_hit_3 = 0
            doc_hit_5 = 0
            
            for sample_idx, sample in enumerate(doc_samples):
                if sample_idx % 10 == 0:
                    print(f"   è¯„ä¼°è¿›åº¦: {sample_idx+1}/{len(doc_samples)}")
                
                try:
                    result = self._evaluate_question(sample, node_embeddings, nodes)
                    
                    if result is None:
                        continue
                    
                    doc_results.append(result)
                    self.results['per_question_results'].append(result)
                    
                    # åªæœ‰æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬æ‰å‚ä¸ç»Ÿè®¡è®¡ç®—
                    if result['has_evidence']:
                        doc_coverage += result['coverage_ratio']
                        if result['all_pages_covered']:
                            doc_all_covered += 1
                        if result['hit_at_1']:
                            doc_hit_1 += 1
                        if result['hit_at_3']:
                            doc_hit_3 += 1
                        if result['hit_at_5']:
                            doc_hit_5 += 1
                    else:
                        total_skipped += 1
                        print(f"   âš ï¸  è·³è¿‡ç»Ÿè®¡ï¼ˆæ— è¯æ®é¡µé¢ï¼‰: {sample['question'][:50]}...")
                    
                except Exception as e:
                    print(f"   âŒ é—®é¢˜è¯„ä¼°å¤±è´¥: {e}")
                    continue
            
            # æ–‡æ¡£çº§åˆ«ç»Ÿè®¡
            if doc_results:
                # è®¡ç®—æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬æ•°é‡
                doc_valid_count = sum(1 for r in doc_results if r['has_evidence'])
                
                doc_stats = {
                    'questions_count': len(doc_results),
                    'valid_questions_count': doc_valid_count,
                    'skipped_questions_count': len(doc_results) - doc_valid_count,
                    'avg_coverage': doc_coverage / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'all_pages_covered_rate': doc_all_covered / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'hit_at_1_rate': doc_hit_1 / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'hit_at_3_rate': doc_hit_3 / doc_valid_count if doc_valid_count > 0 else 0.0,
                    'hit_at_5_rate': doc_hit_5 / doc_valid_count if doc_valid_count > 0 else 0.0
                }
                self.results['per_document_stats'][doc_id] = doc_stats
                
                # ç´¯è®¡åˆ°æ€»ä½“ç»Ÿè®¡ï¼ˆåªè®¡ç®—æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬ï¼‰
                total_questions += doc_valid_count
                total_coverage += doc_coverage
                total_all_covered += doc_all_covered
                total_hit_1 += doc_hit_1
                total_hit_3 += doc_hit_3
                total_hit_5 += doc_hit_5
                
                print(f"   ğŸ“Š æ–‡æ¡£ç»Ÿè®¡: è¦†ç›–ç‡={doc_stats['avg_coverage']:.3f}, "
                      f"å®Œå…¨è¦†ç›–={doc_stats['all_pages_covered_rate']:.3f}, "
                      f"Hit@1={doc_stats['hit_at_1_rate']:.3f}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if total_questions > 0:
            self.results['overall_stats'] = {
                'total_questions': total_questions,
                'total_documents': len(self.results['per_document_stats']),
                'total_skipped': total_skipped,
                'total_samples': len(self.samples),
                'avg_coverage': total_coverage / total_questions,
                'all_pages_covered_rate': total_all_covered / total_questions,
                'hit_at_1_rate': total_hit_1 / total_questions,
                'hit_at_3_rate': total_hit_3 / total_questions,
                'hit_at_5_rate': total_hit_5 / total_questions
            }
        
        # è®°å½•è¯„ä¼°æ—¶é—´
        self.results['evaluation_time'] = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        self._display_results()
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        # æ˜¾ç¤ºagentsæ–‡ä»¶ä¿å­˜ä¿¡æ¯
        agents_files_count = sum(1 for r in self.results['per_question_results'] if r.get('agents_file'))
        valid_agents_count = sum(1 for r in self.results['per_question_results'] if r.get('agents_file') and r['has_evidence'])
        no_evidence_agents_count = sum(1 for r in self.results['per_question_results'] if r.get('agents_file') and not r['has_evidence'])
        
        print(f"\nğŸ’¾ å·²ä¸º {agents_files_count} ä¸ªé—®é¢˜ä¿å­˜æ£€ç´¢å†…å®¹åˆ° ./retrieved_nodes_for_agents/ ç›®å½•")
        print(f"   - æœ‰è¯æ®é¡µé¢: {valid_agents_count} ä¸ª")
        print(f"   - æ— è¯æ®é¡µé¢: {no_evidence_agents_count} ä¸ª")
        print("   æ‰€æœ‰æ–‡ä»¶éƒ½å¯ä¾›agentsç³»ç»Ÿä½¿ç”¨ï¼ˆé—®ç­”ç³»ç»Ÿå¯æ®æ­¤åˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”ï¼‰")
    
    def _display_results(self):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
        stats = self.results['overall_stats']
        if not stats:
            print("âŒ æ— è¯„ä¼°ç»“æœ")
            return
        
        print("\n" + "="*80)
        print("ğŸ¯ å®Œæ•´æ£€ç´¢è¯„ä¼°ç»“æœ")
        print("="*80)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"   æœ‰æ•ˆé—®é¢˜æ•°: {stats['total_questions']} (æœ‰è¯æ®é¡µé¢)")
        print(f"   è·³è¿‡æ ·æœ¬æ•°: {stats['total_skipped']} (æ— è¯æ®é¡µé¢ï¼Œä½†å·²æ£€ç´¢)")
        print(f"   å¤„ç†æ–‡æ¡£æ•°: {stats['total_documents']}")
        print(f"   è¯„ä¼°æ—¶é—´: {self.results['evaluation_time']:.1f}ç§’")
        
        print(f"\nğŸ“ˆ æ£€ç´¢æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å¹³å‡é¡µé¢è¦†ç›–ç‡: {stats['avg_coverage']:.3f}")
        print(f"   å®Œå…¨è¦†ç›–ç‡: {stats['all_pages_covered_rate']:.3f}")
        print(f"   Hit@1: {stats['hit_at_1_rate']:.3f}")
        print(f"   Hit@3: {stats['hit_at_3_rate']:.3f}")
        print(f"   Hit@5: {stats['hit_at_5_rate']:.3f}")
        
        # æ˜¾ç¤ºä¸€äº›æˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹ï¼ˆåªè€ƒè™‘æœ‰è¯æ®é¡µé¢çš„æ ·æœ¬ï¼‰
        valid_results = [r for r in self.results['per_question_results'] if r['has_evidence']]
        success_cases = [r for r in valid_results if r['all_pages_covered']]
        fail_cases = [r for r in valid_results if not r['all_pages_covered']]
        no_evidence_cases = [r for r in self.results['per_question_results'] if not r['has_evidence']]
        
        if success_cases:
            print(f"\nâœ… æˆåŠŸæ¡ˆä¾‹ ({len(success_cases)}ä¸ª):")
            for case in success_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   è¯æ®é¡µé¢: {case['evidence_pages']}")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print()
        
        if fail_cases:
            print(f"\nâŒ å¤±è´¥æ¡ˆä¾‹ ({len(fail_cases)}ä¸ª):")
            for case in fail_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   è¯æ®é¡µé¢: {case['evidence_pages']}")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print(f"   è¦†ç›–ç‡: {case['coverage_ratio']:.3f}")
                print()
        
        if no_evidence_cases:
            print(f"\nâš ï¸  æ— è¯æ®é¡µé¢æ¡ˆä¾‹ ({len(no_evidence_cases)}ä¸ªï¼Œå·²æ£€ç´¢ä½†æœªå‚ä¸ç»Ÿè®¡):")
            for case in no_evidence_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print(f"   agentsæ–‡ä»¶: {case.get('agents_file', 'N/A')}")
                print()
    
    def _save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_file = "retrieval_evaluation_results.json"
        
        # ä¸ºäº†JSONåºåˆ—åŒ–ï¼Œè½¬æ¢numpyæ•°ç»„ç­‰
        serializable_results = {
            'overall_stats': self.results['overall_stats'],
            'per_document_stats': self.results['per_document_stats'],
            'evaluation_time': self.results['evaluation_time'],
            'model_info': {
                'model_name': self.embedder.model_name if self.embedder else 'unknown',
                'embedding_dim': self.embedder.text_encoder.target_dim if self.embedder else 'unknown'
            },
            'evaluation_config': {
                'top_k': 10,
                'max_depth': 2,
                'data_dir': self.data_dir
            }
        }
        
        # ä¿å­˜ç®€åŒ–ç‰ˆçš„é—®é¢˜ç»“æœï¼ˆå»æ‰embeddingç­‰å¤§å¯¹è±¡ï¼‰
        simplified_question_results = []
        for result in self.results['per_question_results']:
            simplified_result = {
                'doc_id': result['doc_id'],
                'question': result['question'][:200],  # æˆªæ–­é—®é¢˜
                'evidence_pages': result['evidence_pages'],
                'coverage_ratio': result['coverage_ratio'],
                'all_pages_covered': result['all_pages_covered'],
                'hit_at_1': result['hit_at_1'],
                'hit_at_3': result['hit_at_3'],
                'hit_at_5': result['hit_at_5'],
                'top_3_results': [
                    {
                        'page_number': r['page_number'],
                        'score': r['score'],
                        'element_type': r['element_type']
                    }
                    for r in result['top_10_results'][:3]
                ]
            }
            simplified_question_results.append(simplified_result)
        
        serializable_results['per_question_results'] = simplified_question_results
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ç®€åŒ–æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SimpleRetrievalEvaluator()
    
    # åˆå§‹åŒ–åµŒå…¥å™¨
    evaluator.initialize_embedder()
    
    # è¿è¡Œå®Œæ•´è¯„ä¼°
    evaluator.run_full_evaluation()
    
    print("\nğŸ‰ è¯„ä¼°å®Œæˆ!")

if __name__ == "__main__":
    main()