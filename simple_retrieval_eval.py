#!/usr/bin/env python3
"""
ç®€åŒ–æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ - è¯„ä¼°å‰10ä¸ªèŠ‚ç‚¹ä¸­æ˜¯å¦åŒ…å«è¯æ®é¡µé¢
ä½¿ç”¨CLIPæ¨¡å‹ï¼Œæ”¯æŒGPUåŠ é€Ÿï¼Œæµ‹è¯•æ‰€æœ‰æ–‡æ¡£æ‰€æœ‰é—®é¢˜ï¼Œä¿å­˜ç»“æœ
"""

import json
import os
import sys
import numpy as np
import time
from typing import Dict, List, Any
from collections import defaultdict
import ast
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

class SimpleRetrievalEvaluator:
    """ç®€åŒ–æ£€ç´¢è¯„ä¼°å™¨"""
    
    def __init__(self, samples_path: str = "samples.json", 
                 data_dir: str = "./data/dom/MMLongBench-Doc"):
        self.samples_path = samples_path
        self.data_dir = data_dir
        self.samples = []
        self.embedder = None
        
        # åŠ è½½æ ·æœ¬
        self._load_samples()
        
        # ç»“æœå­˜å‚¨
        self.results = {
            'overall_stats': {},
            'per_document_stats': {},
            'per_question_results': [],
            'evaluation_time': 0
        }
    
    def _load_samples(self):
        """åŠ è½½æ ·æœ¬æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ ·æœ¬æ•°æ®: {self.samples_path}")
        with open(self.samples_path, 'r', encoding='utf-8') as f:
            self.samples = json.load(f)
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def initialize_embedder(self):
        """åˆå§‹åŒ–CLIPåµŒå…¥å™¨"""
        print("ğŸš€ åˆå§‹åŒ–CLIPåµŒå…¥å™¨...")
        from subtree_embedding import SubtreeEmbedder
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä½¿ç”¨CLIPæ¨¡å‹
        self.embedder = SubtreeEmbedder(
            model_name="openai/clip-vit-large-patch14",  # ä½¿ç”¨CLIP
            aggregator="attention",
            embedding_dim=768,  # CLIPçš„ç»´åº¦
            data_dir=self.data_dir,
            device=device  # ä¼ é€’è®¾å¤‡ä¿¡æ¯
        )
        print(f"âœ… åµŒå…¥å™¨åˆå§‹åŒ–å®Œæˆ: {self.embedder.model_name}")
    
    def _load_document(self, doc_id: str) -> Dict:
        """åŠ è½½æ–‡æ¡£DOM JSON"""
        doc_name = doc_id.replace('.pdf', '.json')
        doc_path = os.path.join(self.data_dir, doc_name)
        
        if not os.path.exists(doc_path):
            return None
        
        try:
            with open(doc_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥ {doc_name}: {e}")
            return None
    
    def _collect_document_nodes(self, root_node) -> List[Dict]:
        """æ”¶é›†æ–‡æ¡£ä¸­çš„æ‰€æœ‰æœ‰æ•ˆèŠ‚ç‚¹"""
        nodes = []
        
        def dfs_collect(node):
            if isinstance(node, dict):
                metadata = node.get('metadata', {})
                if (metadata.get('global_id') and 
                    metadata.get('page_number') is not None):
                    nodes.append(node)
                
                # é€’å½’å¤„ç†å­èŠ‚ç‚¹
                for child in node.get('children', []):
                    dfs_collect(child)
        
        dfs_collect(root_node)
        return nodes
    
    def _embed_nodes_batch(self, nodes: List[Dict], batch_size: int = 50) -> Dict[str, Dict]:
        """æ‰¹é‡ç¼–ç èŠ‚ç‚¹"""
        node_embeddings = {}
        total_nodes = len(nodes)
        
        for i in range(0, total_nodes, batch_size):
            batch = nodes[i:i+batch_size]
            print(f"   ç¼–ç èŠ‚ç‚¹æ‰¹æ¬¡: {i+1}-{min(i+batch_size, total_nodes)}/{total_nodes}")
            
            for node in batch:
                try:
                    metadata = node.get('metadata', {})
                    node_id = metadata.get('global_id')
                    page_number = metadata.get('page_number', 0)
                    element_type = metadata.get('element_type', 'unknown')
                    
                    # ç¼–ç èŠ‚ç‚¹
                    result = self.embedder.encode_subtree(node, max_depth=2)
                    
                    # æå–å†…å®¹é¢„è§ˆ
                    text = node.get('text', '').strip()
                    ai_desc = metadata.get('ai_description', '')
                    content_preview = text[:100] if text else f"[{element_type}]"
                    
                    node_embeddings[node_id] = {
                        'embedding': result['embedding'],
                        'page_number': page_number,
                        'element_type': element_type,
                        'content_preview': content_preview,
                        'metadata': metadata
                    }
                    
                except Exception as e:
                    # é™é»˜å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                    continue
        
        return node_embeddings
    
    def _retrieve_top_k(self, query: str, node_embeddings: Dict[str, Dict], k: int = 10) -> List[Dict]:
        """æ£€ç´¢top-kç›¸å…³èŠ‚ç‚¹"""
        if not node_embeddings:
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedder.text_encoder.encode(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for node_id, node_data in node_embeddings.items():
            try:
                node_embedding = node_data['embedding']
                similarity = np.dot(query_embedding, node_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding)
                )
                similarities.append({
                    'node_id': node_id,
                    'score': float(similarity),
                    'page_number': node_data['page_number'],
                    'element_type': node_data['element_type'],
                    'content_preview': node_data['content_preview']
                })
            except Exception:
                continue
        
        # æ’åºå¹¶è¿”å›top-k
        similarities.sort(key=lambda x: x['score'], reverse=True)
        return similarities[:k]
    
    def _evaluate_question(self, sample: Dict, node_embeddings: Dict[str, Dict]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        # è§£æè¯æ®é¡µé¢
        try:
            evidence_pages = ast.literal_eval(sample['evidence_pages'])
            if not isinstance(evidence_pages, list):
                evidence_pages = [evidence_pages]
        except:
            evidence_pages = []
        
        # æ‰§è¡Œæ£€ç´¢
        query = sample['question']
        top_10_results = self._retrieve_top_k(query, node_embeddings, k=10)
        
        # æ£€æŸ¥å‰10ä¸ªèŠ‚ç‚¹ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰è¯æ®é¡µé¢
        retrieved_pages = set()
        for result in top_10_results:
            page_num = result['page_number']
            # æ·»åŠ 0-basedå’Œ1-basedé¡µé¢ï¼ˆå…¼å®¹æ€§æ£€æŸ¥ï¼‰
            # retrieved_pages.add(page_num)
            retrieved_pages.add(page_num + 1)
        
        # è®¡ç®—è¦†ç›–æƒ…å†µ
        evidence_pages_set = set(evidence_pages)
        covered_pages = evidence_pages_set.intersection(retrieved_pages)
        coverage_ratio = len(covered_pages) / len(evidence_pages_set) if evidence_pages_set else 0.0
        all_pages_covered = coverage_ratio == 1.0
        
        # è®¡ç®—Hit@1, Hit@3, Hit@5
        hit_at_1 = False
        hit_at_3 = False  
        hit_at_5 = False
        
        if top_10_results:
            # æ£€æŸ¥å‰1ä¸ª
            page_1 = top_10_results[0]['page_number']
            hit_at_1 = page_1 in evidence_pages or (page_1 + 1) in evidence_pages
            
            # æ£€æŸ¥å‰3ä¸ª
            pages_3 = [r['page_number'] for r in top_10_results[:3]]
            hit_at_3 = any(p in evidence_pages or (p + 1) in evidence_pages for p in pages_3)
            
            # æ£€æŸ¥å‰5ä¸ª
            pages_5 = [r['page_number'] for r in top_10_results[:5]]
            hit_at_5 = any(p in evidence_pages or (p + 1) in evidence_pages for p in pages_5)
        
        return {
            'doc_id': sample['doc_id'],
            'question': sample['question'],
            'answer': sample['answer'],
            'evidence_pages': evidence_pages,
            'evidence_sources': sample.get('evidence_sources', ''),
            'retrieved_pages': list(retrieved_pages),
            'coverage_ratio': coverage_ratio,
            'all_pages_covered': all_pages_covered,
            'hit_at_1': hit_at_1,
            'hit_at_3': hit_at_3,
            'hit_at_5': hit_at_5,
            'top_10_results': top_10_results
        }
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼šæ‰€æœ‰æ–‡æ¡£ï¼Œæ‰€æœ‰é—®é¢˜"""
        if not self.embedder:
            print("âŒ è¯·å…ˆåˆå§‹åŒ–åµŒå…¥å™¨")
            return
        
        start_time = time.time()
        print(f"\nğŸ¯ å¼€å§‹å®Œæ•´æ£€ç´¢è¯„ä¼°")
        print(f"æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        
        # æŒ‰æ–‡æ¡£åˆ†ç»„æ ·æœ¬
        samples_by_doc = defaultdict(list)
        for sample in self.samples:
            samples_by_doc[sample['doc_id']].append(sample)
        
        print(f"æ–‡æ¡£æ•°: {len(samples_by_doc)}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_questions = 0
        total_coverage = 0
        total_all_covered = 0
        total_hit_1 = 0
        total_hit_3 = 0
        total_hit_5 = 0
        
        # é€æ–‡æ¡£å¤„ç†
        for doc_idx, (doc_id, doc_samples) in enumerate(samples_by_doc.items()):
            print(f"\nğŸ“– [{doc_idx+1}/{len(samples_by_doc)}] å¤„ç†æ–‡æ¡£: {doc_id}")
            print(f"   é—®é¢˜æ•°: {len(doc_samples)}")
            
            # åŠ è½½æ–‡æ¡£
            document = self._load_document(doc_id)
            if not document:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆåŠ è½½å¤±è´¥ï¼‰")
                continue
            
            # æ”¶é›†èŠ‚ç‚¹
            nodes = self._collect_document_nodes(document)
            if not nodes:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆæ— æœ‰æ•ˆèŠ‚ç‚¹ï¼‰")
                continue
            
            print(f"   èŠ‚ç‚¹æ•°: {len(nodes)}")
            
            # ç¼–ç èŠ‚ç‚¹
            try:
                node_embeddings = self._embed_nodes_batch(nodes)
                if not node_embeddings:
                    print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆç¼–ç å¤±è´¥ï¼‰")
                    continue
                print(f"   âœ… æˆåŠŸç¼–ç : {len(node_embeddings)} ä¸ªèŠ‚ç‚¹")
            except Exception as e:
                print(f"   âŒ è·³è¿‡æ–‡æ¡£ï¼ˆç¼–ç å¼‚å¸¸ï¼‰: {e}")
                continue
            
            # è¯„ä¼°è¯¥æ–‡æ¡£çš„æ‰€æœ‰é—®é¢˜
            doc_results = []
            doc_coverage = 0
            doc_all_covered = 0
            doc_hit_1 = 0
            doc_hit_3 = 0
            doc_hit_5 = 0
            
            for sample_idx, sample in enumerate(doc_samples):
                if sample_idx % 10 == 0:
                    print(f"   è¯„ä¼°è¿›åº¦: {sample_idx+1}/{len(doc_samples)}")
                
                try:
                    result = self._evaluate_question(sample, node_embeddings)
                    doc_results.append(result)
                    self.results['per_question_results'].append(result)
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    doc_coverage += result['coverage_ratio']
                    if result['all_pages_covered']:
                        doc_all_covered += 1
                    if result['hit_at_1']:
                        doc_hit_1 += 1
                    if result['hit_at_3']:
                        doc_hit_3 += 1
                    if result['hit_at_5']:
                        doc_hit_5 += 1
                    
                except Exception as e:
                    print(f"   âŒ é—®é¢˜è¯„ä¼°å¤±è´¥: {e}")
                    continue
            
            # æ–‡æ¡£çº§åˆ«ç»Ÿè®¡
            if doc_results:
                doc_stats = {
                    'questions_count': len(doc_results),
                    'avg_coverage': doc_coverage / len(doc_results),
                    'all_pages_covered_rate': doc_all_covered / len(doc_results),
                    'hit_at_1_rate': doc_hit_1 / len(doc_results),
                    'hit_at_3_rate': doc_hit_3 / len(doc_results),
                    'hit_at_5_rate': doc_hit_5 / len(doc_results)
                }
                self.results['per_document_stats'][doc_id] = doc_stats
                
                # ç´¯è®¡åˆ°æ€»ä½“ç»Ÿè®¡
                total_questions += len(doc_results)
                total_coverage += doc_coverage
                total_all_covered += doc_all_covered
                total_hit_1 += doc_hit_1
                total_hit_3 += doc_hit_3
                total_hit_5 += doc_hit_5
                
                print(f"   ğŸ“Š æ–‡æ¡£ç»Ÿè®¡: è¦†ç›–ç‡={doc_stats['avg_coverage']:.3f}, "
                      f"å®Œå…¨è¦†ç›–={doc_stats['all_pages_covered_rate']:.3f}, "
                      f"Hit@1={doc_stats['hit_at_1_rate']:.3f}")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if total_questions > 0:
            self.results['overall_stats'] = {
                'total_questions': total_questions,
                'total_documents': len(self.results['per_document_stats']),
                'avg_coverage': total_coverage / total_questions,
                'all_pages_covered_rate': total_all_covered / total_questions,
                'hit_at_1_rate': total_hit_1 / total_questions,
                'hit_at_3_rate': total_hit_3 / total_questions,
                'hit_at_5_rate': total_hit_5 / total_questions
            }
        
        # è®°å½•è¯„ä¼°æ—¶é—´
        self.results['evaluation_time'] = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        self._display_results()
        
        # ä¿å­˜ç»“æœ
        self._save_results()
    
    def _display_results(self):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
        stats = self.results['overall_stats']
        if not stats:
            print("âŒ æ— è¯„ä¼°ç»“æœ")
            return
        
        print("\n" + "="*80)
        print("ğŸ¯ å®Œæ•´æ£€ç´¢è¯„ä¼°ç»“æœ")
        print("="*80)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»é—®é¢˜æ•°: {stats['total_questions']}")
        print(f"   å¤„ç†æ–‡æ¡£æ•°: {stats['total_documents']}")
        print(f"   è¯„ä¼°æ—¶é—´: {self.results['evaluation_time']:.1f}ç§’")
        
        print(f"\nğŸ“ˆ æ£€ç´¢æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å¹³å‡é¡µé¢è¦†ç›–ç‡: {stats['avg_coverage']:.3f}")
        print(f"   å®Œå…¨è¦†ç›–ç‡: {stats['all_pages_covered_rate']:.3f}")
        print(f"   Hit@1: {stats['hit_at_1_rate']:.3f}")
        print(f"   Hit@3: {stats['hit_at_3_rate']:.3f}")
        print(f"   Hit@5: {stats['hit_at_5_rate']:.3f}")
        
        # æ˜¾ç¤ºä¸€äº›æˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹
        success_cases = [r for r in self.results['per_question_results'] if r['all_pages_covered']]
        fail_cases = [r for r in self.results['per_question_results'] if not r['all_pages_covered']]
        
        if success_cases:
            print(f"\nâœ… æˆåŠŸæ¡ˆä¾‹ ({len(success_cases)}ä¸ª):")
            for case in success_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   è¯æ®é¡µé¢: {case['evidence_pages']}")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print()
        
        if fail_cases:
            print(f"\nâŒ å¤±è´¥æ¡ˆä¾‹ ({len(fail_cases)}ä¸ª):")
            for case in fail_cases[:2]:
                print(f"   æ–‡æ¡£: {case['doc_id']}")
                print(f"   é—®é¢˜: {case['question'][:60]}...")
                print(f"   è¯æ®é¡µé¢: {case['evidence_pages']}")
                print(f"   æ£€ç´¢é¡µé¢: {sorted(list(set(case['retrieved_pages'])))[:10]}")
                print(f"   è¦†ç›–ç‡: {case['coverage_ratio']:.3f}")
                print()
    
    def _save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_file = "retrieval_evaluation_results.json"
        
        # ä¸ºäº†JSONåºåˆ—åŒ–ï¼Œè½¬æ¢numpyæ•°ç»„ç­‰
        serializable_results = {
            'overall_stats': self.results['overall_stats'],
            'per_document_stats': self.results['per_document_stats'],
            'evaluation_time': self.results['evaluation_time'],
            'model_info': {
                'model_name': self.embedder.model_name if self.embedder else 'unknown',
                'embedding_dim': self.embedder.text_encoder.target_dim if self.embedder else 'unknown'
            },
            'evaluation_config': {
                'top_k': 10,
                'max_depth': 2,
                'data_dir': self.data_dir
            }
        }
        
        # ä¿å­˜ç®€åŒ–ç‰ˆçš„é—®é¢˜ç»“æœï¼ˆå»æ‰embeddingç­‰å¤§å¯¹è±¡ï¼‰
        simplified_question_results = []
        for result in self.results['per_question_results']:
            simplified_result = {
                'doc_id': result['doc_id'],
                'question': result['question'][:200],  # æˆªæ–­é—®é¢˜
                'evidence_pages': result['evidence_pages'],
                'coverage_ratio': result['coverage_ratio'],
                'all_pages_covered': result['all_pages_covered'],
                'hit_at_1': result['hit_at_1'],
                'hit_at_3': result['hit_at_3'],
                'hit_at_5': result['hit_at_5'],
                'top_3_results': [
                    {
                        'page_number': r['page_number'],
                        'score': r['score'],
                        'element_type': r['element_type']
                    }
                    for r in result['top_10_results'][:3]
                ]
            }
            simplified_question_results.append(simplified_result)
        
        serializable_results['per_question_results'] = simplified_question_results
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ç®€åŒ–æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SimpleRetrievalEvaluator()
    
    # åˆå§‹åŒ–åµŒå…¥å™¨
    evaluator.initialize_embedder()
    
    # è¿è¡Œå®Œæ•´è¯„ä¼°
    evaluator.run_full_evaluation()
    
    print("\nğŸ‰ è¯„ä¼°å®Œæˆ!")

if __name__ == "__main__":
    main()