import json
import os
from typing import List, Dict, Optional, Tuple, Any, Union
from pathlib import Path
import fitz  # PyMuPDF
from PIL import Image
import base64
import requests
import openai
from collections import defaultdict
import torch
import logging

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# é™æ€é…ç½®å¸¸é‡
# STATIC_QWEN2VL_MODEL = "Qwen/Qwen2-VL-7B-Instruct"
# STATIC_QWEN2VL_DEVICE = 0

try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    QWEN2VL_AVAILABLE = True
except ImportError:
    QWEN2VL_AVAILABLE = False
    logger.warning("Qwen2VL not available. Please install transformers and torch to use Qwen2VL.")


def generate_output_dir_name(dataset_name: str = "MMLongBench-Doc",
                            enable_image_description: bool = False,
                            prefer_model: str = "qwen2vl", 
                            min_image_size: int = 100,
                            max_merge_chars: int = 128) -> str:
    """ç”Ÿæˆè¾“å‡ºç›®å½•åç§°
    
    æ ¼å¼:
    - ç¦ç”¨å›¾åƒæè¿°: MMLongBench-Doc_skip-images-description
    - å¯ç”¨å›¾åƒæè¿°: MMLongBench-Doc_qwen2vl-250-256
    """
    if not enable_image_description:
        prefer_model = 'skip'
        min_image_size = 0
        return f"{dataset_name}_{prefer_model}-{min_image_size}-{max_merge_chars}"
    else:
        return f"{dataset_name}_{prefer_model}-{min_image_size}-{max_merge_chars}"


class DOMNode:
    def __init__(self, tag: str, text: str = '', attrs: Optional[Dict] = None,
                 depth: int = 0, page_id: Optional[str] = None, parent: Optional['DOMNode'] = None):
        self.tag = tag
        self.text = text.strip() if text else ''
        self.attrs = attrs or {}
        self.children: List['DOMNode'] = []
        self.parent = parent
        self.depth = depth
        self.page_id = page_id
        self.metadata = {}
        self.node_type = None
        self.global_id = None
        
        self.image_context = {
            'caption': '',
            'alt_text': '',
            'title_text': '',
            'surrounding_text': '',
            'reference_texts': []
        }

    def add_child(self, child: 'DOMNode'):
        self.children.append(child)
        child.parent = self

    def assign_ids(self, prefix='node', counter=[0]):
        self.global_id = f"{prefix}_{counter[0]}"
        self.metadata['global_id'] = self.global_id
        counter[0] += 1
        for child in self.children:
            child.assign_ids(prefix, counter)

    def to_json_dict(self):
        data = {
            'tag': self.tag,
            'text': self.text if self.text else None,
            'metadata': {
                'depth': self.depth,
                'page_id': self.page_id,
                'global_id': self.global_id,
                'node_type': self.node_type,
                **self.metadata
            },
            'children': [child.to_json_dict() for child in self.children] if self.children else []
        }
        
        for k in ['class', 'src', 'href', 'data-page', 'data-id', 'style']:
            if k in self.attrs:
                data[k] = self.attrs[k]

        return {k: v for k, v in data.items() if v is not None and v != []}
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'DOMNode':
        """ä»å­—å…¸æ•°æ®é‡æ„ DOMNode å¯¹è±¡"""
        # åˆ›å»ºèŠ‚ç‚¹
        metadata = data.get('metadata', {})
        
        # æå–å±æ€§
        attrs = {}
        for k in ['class', 'src', 'href', 'data-page', 'data-id', 'style']:
            if k in data:
                attrs[k] = data[k]
        
        # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
        node = cls(
            tag=data['tag'],
            text=data.get('text', ''),
            attrs=attrs,
            depth=metadata.get('depth', 0),
            page_id=metadata.get('page_id'),
            parent=None
        )
        
        # è®¾ç½®å…¶ä»–å±æ€§
        node.global_id = metadata.get('global_id')
        node.node_type = metadata.get('node_type')
        
        # æ¢å¤å®Œæ•´çš„ metadataï¼Œè¿‡æ»¤æ‰å·²ç»è®¾ç½®çš„åŸºæœ¬å­—æ®µ
        node.metadata = {k: v for k, v in metadata.items() 
                        if k not in ['depth', 'page_id', 'global_id', 'node_type']}
        
        # é€’å½’åˆ›å»ºå­èŠ‚ç‚¹
        children_data = data.get('children', [])
        for child_data in children_data:
            child_node = cls.from_dict(child_data)
            node.add_child(child_node)
        
        return node


class Qwen2VLService:
    """Qwen2VLè§†è§‰è¯­è¨€æ¨¡å‹æœåŠ¡ï¼Œç”¨äºå›¾åƒæè¿°"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2-VL-7B-Instruct", 
                 cuda_device: int = 0, seed: int = 42):
        if not QWEN2VL_AVAILABLE:
            raise ImportError("Qwen2VL dependencies not available. Please install transformers and torch.")
        
        self.model_name = model_name
        # self.temperature = temperature
        self.seed = seed
        self.device = f"cuda:{cuda_device}" if torch.cuda.is_available() else "cpu"
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        self.model = None
        self.processor = None
        self._model_loaded = False
        
        logger.info(f"Qwen2VL service initialized. Model will be loaded on first use.")
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self._model_loaded:
            return
        
        logger.info(f"Loading Qwen2VL model: {self.model_name}...")
        
        try:
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_name,
                torch_dtype=torch.bfloat16,
                device_map={"": self.device}
            )
            self._model_loaded = True
            logger.info(f"Qwen2VL model loaded successfully on {self.device}")
        except Exception as e:
            logger.error(f"Failed to load Qwen2VL model: {e}")
            raise
    
    def _prepare_image(self, image_path: str) -> Image.Image:
        """å›¾åƒé¢„å¤„ç†"""
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image not found: {image_path}")
        
        image = Image.open(image_path)
        return image.convert('RGB') if image.mode != 'RGB' else image
    
    
    def describe_region_in_full_page(self, full_page_image_path: str, bbox: List[float], 
                                   page_context: str = "") -> str:
        """åŸºäºæ•´é¡µå›¾åƒå’Œåæ ‡åŒºåŸŸç”Ÿæˆæè¿°"""
        if not self._model_loaded:
            self._load_model()
        
        try:
            # å‡†å¤‡æ•´é¡µå›¾åƒ
            image = self._prepare_image(full_page_image_path)
            
            # æ„å»ºåŒ…å«åæ ‡ä¿¡æ¯çš„æç¤ºè¯
            prompt = self._build_region_description_prompt(bbox, page_context)
            
            # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text,
                images=[image],
                padding=True,
                return_tensors="pt",
            ).to(self.device)
            
            # ç”Ÿæˆå‚æ•°
            generation_params = {
                "max_new_tokens": 512,
                "do_sample": False,
            }
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    **generation_params
                )
            
            # è§£ç å“åº”
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            response = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Qwen2VL region description failed: {e}")
            return f"Region description failed: {str(e)}"
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def _build_region_description_prompt(self, bbox: List[float], page_context: str = "") -> str:
        """æ„å»ºåŸºäºåæ ‡åŒºåŸŸçš„å›¾åƒæè¿°æç¤ºè¯"""
        x1, y1, x2, y2 = bbox
        
        base_prompt = f"""You are an expert document analyst. Please analyze the content within the specified rectangular region in this document page image.

**Target Region Coordinates:**
- Top-left: ({x1:.1f}, {y1:.1f})  
- Bottom-right: ({x2:.1f}, {y2:.1f})

**Instructions:**
1. Focus specifically on the content within these coordinates
2. Describe what you see in this rectangular area
3. Consider how this region relates to the surrounding page content
4. Provide a structured analysis

**Output Format:**
1. **Summary**: One sentence describing what's in the target region
2. **Content Details**: Bullet points of specific elements within the coordinates
3. **Visual Features**: Colors, layout, text, graphics within the region
4. **Context Integration**: How this region fits with the overall page layout
5. **Purpose**: The likely function or meaning of this content

Focus only on the specified coordinate region, but use the surrounding context to better understand its purpose."""
        
        if page_context.strip():
            context_prompt = f"""

**Document Context:**
{page_context[:1000]}

Use this context to better understand the purpose and meaning of the content in the target region."""
            return base_prompt + context_prompt
        
        return base_prompt
    
    
    def unload(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        self._model_loaded = False
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("Qwen2VL model unloaded")


class ImageDescriptionService:
    """å›¾åƒæè¿°æœåŠ¡ï¼Œæ”¯æŒGPTå’ŒQwen2VLä¸¤ç§æ¨¡å‹"""
    
    def __init__(self, openai_api_key: Optional[str] = None, prefer_model: str = "qwen2vl"):
        self.openai_api_key = openai_api_key
        self.prefer_model = prefer_model
        self.qwen2vl_service = None
        
        if openai_api_key:
            self.openai_client = openai.OpenAI(api_key=openai_api_key)
        else:
            self.openai_client = None
        
        # åªåœ¨é€‰æ‹© qwen2vl æ—¶åˆå§‹åŒ– Qwen2VL æœåŠ¡
        if prefer_model == "qwen2vl" and STATIC_QWEN2VL_MODEL and QWEN2VL_AVAILABLE:
            try:
                self.qwen2vl_service = Qwen2VLService(
                    model_name=STATIC_QWEN2VL_MODEL,
                    cuda_device=STATIC_QWEN2VL_DEVICE,
                    # temperature=0.0
                )
                logger.info(f"Qwen2VL service initialized with model: {STATIC_QWEN2VL_MODEL}")
            except Exception as e:
                logger.error(f"Failed to initialize Qwen2VL service: {e}")
                self.qwen2vl_service = None
    
    
    def describe_region_with_qwen2vl(self, full_page_image_path: str, bbox: List[float], 
                                   page_context: str = "") -> Optional[str]:
        """ä½¿ç”¨Qwen2VLåŸºäºæ•´é¡µå›¾åƒå’Œåæ ‡æè¿°åŒºåŸŸ"""
        if not self.qwen2vl_service:
            return None
        
        try:
            return self.qwen2vl_service.describe_region_in_full_page(
                full_page_image_path, bbox, page_context
            )
        except Exception as e:
            logger.error(f"Qwen2VLåŒºåŸŸæè¿°å¤±è´¥: {e}")
            return None
    
    def describe_region_with_gpt(self, full_page_image_path: str, bbox: List[float], 
                               page_context: str = "") -> Optional[str]:
        """ä½¿ç”¨GPTåŸºäºæ•´é¡µå›¾åƒå’Œåæ ‡æè¿°åŒºåŸŸ"""
        if not self.openai_client:
            return None
        
        try:
            # è¯»å–æ•´é¡µå›¾åƒå¹¶è½¬æ¢ä¸ºbase64
            with open(full_page_image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode('utf-8')
            
            # æ„å»ºåŒ…å«åæ ‡ä¿¡æ¯çš„æè¿°æç¤ºè¯
            prompt = self._build_gpt_region_description_prompt(bbox, page_context)
            
            # è°ƒç”¨GPT-4 Vision API (æ–°ç‰ˆæœ¬)
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_data}"
                            }
                        }
                    ]
                }],
                max_tokens=500
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"GPTåŒºåŸŸæè¿°å¤±è´¥: {e}")
            return None
    
    def _build_gpt_region_description_prompt(self, bbox: List[float], page_context: str = "") -> str:
        """æ„å»ºGPTåŸºäºåæ ‡åŒºåŸŸçš„å›¾åƒæè¿°æç¤ºè¯"""
        x1, y1, x2, y2 = bbox
        
        base_prompt = f"""Please analyze the content within the specified rectangular region in this document page image.

**Target Region Coordinates:**
- Top-left: ({x1:.1f}, {y1:.1f})  
- Bottom-right: ({x2:.1f}, {y2:.1f})

**Instructions:**
1. Focus specifically on the content within these coordinates
2. Describe what you see in this rectangular area
3. Consider how this region relates to the surrounding page content
4. Provide a structured analysis

**Output Format:**
1. **Summary**: One sentence describing what's in the target region
2. **Content Details**: Bullet points of specific elements within the coordinates
3. **Visual Features**: Colors, layout, text, graphics within the region
4. **Context Integration**: How this region fits with the overall page layout
5. **Purpose**: The likely function or meaning of this content

Focus only on the specified coordinate region, but use the surrounding context to better understand its purpose."""
        
        if page_context.strip():
            context_prompt = f"""

**Document Context:**
{page_context[:1000]}

Use this context to better understand the purpose and meaning of the content in the target region."""
            return base_prompt + context_prompt
        
        return base_prompt
    
    def describe_region(self, full_page_image_path: str, bbox: List[float], 
                       page_context: str = "") -> Optional[str]:
        """æ ¹æ®é…ç½®çš„æ¨¡å‹æè¿°å›¾åƒåŒºåŸŸ"""
        if self.prefer_model == "gpt":
            return self.describe_region_with_gpt(full_page_image_path, bbox, page_context)
        elif self.prefer_model == "qwen2vl":
            return self.describe_region_with_qwen2vl(full_page_image_path, bbox, page_context)
        else:
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.qwen2vl_service:
            self.qwen2vl_service.unload()


class JSONToDOMProcessor:
    """å°†JSONè§£æç»“æœè½¬æ¢ä¸ºDOMæ ‘ç»“æ„çš„å¤„ç†å™¨"""
    
    def __init__(self, max_merge_chars: int = 128, 
                 openai_api_key: Optional[str] = None,
                 prefer_model: str = "qwen2vl",
                 enable_image_description: bool = True,
                 min_image_size: int = 50,
                 output_base_dir: str = "data/dom/MMLongBench-Doc"):
        self.max_merge_chars = max_merge_chars
        self.output_base_dir = output_base_dir
        self.prefer_model = prefer_model
        self.enable_image_description = enable_image_description
        self.min_image_size = min_image_size  # æœ€å°å›¾ç‰‡å°ºå¯¸ï¼ˆåƒç´ ï¼‰
        self.use_full_page_method = True  # é»˜è®¤ä½¿ç”¨æ•´é¡µ+åæ ‡æ–¹æ³•
        
        # åˆå§‹åŒ–å›¾åƒæè¿°æœåŠ¡
        self.image_service = ImageDescriptionService(
            openai_api_key, prefer_model
        ) if enable_image_description else None
        
        # å­˜å‚¨å…ƒç´ ç´¢å¼•æ˜ å°„
        self.element_by_index = {}
        self.image_output_dir = ""
        
        # ç¼“å­˜æ•´é¡µå›¾åƒ
        self.page_images_cache = {}
        
    def process_json_file(self, json_path: str, pdf_path: Optional[str] = None) -> DOMNode:
        """
        å¤„ç†å•ä¸ªJSONæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºDOMæ ‘
        
        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„
            pdf_path: å¯¹åº”çš„PDFæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå›¾åƒæå–ï¼‰
            
        Returns:
            DOMNode: æ ¹èŠ‚ç‚¹
        """
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        if 'data' not in data or 'elements' not in data['data']:
            raise ValueError("Invalid JSON structure")
            
        document_info = data['data']['document']
        elements = data['data']['elements']
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        doc_name = Path(json_path).stem
        self.image_output_dir = os.path.join(self.output_base_dir, doc_name)
        Path(self.image_output_dir).mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºå…ƒç´ ç´¢å¼•
        self.element_by_index = {elem['index']: elem for elem in elements}
        
        # åˆ›å»ºæ–‡æ¡£æ ¹èŠ‚ç‚¹
        root = DOMNode(
            tag='document',
            text='',
            attrs={'document_id': document_info.get('id', '')},
            depth=0,
            page_id=None
        )
        root.metadata.update({
            'document_name': document_info.get('name', ''),
            'page_count': document_info.get('page_count', 0),
            'source_type': 'pdf_json'
        })
        
        # æŒ‰ç…§parent_chapterå±‚æ¬¡ç»“æ„æ„å»ºDOMæ ‘
        self._build_hierarchical_dom(root, elements, pdf_path)
        
        # åˆ†é…å…¨å±€ID
        root.assign_ids(prefix=f"doc_{doc_name}")
        
        return root
    
    def _build_hierarchical_dom(self, root: DOMNode, elements: List[Dict], pdf_path: Optional[str]):
        """åŸºäºparent_chapteræ„å»ºå±‚æ¬¡åŒ–DOMç»“æ„"""
        
        # æ‰¾åˆ°æ‰€æœ‰é¡¶çº§å…ƒç´  (parent_chapter = -1)
        root_elements = [elem for elem in elements if elem.get('parent_chapter', -1) == -1]
        
        # ä¸ºæ¯ä¸ªé¡¶çº§å…ƒç´ åˆ›å»ºèŠ‚ç‚¹å¹¶é€’å½’æ„å»ºå­æ ‘
        for elem in sorted(root_elements, key=lambda x: x.get('index', 0)):
            element_node = self._create_element_node(elem, pdf_path, depth=1)
            if element_node:
                root.add_child(element_node)
                # é€’å½’æ·»åŠ å­å…ƒç´ 
                self._add_child_elements(element_node, elem['index'], elements, pdf_path, depth=2)
    
    def _add_child_elements(self, parent_node: DOMNode, parent_index: int, 
                           elements: List[Dict], pdf_path: Optional[str], depth: int):
        """é€’å½’æ·»åŠ å­å…ƒç´  - æ”¹è¿›ç‰ˆæœ¬"""
        
        # æ‰¾åˆ°æ‰€æœ‰å­å…ƒç´ 
        child_elements = [elem for elem in elements if elem.get('parent_chapter', -1) == parent_index]
        if not child_elements:
            return
            
        # æŒ‰indexæ’åº
        child_elements = sorted(child_elements, key=lambda x: x.get('index', 0))
        
        # ğŸ”§ æ–°çš„åˆ†ç»„ç­–ç•¥ï¼šè¯†åˆ«è¯­ä¹‰ç›¸å…³çš„è¿ç»­å…ƒç´ ç»„
        element_groups = self._group_semantically_related_elements(child_elements)
        
        # å¤„ç†æ¯ä¸ªç»„
        for group in element_groups:
            if self._should_merge_elements(group):
                # åˆå¹¶è¿™ä¸ªç»„
                merged_node = self._create_merged_text_node(group, depth)
                if merged_node:
                    parent_node.add_child(merged_node)
                    # å¯¹äºåˆå¹¶çš„èŠ‚ç‚¹ï¼Œä¸éœ€è¦è¿›ä¸€æ­¥é€’å½’ï¼Œå› ä¸ºå†…å®¹å·²ç»è¢«åˆå¹¶äº†
            else:
                # åˆ†åˆ«å¤„ç†ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ 
                for elem in group:
                    element_node = self._create_element_node(elem, pdf_path, depth)
                    if element_node:
                        parent_node.add_child(element_node)
                    # æ— è®ºæ˜¯å¦åˆ›å»ºäº†èŠ‚ç‚¹ï¼Œéƒ½è¦ç»§ç»­é€’å½’æŸ¥æ‰¾å­å…ƒç´ 
                    self._add_child_elements(element_node if element_node else parent_node, elem['index'], elements, pdf_path, depth + 1)
    
    def _group_semantically_related_elements(self, elements: List[Dict]) -> List[List[Dict]]:
        """ğŸ”§ å°†åˆ—è¡¨é¡¹åˆ†ç»„ï¼Œå¦‚æœç»„å¤ªå¤§åˆ™åˆ†å‰²æˆå¤šä¸ªå­ç»„"""
        if not elements:
            return []
            
        groups = []
        current_group = [elements[0]]
        
        for i in range(1, len(elements)):
            prev_elem = elements[i-1]
            curr_elem = elements[i]
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸å‰ä¸€ä¸ªå…ƒç´ åˆ†ç»„ï¼ˆè¯­ä¹‰ç›¸å…³ï¼‰
            if self._should_group_together(prev_elem, curr_elem):
                current_group.append(curr_elem)
            else:
                # å¤„ç†å½“å‰ç»„ï¼ˆå¦‚æœéœ€è¦ï¼Œåˆ†å‰²æˆå¤šä¸ªå­ç»„ï¼‰
                groups.extend(self._split_group_by_char_limit(current_group))
                current_group = [curr_elem]
        
        # å¤„ç†æœ€åä¸€ç»„
        groups.extend(self._split_group_by_char_limit(current_group))
        
        return groups
    
    def _split_group_by_char_limit(self, group: List[Dict]) -> List[List[Dict]]:
        """å°†ä¸€ä¸ªå¤§ç»„åˆ†å‰²æˆå¤šä¸ªç¬¦åˆå­—ç¬¦é™åˆ¶çš„å­ç»„"""
        if len(group) <= 1:
            return [group]
        
        # ğŸ”§ æ–°ç­–ç•¥ï¼šé¦–å…ˆåˆ†ç¦»åˆ—è¡¨é¡¹å’Œéåˆ—è¡¨é¡¹
        list_markers = ['â€¢', 'Â·', '-', '*', 'â—‹', 'â—¦']
        list_items = []
        non_list_items = []
        
        for elem in group:
            text = elem.get('text', '').strip()
            is_list_item = (
                any(text.startswith(marker) for marker in list_markers) or 
                text.startswith(tuple('123456789'))
            )
            
            if is_list_item:
                list_items.append(elem)
            else:
                non_list_items.append(elem)
        
        result_groups = []
        
        # éåˆ—è¡¨é¡¹ä¿æŒå•ç‹¬
        for elem in non_list_items:
            result_groups.append([elem])
        
        # åˆ—è¡¨é¡¹æŒ‰å­—ç¬¦é™åˆ¶åˆ†ç»„
        if list_items:
            current_subgroup = []
            current_chars = 0
            
            for elem in list_items:
                elem_chars = len(elem.get('text', ''))
                
                if current_chars + elem_chars <= self.max_merge_chars and current_subgroup:
                    current_subgroup.append(elem)
                    current_chars += elem_chars
                else:
                    # å¼€å§‹æ–°çš„å­ç»„
                    if current_subgroup:
                        result_groups.append(current_subgroup)
                    current_subgroup = [elem]
                    current_chars = elem_chars
            
            # æ·»åŠ æœ€åä¸€ä¸ªå­ç»„
            if current_subgroup:
                result_groups.append(current_subgroup)
        
        return result_groups
    
    def _should_group_together(self, elem1: Dict, elem2: Dict) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå…ƒç´ æ˜¯å¦åº”è¯¥åˆ†ç»„"""
        # å¿…é¡»éƒ½æ˜¯æ®µè½ç±»å‹
        if elem1.get('type') != 'paragraph' or elem2.get('type') != 'paragraph':
            return False
            
        text1 = elem1.get('text', '').strip()
        text2 = elem2.get('text', '').strip()
        
        # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯åˆ—è¡¨é¡¹
        list_markers = ['â€¢', 'Â·', '-', '*', 'â—‹', 'â—¦']
        is_list1 = any(text1.startswith(marker) for marker in list_markers) or text1.startswith(tuple('123456789'))
        is_list2 = any(text2.startswith(marker) for marker in list_markers) or text2.startswith(tuple('123456789'))
        
        if is_list1 and is_list2:
            # ä¸¤ä¸ªéƒ½æ˜¯åˆ—è¡¨é¡¹ï¼Œæ£€æŸ¥ç©ºé—´æ˜¯å¦è¿ç»­
            return self._are_elements_spatially_close(elem1, elem2)
        
        # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯çŸ­å¥
        if len(text1) < 50 and len(text2) < 50:
            # æ£€æŸ¥ç©ºé—´æ˜¯å¦è¿ç»­
            return self._are_elements_spatially_close(elem1, elem2)
        
        return False
    
    def _are_elements_spatially_close(self, elem1: Dict, elem2: Dict) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå…ƒç´ åœ¨ç©ºé—´ä¸Šæ˜¯å¦æ¥è¿‘"""
        outline1 = elem1.get('outline', [0, 0, 0, 0])
        outline2 = elem2.get('outline', [0, 0, 0, 0])
        
        if len(outline1) < 4 or len(outline2) < 4:
            return False
            
        # æ£€æŸ¥å‚ç›´é—´è·
        bottom1 = outline1[3]
        top2 = outline2[1]
        gap = abs(top2 - bottom1)
        
        # å¦‚æœé—´è·å°äº50åƒç´ ï¼Œè®¤ä¸ºæ˜¯è¿ç»­çš„
        return gap < 50
    
    def _should_merge_elements(self, elements: List[Dict]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶å…ƒç´  - æ”¹è¿›ç‰ˆæœ¬"""
        if len(elements) <= 1:
            return False
            
        # åªåˆå¹¶æ–‡æœ¬å…ƒç´ 
        text_elements = [elem for elem in elements if elem.get('type') == 'paragraph']
        if len(text_elements) != len(elements):
            return False
            
        # æ£€æŸ¥æ€»å­—ç¬¦æ•°
        total_chars = sum(len(elem.get('text', '')) for elem in text_elements)
        if total_chars > self.max_merge_chars:
            return False
            
        # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯åˆ—è¡¨é¡¹
        list_markers = ['â€¢', 'Â·', '-', '*', 'â—‹', 'â—¦']
        all_list_items = True
        
        for elem in text_elements:
            text = elem.get('text', '').strip()
            is_list_item = (any(text.startswith(marker) for marker in list_markers) or 
                           text.startswith(tuple('123456789')))
            if not is_list_item:
                all_list_items = False
                break
        
        if all_list_items and len(text_elements) >= 2:
            return True
            
        # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯çŸ­å¥ï¼ˆç–‘ä¼¼åˆ—è¡¨é¡¹ï¼‰
        avg_chars = total_chars / len(text_elements)
        if avg_chars < 30:  # å¹³å‡æ¯ä¸ªå…ƒç´ å°‘äº30å­—ç¬¦ï¼Œå¯èƒ½æ˜¯åˆ—è¡¨é¡¹
            return True
            
        return False
    
    def _are_elements_sequential(self, elements: List[Dict]) -> bool:
        """æ£€æŸ¥å…ƒç´ æ˜¯å¦åœ¨ç©ºé—´ä¸Šè¿ç»­"""
        if len(elements) < 2:
            return False
            
        # æŒ‰Yåæ ‡æ’åº
        sorted_elements = sorted(elements, key=lambda x: x.get('outline', [0, 0, 0, 0])[1])
        
        for i in range(1, len(sorted_elements)):
            prev_outline = sorted_elements[i-1].get('outline', [0, 0, 0, 0])
            curr_outline = sorted_elements[i].get('outline', [0, 0, 0, 0])
            
            if len(prev_outline) < 4 or len(curr_outline) < 4:
                continue
                
            # æ£€æŸ¥å‚ç›´é—´è·
            prev_bottom = prev_outline[3]
            curr_top = curr_outline[1]
            gap = curr_top - prev_bottom
            
            # å¦‚æœé—´è·å¤ªå¤§ï¼Œè®¤ä¸ºä¸è¿ç»­
            if gap > 50:  # å¢åŠ åˆ°50åƒç´ 
                return False
                
        return True
    
    def _create_merged_text_node(self, elements: List[Dict], depth: int) -> DOMNode:
        """åˆ›å»ºåˆå¹¶çš„æ–‡æœ¬èŠ‚ç‚¹"""
        texts = []
        for elem in sorted(elements, key=lambda x: x.get('index', 0)):
            text = elem.get('text', '').strip()
            if text:
                texts.append(text)
        
        # ğŸ”§ æ”¹è¿›çš„åˆå¹¶ç­–ç•¥ï¼šä¿æŒåˆ—è¡¨æ ¼å¼
        if texts:
            # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯åˆ—è¡¨é¡¹
            list_markers = ['â€¢', 'Â·', '-', '*', 'â—‹', 'â—¦']
            all_list_items = all(any(text.startswith(marker) for marker in list_markers) or 
                               text.startswith(tuple('123456789')) for text in texts)
            
            if all_list_items:
                # å¯¹äºåˆ—è¡¨é¡¹ï¼Œä½¿ç”¨æ¢è¡Œåˆ†éš”è€Œä¸æ˜¯ç©ºæ ¼
                merged_text = '\n'.join(texts)
                tag = 'ul'  # ä½¿ç”¨åˆ—è¡¨æ ‡ç­¾
                attrs = {'class': 'merged-list'}
            else:
                # å¯¹äºæ™®é€šçŸ­å¥ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†éš”
                merged_text = ' '.join(texts)
                tag = 'p'
                attrs = {'class': 'merged-content'}
        else:
            merged_text = ''
            tag = 'p'
            attrs = {'class': 'merged-content'}
        
        node = DOMNode(
            tag=tag,
            text=merged_text,
            attrs=attrs,
            depth=depth
        )
        
        node.metadata.update({
            'merged_count': len(elements),
            'original_indices': [elem.get('index', -1) for elem in elements],
            'element_type': 'merged_paragraph'
        })
        
        return node
    
    def _create_element_node(self, element: Dict, pdf_path: Optional[str], depth: int) -> Optional[DOMNode]:
        """åˆ›å»ºå•ä¸ªå…ƒç´ çš„DOMèŠ‚ç‚¹"""
        element_type = element.get('type', 'unknown')
        
        if element_type == 'paragraph':
            return self._create_paragraph_node(element, depth)
        elif element_type == 'figure':
            return self._create_figure_node(element, pdf_path, depth)
        elif element_type == 'table':
            return self._create_table_node(element, pdf_path, depth)
        elif element_type in ['page_header', 'page_footer']:
            return self._create_header_footer_node(element, element_type, depth)
        else:
            return self._create_generic_node(element, depth)
    
    def _create_paragraph_node(self, element: Dict, depth: int) -> DOMNode:
        """åˆ›å»ºæ®µè½èŠ‚ç‚¹"""
        if element.get('is_chapter_title'):
            # è®¡ç®—æ ‡é¢˜å±‚çº§
            heading_level = self._calculate_heading_level(element)
            tag = f'h{heading_level}'
        else:
            tag = 'p'
        
        node = DOMNode(
            tag=tag,
            text=element.get('text', ''),
            attrs=self._extract_attrs(element),
            depth=depth
        )
        node.metadata.update(self._extract_metadata(element))
        node.metadata['heading_level'] = heading_level if element.get('is_chapter_title') else None
        return node
    
    def _calculate_heading_level(self, element: Dict) -> int:
        """è®¡ç®—æ ‡é¢˜çš„å±‚çº§ (h1-h6)"""
        # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è®¡ç®—
        if not hasattr(self, '_heading_level_cache'):
            self._heading_level_cache = {}
        
        index = element['index']
        if index in self._heading_level_cache:
            return self._heading_level_cache[index]
        
        # è¿½è¸ªparent_chapterå…³ç³»è®¡ç®—å±‚çº§
        level = 1
        current_parent = element.get('parent_chapter', -1)
        checked_indices = set()
        
        while current_parent != -1 and current_parent not in checked_indices:
            checked_indices.add(current_parent)
            
            # æ‰¾åˆ°çˆ¶å…ƒç´ 
            parent_elem = self.element_by_index.get(current_parent)
            if parent_elem and parent_elem.get('is_chapter_title', False):
                level += 1
                current_parent = parent_elem.get('parent_chapter', -1)
            else:
                break
        
        # é™åˆ¶åœ¨h1-h6èŒƒå›´å†…
        level = min(level, 6)
        
        # ç¼“å­˜ç»“æœ
        self._heading_level_cache[index] = level
        return level
    
    def _create_figure_node(self, element: Dict, pdf_path: Optional[str], depth: int) -> DOMNode:
        """åˆ›å»ºå›¾ç‰‡èŠ‚ç‚¹"""
        node = DOMNode(
            tag='figure',
            text=element.get('text', ''),
            attrs=self._extract_attrs(element),
            depth=depth
        )
        node.metadata.update(self._extract_metadata(element))
        
        # æå–å›¾ç‰‡
        if pdf_path and os.path.exists(pdf_path):
            image_path = self._extract_image_from_pdf(element, pdf_path, 'figure')
            if image_path:
                # è®¾ç½®srcå±æ€§
                relative_path = os.path.relpath(image_path, self.output_base_dir)
                node.attrs['src'] = relative_path
                node.metadata['image_extracted'] = True
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå›¾åƒæè¿°
                should_describe = self._should_describe_image(element, image_path)
                description = None
                
                if should_describe:
                    # è·å–é¡µé¢ä¸Šä¸‹æ–‡ç”¨äºå›¾åƒæè¿°
                    page_context = self._get_page_context(element)
                    
                    # ä½¿ç”¨æ•´é¡µ+åæ ‡æ–¹æ³•
                    page_num = element.get('page', 0)
                    full_page_image = self._get_full_page_image(pdf_path, page_num)
                    
                    if full_page_image and 'outline' in element:
                        bbox = element['outline']  # [x1, y1, x2, y2]
                        description = self.image_service.describe_region(
                            full_page_image, bbox, page_context
                        )
                        node.metadata['description_method'] = f'{self.prefer_model}_full_page_region'
                    else:
                        description = None
                        node.metadata['description_method'] = 'failed_no_full_page'
                    
                    if description:
                        node.image_context['caption'] = description
                        node.metadata['ai_description'] = description
                    else:
                        node.metadata['ai_description'] = 'Failed to generate description'
                else:
                    node.metadata['ai_description'] = 'Skipped (disabled or too small)'
                
                # åˆ›å»ºimgå­èŠ‚ç‚¹
                img_node = DOMNode(
                    tag='img',
                    text='',
                    attrs={
                        'src': relative_path, 
                        'alt': description or element.get('text', '') or 'Image',
                        'title': element.get('text', '')
                    },
                    depth=depth + 1
                )
                node.add_child(img_node)
        
        return node
    
    def _create_table_node(self, element: Dict, pdf_path: Optional[str], depth: int) -> DOMNode:
        """åˆ›å»ºè¡¨æ ¼èŠ‚ç‚¹"""
        node = DOMNode(
            tag='table',
            text='',
            attrs=self._extract_attrs(element),
            depth=depth
        )
        node.metadata.update(self._extract_metadata(element))
        
        # å¤„ç†è¡¨æ ¼å†…å®¹
        cells = element.get('cells', {})
        if cells:
            self._build_table_structure(node, cells, depth + 1)
        
        # æå–è¡¨æ ¼å›¾ç‰‡
        if pdf_path and os.path.exists(pdf_path):
            table_image_path = self._extract_image_from_pdf(element, pdf_path, 'table')
            if table_image_path:
                relative_path = os.path.relpath(table_image_path, self.output_base_dir)
                node.attrs['src'] = relative_path
                node.metadata['table_image_extracted'] = True
        
        return node
    
    def _create_header_footer_node(self, element: Dict, element_type: str, depth: int) -> DOMNode:
        """åˆ›å»ºé¡µçœ‰/é¡µè„šèŠ‚ç‚¹"""
        tag = 'header' if element_type == 'page_header' else 'footer'
        node = DOMNode(
            tag=tag,
            text=element.get('text', ''),
            attrs=self._extract_attrs(element),
            depth=depth
        )
        node.metadata.update(self._extract_metadata(element))
        return node
    
    def _create_generic_node(self, element: Dict, depth: int) -> DOMNode:
        """åˆ›å»ºé€šç”¨èŠ‚ç‚¹"""
        node = DOMNode(
            tag='div',
            text=element.get('text', ''),
            attrs=self._extract_attrs(element),
            depth=depth
        )
        node.metadata.update(self._extract_metadata(element))
        return node
    
    def _build_table_structure(self, table_node: DOMNode, cells: Dict, depth: int):
        """æ„å»ºè¡¨æ ¼çš„DOMç»“æ„"""
        rows = {}
        for cell_key, cell_data in cells.items():
            if '_' in cell_key:
                row_idx, col_idx = map(int, cell_key.split('_'))
                if row_idx not in rows:
                    rows[row_idx] = {}
                rows[row_idx][col_idx] = cell_data.get('text', '')
        
        for row_idx in sorted(rows.keys()):
            tr_node = DOMNode(tag='tr', text='', depth=depth)
            
            for col_idx in sorted(rows[row_idx].keys()):
                td_node = DOMNode(
                    tag='td',
                    text=rows[row_idx][col_idx],
                    depth=depth + 1
                )
                tr_node.add_child(td_node)
                
            table_node.add_child(tr_node)
    
    def _get_page_context(self, element: Dict) -> str:
        """è·å–å…ƒç´ æ‰€åœ¨é¡µé¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        page_num = element.get('page', 0)
        page_elements = [elem for elem in self.element_by_index.values() 
                        if elem.get('page') == page_num and elem.get('type') == 'paragraph']
        
        # æå–é¡µé¢ä¸Šçš„æ–‡æœ¬å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
        context_texts = []
        for elem in sorted(page_elements, key=lambda x: x.get('index', 0)):
            text = elem.get('text', '').strip()
            if text and len(text) > 10:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ–‡æœ¬
                context_texts.append(text)
        
        return ' '.join(context_texts[:5])  # æœ€å¤šå–å‰5ä¸ªæ–‡æœ¬æ®µè½
    
    def _get_full_page_image(self, pdf_path: str, page_num: int) -> Optional[str]:
        """ç”Ÿæˆå¹¶ç¼“å­˜æ•´é¡µå›¾åƒ"""
        cache_key = f"{pdf_path}_{page_num}"
        
        if cache_key in self.page_images_cache:
            return self.page_images_cache[cache_key]
        
        try:
            doc = fitz.open(pdf_path)
            page = doc[page_num]
            
            # é«˜è´¨é‡æ¸²æŸ“æ•´é¡µ
            mat = fitz.Matrix(2.0, 2.0)
            pix = page.get_pixmap(matrix=mat)
            
            # ä¿å­˜æ•´é¡µå›¾åƒ
            filename = f"page_{page_num+1}_full.png"
            filepath = os.path.join(self.image_output_dir, filename)
            
            pix.save(filepath)
            doc.close()
            
            # ç¼“å­˜è·¯å¾„
            self.page_images_cache[cache_key] = filepath
            return filepath
            
        except Exception as e:
            print(f"æ•´é¡µå›¾åƒç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _extract_image_from_pdf(self, element: Dict, pdf_path: str, prefix: str) -> Optional[str]:
        """ä»PDFä¸­æå–å›¾ç‰‡"""
        try:
            outline = element.get('outline', [])
            if len(outline) < 4:
                return None
                
            doc = fitz.open(pdf_path)
            page_num = element.get('page', 0)
            page = doc[page_num]
            
            x1, y1, x2, y2 = outline
            rect = fitz.Rect(x1, y1, x2, y2)
            
            # é«˜è´¨é‡æå–
            mat = fitz.Matrix(2.0, 2.0)
            pix = page.get_pixmap(matrix=mat, clip=rect)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼šé¡µé¢_ç±»å‹_ç´¢å¼•.png
            element_index = element.get('index', 0)
            filename = f"page_{page_num+1}_{prefix}_{element_index}.png"
            filepath = os.path.join(self.image_output_dir, filename)
            
            pix.save(filepath)
            doc.close()
            
            return filepath
            
        except Exception as e:
            print(f"å›¾ç‰‡æå–å¤±è´¥: {e}")
            return None
    
    def _extract_attrs(self, element: Dict) -> Dict:
        """æå–å…ƒç´ å±æ€§"""
        attrs = {}
        
        outline = element.get('outline', [])
        if len(outline) >= 4:
            attrs['data-bbox'] = f"{outline[0]},{outline[1]},{outline[2]},{outline[3]}"
        
        if element.get('rotation', 0) != 0:
            attrs['data-rotation'] = str(element['rotation'])
            
        attrs['data-page'] = str(element.get('page', 0))
        attrs['data-index'] = str(element.get('index', -1))
            
        return attrs
    
    def _extract_metadata(self, element: Dict) -> Dict:
        """æå–å…ƒç´ å…ƒæ•°æ®"""
        metadata = {
            'original_index': element.get('index', -1),
            'parent_chapter': element.get('parent_chapter', -1),
            'is_chapter_title': element.get('is_chapter_title', False),
            'element_type': element.get('type', 'unknown'),
            'page_number': element.get('page', 0)
        }
        
        if 'outline' in element:
            metadata['bbox'] = element['outline']
            
        return metadata
    
    def _should_describe_image(self, element: Dict, image_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ºå›¾ç‰‡ç”Ÿæˆæè¿°"""
        # å¦‚æœå›¾åƒæè¿°åŠŸèƒ½è¢«ç¦ç”¨
        if not self.enable_image_description or not self.image_service:
            return False
        
        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸
        outline = element.get('outline', [])
        if len(outline) >= 4:
            width = abs(outline[2] - outline[0])
            height = abs(outline[3] - outline[1])
            
            # å¦‚æœå›¾ç‰‡å¤ªå°ï¼Œè·³è¿‡æè¿°
            if width < self.min_image_size or height < self.min_image_size:
                print(f"è·³è¿‡å°å›¾ç‰‡: {width}x{height} < {self.min_image_size}px")
                return False
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        if not os.path.exists(image_path):
            return False
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé¿å…ç©ºæ–‡ä»¶ï¼‰
        file_size = os.path.getsize(image_path)
        if file_size < 1024:  # å°äº1KB
            print(f"è·³è¿‡æ–‡ä»¶è¿‡å°çš„å›¾ç‰‡: {file_size} bytes")
            return False
        
        return True


def process_document(json_path: str, pdf_path: Optional[str] = None, 
                    openai_api_key: Optional[str] = None,
                    prefer_model: str = "qwen2vl",
                    max_merge_chars: int = 128,
                    enable_image_description: bool = True,
                    min_image_size: int = 50,
                    output_base_dir: str = "data/dom/MMLongBench-Doc") -> DOMNode:
    """å¤„ç†å•ä¸ªæ–‡æ¡£çš„ä¾¿æ·å‡½æ•°"""
    processor = JSONToDOMProcessor(
        max_merge_chars=max_merge_chars,
        openai_api_key=openai_api_key,
        prefer_model=prefer_model,
        enable_image_description=enable_image_description,
        min_image_size=min_image_size,
        output_base_dir=output_base_dir
    )
    return processor.process_json_file(json_path, pdf_path)


def batch_process_documents(json_dir: str, pdf_dir: Optional[str] = None,
                           openai_api_key: Optional[str] = None,
                           prefer_model: str = "qwen2vl",
                           max_merge_chars: int = 128,
                           output_base_dir: str = "data/dom/MMLongBench-Doc") -> Dict[str, DOMNode]:
    """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
    results = {}
    processor = JSONToDOMProcessor(
        max_merge_chars=max_merge_chars,
        openai_api_key=openai_api_key,
        prefer_model=prefer_model,
        output_base_dir=output_base_dir
    )
    
    json_files = Path(json_dir).glob("*.json")
    
    for json_file in json_files:
        json_path = str(json_file)
        pdf_path = None
        
        if pdf_dir:
            pdf_name = json_file.stem + ".pdf"
            potential_pdf_path = os.path.join(pdf_dir, pdf_name)
            if os.path.exists(potential_pdf_path):
                pdf_path = potential_pdf_path
        
        try:
            dom_tree = processor.process_json_file(json_path, pdf_path)
            results[json_file.stem] = dom_tree
            print(f"âœ… å¤„ç†å®Œæˆ: {json_file.name}")
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ {json_file.name}: {e}")
            
    return results


def process_batch_documents(json_dir: str, pdf_dir: str, base_output_dir: str,
                           mode_name: str = "æ‰¹é‡å¤„ç†",
                           enable_image_description: bool = False,
                           prefer_model: str = "qwen2vl",
                           openai_api_key: Optional[str] = None,
                           min_image_size: int = 100,
                           max_merge_chars: int = 128):
    """é€šç”¨æ‰¹é‡å¤„ç†æ–‡æ¡£å‡½æ•°"""
    print(f"=== {mode_name}æ¨¡å¼: å¤„ç†æ‰€æœ‰PDFæ–‡æ¡£ ===")
    
    # ç”Ÿæˆè¾“å‡ºç›®å½•åç§°
    output_dir_name = generate_output_dir_name(
        dataset_name="MMLongBench-Doc",
        enable_image_description=enable_image_description,
        prefer_model=prefer_model,
        min_image_size=min_image_size,
        max_merge_chars=max_merge_chars
    )
    output_dir = os.path.join(base_output_dir, output_dir_name)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = list(Path(json_dir).glob("*.json"))
    file_type = "æµ‹è¯•" if "test_data" in json_dir else ""
    print(f"å‘ç° {len(json_files)} ä¸ª{file_type}JSONæ–‡ä»¶")
    
    # é…ç½®å¤„ç†å™¨
    processor = JSONToDOMProcessor(
        max_merge_chars=max_merge_chars,
        openai_api_key=openai_api_key,
        prefer_model=prefer_model,
        enable_image_description=enable_image_description,
        min_image_size=min_image_size,
        output_base_dir=output_dir
    )
    
    processed_count = 0
    failed_count = 0
    
    for json_file in json_files:
        try:
            print(f"\nå¤„ç†: {json_file.name}")
            
            # æ„å»ºå¯¹åº”çš„PDFè·¯å¾„
            pdf_path = os.path.join(pdf_dir, json_file.stem + ".pdf")
            if not os.path.exists(pdf_path):
                print(f"  è­¦å‘Š: å¯¹åº”PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                pdf_path = None
            
            # å¤„ç†æ–‡æ¡£
            dom_tree = processor.process_json_file(str(json_file), pdf_path)
            
            # ä¿å­˜ç»“æœ
            output_path = os.path.join(output_dir, json_file.stem + ".json")
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(dom_tree.to_json_dict(), f, indent=2, ensure_ascii=False)
            
            print(f"  âœ… æˆåŠŸä¿å­˜åˆ°: {output_path}")
            processed_count += 1
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            failed_count += 1
    
    print(f"\n=== {mode_name}æ¨¡å¼å¤„ç†å®Œæˆ ===")
    print(f"æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ–‡æ¡£")
    print(f"å¤„ç†å¤±è´¥: {failed_count} ä¸ªæ–‡æ¡£")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")


def process_test_documents(enable_image_description: bool = False,
                          prefer_model: str = "qwen2vl",
                          openai_api_key: Optional[str] = None,
                          min_image_size: int = 100,
                          max_merge_chars: int = 128):
    """å¤„ç†test_dataä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
    process_batch_documents(
        json_dir="test_data/dict/MMLongBench-Doc",
        pdf_dir="test_data/doc/MMLongBench-Doc",
        base_output_dir="test_data/dom",
        mode_name="æµ‹è¯•",
        enable_image_description=enable_image_description,
        prefer_model=prefer_model,
        openai_api_key=openai_api_key,
        min_image_size=min_image_size,
        max_merge_chars=max_merge_chars
    )


def process_single_document(enable_image_description: bool = False,
                           prefer_model: str = "qwen2vl",
                           openai_api_key: Optional[str] = None,
                           min_image_size: int = 100,
                           max_merge_chars: int = 128):
    """å¤„ç†å•ä¸ªwelcome-to-nusæ–‡æ¡£"""
    json_path = "test_data/dict/MMLongBench-Doc/welcome-to-nus.json"
    pdf_path = "test_data/doc/MMLongBench-Doc/welcome-to-nus.pdf"
    
    print("=== å•æ–‡ä»¶æ¨¡å¼: å¤„ç† welcome-to-nus æ–‡æ¡£ ===")
    
    # ç”Ÿæˆè¾“å‡ºç›®å½•åç§°
    output_dir_name = generate_output_dir_name(
        dataset_name="MMLongBench-Doc",
        enable_image_description=enable_image_description,
        prefer_model=prefer_model,
        min_image_size=min_image_size,
        max_merge_chars=max_merge_chars
    )
    output_base_dir = os.path.join("test_data/dom", output_dir_name)
    
    dom_tree = process_document(
        json_path,
        pdf_path,
        openai_api_key=openai_api_key,
        prefer_model=prefer_model,
        max_merge_chars=max_merge_chars,
        enable_image_description=enable_image_description,
        min_image_size=min_image_size,
        output_base_dir=output_base_dir
    )
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    output_path = os.path.join(output_base_dir, "welcome-to-nus.json")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(dom_tree.to_json_dict(), f, indent=2, ensure_ascii=False)
        
    print(f"å•æ–‡ä»¶DOMæ ‘å·²ä¿å­˜åˆ°: {output_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    def count_stats(node, stats=None):
        if stats is None:
            stats = {'headings': {}, 'merged': 0, 'images': 0, 'described': 0, 'skipped': 0}
        
        if node.tag.startswith('h'):
            stats['headings'][node.tag] = stats['headings'].get(node.tag, 0) + 1
        elif 'merged_count' in node.metadata:
            stats['merged'] += 1
        elif node.tag == 'figure':
            stats['images'] += 1
            ai_desc = node.metadata.get('ai_description', '')
            if ai_desc and ai_desc != 'Skipped (disabled or too small)':
                stats['described'] += 1
            else:
                stats['skipped'] += 1
        
        for child in node.children:
            count_stats(child, stats)
        return stats
    
    stats = count_stats(dom_tree)
    print(f"\n=== å•æ–‡ä»¶ç»Ÿè®¡ ===")
    print(f"æ ‡é¢˜: {dict(sorted(stats['headings'].items()))}")
    print(f"åˆå¹¶èŠ‚ç‚¹: {stats['merged']}")
    print(f"å›¾ç‰‡æ€»æ•°: {stats['images']}")
    print(f"AIæè¿°: {stats['described']}")
    print(f"è·³è¿‡æè¿°: {stats['skipped']}")


def process_all_documents(enable_image_description: bool = False,
                         prefer_model: str = "qwen2vl",
                         openai_api_key: Optional[str] = None,
                         min_image_size: int = 100,
                         max_merge_chars: int = 128):
    """å¤„ç†æ‰€æœ‰æ–‡æ¡£"""
    process_batch_documents(
        json_dir="data/dict/MMLongBench-Doc",
        pdf_dir="data/doc/MMLongBench-Doc",
        base_output_dir="data/dom",
        mode_name="æ‰¹é‡å¤„ç†",
        enable_image_description=enable_image_description,
        prefer_model=prefer_model,
        openai_api_key=openai_api_key,
        min_image_size=min_image_size,
        max_merge_chars=max_merge_chars
    )


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='JSON to DOM processor')
    parser.add_argument('--mode', choices=['test', 'batch', 'single'], default='batch', 
                       help='Processing mode: test (test_data files), batch (data files), or single (welcome-to-nus only)')
    parser.add_argument('--skip-images', action='store_true', help='Skip image descriptions completely (faster)')
    parser.add_argument('--image-model', choices=['qwen2vl', 'gpt'], default='qwen2vl', 
                       help='Model to use for image descriptions (ignored if --skip-images)')
    parser.add_argument('--openai-api-key', help='OpenAI API key for GPT model')
    parser.add_argument('--min-image-size', type=int, default=150, help='Minimum image size to describe (pixels, ignored if --skip-images)')
    parser.add_argument('--max-chars', type=int, default=512, help='Max characters for text merging')
    
    args = parser.parse_args()
    
    # ç»Ÿä¸€çš„å›¾åƒæè¿°é€»è¾‘
    enable_image_description = not args.skip_images
    prefer_model = args.image_model if enable_image_description else "none"
    
    # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹å‡†å¤‡å‚æ•°
    openai_key_param = args.openai_api_key if (enable_image_description and args.image_model == 'gpt') else None
    
    print(f"=== å¤„ç†é…ç½® ===")
    print(f"å›¾åƒæè¿°: {'å¯ç”¨' if enable_image_description else 'ç¦ç”¨'}")
    if enable_image_description:
        print(f"é€‰æ‹©æ¨¡å‹: {args.image_model}")
        if args.image_model == 'qwen2vl':
            print(f"æè¿°æ–¹æ³•: æ•´é¡µå›¾åƒ+åæ ‡å®šä½")
            print(f"Qwen2VLæ¨¡å‹: {STATIC_QWEN2VL_MODEL}")
            print(f"CUDAè®¾å¤‡: {STATIC_QWEN2VL_DEVICE}")
        else:
            print(f"æè¿°æ–¹æ³•: å›¾åƒè£å‰ª")
        print(f"æœ€å°å›¾åƒå°ºå¯¸: {args.min_image_size}px")
    print(f"æ–‡æœ¬åˆå¹¶é™åˆ¶: {args.max_chars} å­—ç¬¦")
    print()
    
    if args.mode == 'test':
        process_test_documents(
            enable_image_description=enable_image_description,
            prefer_model=prefer_model,
            openai_api_key=openai_key_param,
            min_image_size=args.min_image_size,
            max_merge_chars=args.max_chars
        )
    elif args.mode == 'batch':
        process_all_documents(
            enable_image_description=enable_image_description,
            prefer_model=prefer_model,
            openai_api_key=openai_key_param,
            min_image_size=args.min_image_size,
            max_merge_chars=args.max_chars
        )
    elif args.mode == 'single':
        process_single_document(
            enable_image_description=enable_image_description,
            prefer_model=prefer_model,
            openai_api_key=openai_key_param,
            min_image_size=args.min_image_size,
            max_merge_chars=args.max_chars
        )
    else:
        print("è¯·æŒ‡å®šå¤„ç†æ¨¡å¼:")
        print("  --mode test   : å¤„ç†test_dataä¸­çš„æ‰€æœ‰æ–‡ä»¶")
        print("  --mode batch  : å¤„ç†dataä¸­çš„æ‰€æœ‰æ–‡ä»¶")  
        print("  --mode single : å¤„ç†welcome-to-nuså•ä¸ªæ–‡ä»¶")