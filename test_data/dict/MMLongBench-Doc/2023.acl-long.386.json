{
  "status": "ok",
  "data": {
    "document": {
      "id": "151fd7af-b46c-4d09-b207-9720f9b25e00",
      "name": "2023.acl-long.386.pdf",
      "package_type": "elite",
      "ocr_type": "auto",
      "page_count": 24,
      "create_time": 1752850458
    },
    "elements": [
      {
        "type": "paragraph",
        "text": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "page": 0,
        "parent_chapter": -1,
        "index": 0,
        "outline": [
          96.0,
          70.0,
          499.0,
          84.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Liangming Pan1,2 Xiaobao Wu3 Xinyuan Lu4 Anh Tuan Luu3 William Yang Wang1 Min-Yen Kan4 Preslav Nakov2 1 University of California, Santa Barbara 2 MBZUAI 3 Nanyang Technological University 4 National University of Singapore liangmingpan@ucsb.edu xiaobao002@e.ntu.edu.sg luxinyuan@u.nus.edu anhtuan.luu@ntu.edu.sg william@cs.ucsb.edu kanmy@comp.nus.edu.sg preslav.nakov@mbzuai.ac.ae",
        "page": 0,
        "parent_chapter": -1,
        "index": 1,
        "outline": [
          96.0,
          88.0,
          500.0,
          197.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Abstract",
        "page": 0,
        "parent_chapter": -1,
        "index": 2,
        "outline": [
          157.0,
          214.0,
          202.5,
          224.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (PROGRAMFC), a novel factchecking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate PROGRAMFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging.1",
        "page": 0,
        "parent_chapter": 2,
        "index": 3,
        "outline": [
          83.5,
          235.0,
          275.5,
          499.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "1 Introduction",
        "page": 0,
        "parent_chapter": -1,
        "index": 4,
        "outline": [
          70.0,
          508.5,
          153.5,
          518.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The proliferation of disinformation, e.g., in social media, has made automated fact-checking a crucial application of natural language processing (NLP). Given a claim, the goal is to find evidence and then to make a verdict about the claim’s veracity based on that evidence (Thorne and Vlachos, 2018;Glockner et al., 2022; Guo et al., 2022).",
        "page": 0,
        "parent_chapter": 4,
        "index": 5,
        "outline": [
          69.0,
          529.5,
          291.0,
          622.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Evaluating the veracity of real-world claims often involves collecting multiple pieces of evidence and applying complex reasoning (Jiang et al., 2020;Nguyen et al., 2020; Aly and Vlachos, 2022; Chen et al., 2022a). For instance, consider the claim “Both James Cameron and the director of the film Interstellar were born in Canada”. It may be challenging to find direct evidence on the web that refutes or supports this claim.",
        "page": 0,
        "parent_chapter": 4,
        "index": 6,
        "outline": [
          67.0,
          624.0,
          292.0,
          746.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Instead, a human fact-checker needs to decompose the claim, gather multiple pieces of evidence, and perform step-by-step reasoning (Nakov et al., 2021a), as illustrated in Figure 1. This makes verifying complex claims much more challenging than the typical setting explored in previous work, where information from a single article is sufficient to support/refute the claim (Thorne et al., 2018; Saakyan et al., 2021; Schuster et al., 2021; Pan et al., 2021;Wadden et al., 2022a; Krishna et al., 2022).",
        "page": 0,
        "parent_chapter": 4,
        "index": 8,
        "outline": [
          304.0,
          213.0,
          527.0,
          349.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Besides multi-step reasoning, we still need to consider two key aspects for developing a reliable fact-checking system: (i) Explanability: The model should not only predict the veracity of the claim, but it should also provide a clear explanation of its reasoning process to help users understand and trust the results. (ii) Data efficiency: Human annotation is often time-consuming, costly, and potentially biased, making it difficult to collect sufficient highquality labeled data for model training, particularly for complex claims. Therefore, it is desirable to build a model that can perform well with minimal or no training data. Despite a few models (Zhou et al., 2019; Zhong et al., 2020; Aly and Vlachos, 2022) being proposed to facilitate multi-step reasoning in fact-checking, they either lack explainability in their reasoning process or require a large number of task-specific training examples.",
        "page": 0,
        "parent_chapter": 4,
        "index": 9,
        "outline": [
          303.0,
          351.0,
          528.0,
          596.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "In this paper, we present Program-Guided FactChecking (PROGRAMFC), a novel fact-checking framework that is both explanatory and dataefficient. Figure 1 illustrates our approach. To verify complex claims, PROGRAMFC decomposes them into simpler sub-tasks that can be solved using a shared library of specialized sub-task functions. To be specific, PROGRAMFC begins by generating a reasoning program for the input claim, which is a sequence of sub-tasks (e.g., S1-S4 in Figure 1) in the form of ACTION[ARGUMENT],where ACTION and ARGUMENT define the type and the content of the sub-task, respectively.",
        "page": 0,
        "parent_chapter": 4,
        "index": 10,
        "outline": [
          303.0,
          597.5,
          527.0,
          775.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6981 Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 6981–7004 July 9-14, 2023 ©2023 Association for Computational Linguistics",
        "page": 0,
        "parent_chapter": 4,
        "index": 11,
        "outline": [
          136.0,
          782.5,
          458.0,
          829.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "figure",
        "text": "Claim: Both James Cameron and the director of the film Interstellar were born in Canada. Knowledge Reasoning ProgramFunctionsSource Language  ModelsS1Verify [James Cameron was born in Canada.]Fact  (Codex, GPT3, …)FACT_1= TRUEChecker   Gold Evidence S2Question [Who is the director of the film Interstellar?]QA  ANSWER_1 = Christopher NolanModel S3Verify [ {ANSWER_1} was born in Canada.]Fact Open-book Claim: ⋯FACT_2 = FALSEChecker ProClgariamm: : ⋯⋯ProC Plgariamm: : ⋯⋯S4Predict [ {FACT_1} AND {FACT_2}]Logical Closed-bookrogram: ⋯Reasoner ExemplarsPREDICTED_LABEL = REFUTES",
        "page": 1,
        "parent_chapter": 4,
        "index": 12,
        "outline": [
          69.0,
          69.0,
          525.0,
          269.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Figure 1: Overview of our PROGRAMFC model, which consists of two modules: (i) Program Generation generates a reasoning program for the input claim using Codex with in-context learning, and then (ii) Program Execution sequentially interprets the program by delegating each step to the corresponding sub-task function.",
        "page": 1,
        "parent_chapter": 4,
        "index": 13,
        "outline": [
          69.0,
          279.0,
          525.0,
          312.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The generated reasoning program serves as a step-by-step guide for verifying the claim. We then execute the program by sequentially delegating each sub-task to the corresponding sub-task handler, as shown in the functions columns in Figure 1. These sub-tasks may include answering questions, verifying simple claims, or conducting logical reasoning.",
        "page": 1,
        "parent_chapter": 4,
        "index": 14,
        "outline": [
          69.0,
          335.5,
          292.0,
          443.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "PROGRAMFC combines explainability with data efficiency. It uses reasoning programs to provide clear explanations of its reasoning process. For data efficiency, Large Language Models (LLMs) can solve various tasks given only a few examples as prompts, e.g., in-context learning (Brown et al., 2020). We leverage this ability of LLMs to generate reasoning programs for a given claim by showing the modelj ust a few dozen of (claim, program) pairs as demonstrations. PROGRAMFC is also flexible as it allows for easy swapping of subtask function implementations to work under different settings of fact-checking, without affecting the rest of the system. We can allow the functions to retrieve information from external sources (in an open-book setting) or we can ask them to generate answers based solely on the LLM’s internal parametric knowledge (in a closed-book setting).",
        "page": 1,
        "parent_chapter": 4,
        "index": 15,
        "outline": [
          67.0,
          445.0,
          293.0,
          690.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We evaluate PROGRAMFC on two challenging datasets designed for fact-checking complex claims: HOVER (Jiang et al., 2020) and FEVEROUS (Aly et al., 2021), and we show that it outperforms seven few-shot fact-checking baselines on both datasets (§ 4.1).",
        "page": 1,
        "parent_chapter": 4,
        "index": 16,
        "outline": [
          69.0,
          693.5,
          292.0,
          773.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The strategy of program-guided reasoning becomes increasingly effective as the required reasoning depth increases (§ 4.1). In the open-domain setting, we find that reasoning programs can enhance the retrieval of relevant evidence from knowledge sources (§ 4.2). Moreover, PROGRAMFC is robust even when we use weak models as sub-task solvers(§ 4.2). We also evaluate the interpretability of the reasoning programs through human evaluation and error analysis (§ 4.3).",
        "page": 1,
        "parent_chapter": 4,
        "index": 17,
        "outline": [
          303.0,
          335.5,
          527.0,
          470.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "2 Related Work",
        "page": 1,
        "parent_chapter": -1,
        "index": 18,
        "outline": [
          305.5,
          482.0,
          395.5,
          492.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Fact-Checking. Automated fact-checking has gained significant attention in the NLP research community in recent years as a means of combating misinformation and disinformation. Various datasets have been proposed that enable the development and the evaluation of systems for automatic fact-checking, the most popular ones being based on human-crafted claims from Wikipedia content (Thorne et al., 2018; Sathe et al., 2020; Schuster et al., 2021) and naturally occurring claims in the political or in the scientific domain (Wang, 2017; Nakov et al., 2021b, 2022; Augenstein et al., 2019; Saakyan et al., 2021; Gupta and Srikumar, 2021; Wadden et al., 2020, 2022a). Notably, most of these datasets are constructed in a way that the evidence to support or to refute a claim can be found in a single document. For example, in FEVER (Thorne et al., 2018), more than 87% of the claims only require information from a single Wikipedia article (Jiang et al., 2020).",
        "page": 1,
        "parent_chapter": 18,
        "index": 19,
        "outline": [
          302.0,
          502.0,
          527.0,
          774.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6982",
        "page": 1,
        "parent_chapter": 18,
        "index": 20,
        "outline": [
          285.5,
          781.0,
          310.0,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "To bridge this gap, datasets have been proposed to study fact-checking complex claims that require multi-step reasoning (Jiang et al., 2020; Aly et al., 2021). Graph-based models (Zhou et al., 2019;Liu et al., 2020; Zhong et al., 2020; Nguyen et al., 2020; Barnabò et al., 2022, 2023) are used to facilitate the reasoning over multiple pieces of evidence. Although such models achieve sizable performance gains, they lack explanability and thet rely on large amounts of training data. To address the above problems, we propose an explainable, flexible, and data-efficient model that generates reasoning graphs as explanations and utilizes incontext learning to enable few-shot learning.",
        "page": 2,
        "parent_chapter": 18,
        "index": 21,
        "outline": [
          67.0,
          70.5,
          292.0,
          262.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Explanation Generation. Facing the complexities of real-world claims, simply giving a final veracity to a claim often fails to be persuasive (Guo et al., 2022). Previous research has proposed various approaches to provide post-hoc explanations for model predictions, such as using attention weights to highlight relevant parts of the evidence (Popat et al., 2017; Cui et al., 2019; Yang et al., 2019; Lu and Li, 2020), generatingj ustifications with logic-based systems based on knowledge graphs (Gad-Elrab et al., 2019; Ahmadi et al., 2019), and generating a summary of the retrieved relevant evidence (Atanasova et al., 2020; Kotonya and Toni, 2020; Jolly et al., 2022). In contrast, we propose to use reasoning programs to provide explanations that consist of sub-tasks described in a program-like natural language. This offers several advantages: it allows for explanations that are not confined to the evidence, like attention weights, it is more flexible than logic-based explanations, and it is more concise than free-form summarization.",
        "page": 2,
        "parent_chapter": 18,
        "index": 22,
        "outline": [
          67.0,
          273.0,
          293.0,
          557.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Chain-of-Thought Reasoning. Moreover, unlike previous work that generates post-hoc explanations, we also use reasoning programs as guidance for predicting the veracity of the claim. This is motivated by the recent success of chain-of-thought prompting (CoT) (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022), which generates step-bystep natural language reasoning steps to guide the model in answering complex questions. We adopt this idea to fact-checking complex claims. Unlike the original CoT, which uses a single LLM for both decomposition and question answering, we use the language model only to generate reasoning programs as the blueprint for problem-solving, and we delegate each sub-task to specialized functions.",
        "page": 2,
        "parent_chapter": 18,
        "index": 23,
        "outline": [
          67.0,
          569.0,
          292.0,
          774.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "This approach reduces the burden on the language model and allows for more flexibility in incorporating necessary components for factchecking such as an evidence retriever. The strategy of program-guided reasoning is also in line with the recent trend of tool-augmented language models (Mialon et al., 2023; Schick et al., 2023), i.e., augmenting language models with access to external tools and resources.",
        "page": 2,
        "parent_chapter": 18,
        "index": 24,
        "outline": [
          304.0,
          72.5,
          527.0,
          191.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "3 PROGRAMFC",
        "page": 2,
        "parent_chapter": -1,
        "index": 25,
        "outline": [
          305.0,
          204.0,
          396.5,
          214.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We first formulate the problem of fact-checking and then we introduce our proposed model for ProgramGuided Fact-Checking (PROGRAMFC).",
        "page": 2,
        "parent_chapter": 25,
        "index": 26,
        "outline": [
          304.0,
          225.0,
          525.0,
          264.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "3.1 Problem Formulation",
        "page": 2,
        "parent_chapter": 25,
        "index": 27,
        "outline": [
          305.0,
          274.5,
          431.5,
          284.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Given a claim \\(C,\\), a fact-checking model \\(\\mathcal{F}\\) aims to predict a label \\(Y\\) to evaluate the claim as TRUE or FALSE, based on a knowledge source \\(\\kappa\\). The model is also required to output an explanation \\(E\\) toj ustify the predicted veracity label. We summarize three different settings of fact-checking depending on the type of knowledge source \\(\\kappa\\).",
        "page": 2,
        "parent_chapter": 27,
        "index": 28,
        "outline": [
          304.0,
          291.0,
          526.0,
          386.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\\(\\bullet\\) Gold evidence: For each claim, \\(\\kappa\\) is the set of gold evidence documents that can support or refute the claim. This setting is also called claim verification (Pan et al., 2021; Wright et al., 2022).",
        "page": 2,
        "parent_chapter": 27,
        "index": 29,
        "outline": [
          305.0,
          388.0,
          526.0,
          442.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\\(\\bullet\\) Open-book setting: \\(\\kappa\\) is a large textual corpus such as Wikipedia. The model first retrieves relevant evidence from the corpus and then predicts the veracity label based on the evidence (Jiang et al., 2021; Wadden et al., 2022b).",
        "page": 2,
        "parent_chapter": 27,
        "index": 30,
        "outline": [
          304.0,
          440.5,
          526.0,
          509.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\\(\\bullet\\) Closed-book setting: The model does not have access to any external knowledge source (\\(\\kappa=\\emptyset\\)).It needs to leverage the knowledge stored in its parameters (acquired during pre-training and finetuning) to verify the claim. This setting was explored in work that applies large language models for fact-checking (Lee et al., 2020, 2021).",
        "page": 2,
        "parent_chapter": 27,
        "index": 31,
        "outline": [
          304.0,
          511.0,
          527.0,
          604.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "3.2 Program-Guided Reasoning",
        "page": 2,
        "parent_chapter": 25,
        "index": 32,
        "outline": [
          305.0,
          614.5,
          462.5,
          627.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Our goal is to fact-check a complex claim \\(C\\) that requires multi-step reasoning. We focus on the fewshot setting, where only a small set of in-domain examples are available to teach the model. To solve this, PROGRAMFC follows a program generationand-execution paradigm, as shown in Figure 1.",
        "page": 2,
        "parent_chapter": 32,
        "index": 33,
        "outline": [
          305.0,
          631.0,
          526.0,
          712.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Program Generation. At this stage, given the input claim \\(C.\\), a planner \\(\\mathcal{P}\\) generates a reasoning program \\(P=[S_{1},\\cdots,S_{n}]\\) for it, which consists of \\(n\\) sequentially ordered reasoning steps \\(S_{i}\\).",
        "page": 2,
        "parent_chapter": 32,
        "index": 34,
        "outline": [
          304.0,
          720.5,
          525.0,
          773.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6983",
        "page": 2,
        "parent_chapter": 32,
        "index": 35,
        "outline": [
          285.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Each reasoning step \\(S_{i}\\in P\\) is an instruction in controlled natural language that directs \\(S_{i}\\) to a function in an auxiliary set of sub-task functions \\(\\mathcal{F}\\) available to the system. To be specific, we define \\(S_{i}~=~(f_{i},A_{i},V_{i})\\), where \\(f_{i}\\) specifies the sub-task function \\(f_{i}\\in\\mathcal{F}\\), \\(A_{i}\\) is the argument passed to the function \\(f_{i}\\), and \\(V_{i}\\) is the variable that stores the returned result from the function call \\(f_{i}(A_{i})\\). For a valid reasoning program, the return value of the last reasoning step must be a Boolean value indicating the veracity label of the claim \\(C\\), i.e., \\(V_{n}\\in\\{\\mathrm{TRUE},\\mathrm{FALSE}\\}\\).",
        "page": 3,
        "parent_chapter": 32,
        "index": 36,
        "outline": [
          67.0,
          71.5,
          292.0,
          236.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Program Execution. In the execution stage, the reasoning program \\(P\\) is run by an interpreter to derive the veracity label of the claim \\(C\\). The interpreter sequentially parses the reasoning steps in \\(P.\\). For each step \\(S_{i}=(f_{i},A_{i},V_{i})\\), it calls the corresponding off-the-shelf sub-task function \\(f_{i}\\) and passes the argument \\(A_{i}\\) to it. The argument \\(A_{i}\\) is either a logical expression or a natural language sentence, e.g., a question or a simple claim. The result of the function call is then stored in the variable \\(V_{i}\\). As it is common for a subsequent step to depend on the results from previous steps, we allow the argument \\(A_{i}\\) to refer to variables \\(V_{1},\\cdots,V_{i-1}\\) in previous steps. For example, in Figure 1, the argument in \\(S_{3}\\) is “{ANSWER_1} was born in Canada.”, which refers to the return variable {ANSWER_1}from \\(S_{2}\\). When executing \\(S_{3}\\), the variable is replaced by its actual value, and the argument becomes “Christopher Nolan was born in Canada”. After executing the last step, the return value is the predicted veracity of the claim \\(C\\).",
        "page": 3,
        "parent_chapter": 32,
        "index": 37,
        "outline": [
          66.0,
          239.5,
          294.0,
          527.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Aggregating Reasoning Paths. Note that there might be multiple reasoning paths that can reach the final veracity label. Therefore, we generate a diverse set of \\(^{\\cdot}N\\) candidate reasoning programs \\(\\mathcal{P}=\\{P_{1},\\cdots,P_{N}\\}\\) for the input claim. After executing all programs in \\(\\mathcal{P},\\), we take the majority vote over all \\(N\\) predicted labels as the final label. This approach is similar to how humans rely on multiple methods of validation to increase their confidence in fact-checking. It also makes the model less susceptible to errors in individual reasoning programs.",
        "page": 3,
        "parent_chapter": 32,
        "index": 38,
        "outline": [
          67.0,
          531.5,
          292.0,
          682.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "3.3 Reasoning Program Generation",
        "page": 3,
        "parent_chapter": 25,
        "index": 39,
        "outline": [
          69.0,
          689.0,
          244.0,
          702.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We base our program generator on Codex (Chen et al., 2021), a code-pretrained LLM, which can parse natural language into symbolic representations such as SQL (Cheng et al., 2022) or Python programs (Gao et al., 2022; Chen et al., 2022b).",
        "page": 3,
        "parent_chapter": 39,
        "index": 40,
        "outline": [
          69.0,
          707.5,
          291.0,
          772.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "However, the grammar of a reasoning program is different from the grammar of a programming language. We take advantage of Codex’s few-shot generalization ability and we find that it can learn effectively from only a small number of in-context examples \\(D=\\{d_{1},\\cdots,d_{|D|}\\}\\). Each example \\(d_{i}\\) consists of a claim and a program. The program has a Python-like grammar, where each reasoning step is written in the format \\(V_{i}=f_{i}(A_{i})\\). At inference time, we prompt Codex with an instruction of the task, \\(K\\) in-context examples, and the input claim \\(C\\). Codex then attempts to complete the following texts, and thereby generates a program for \\(C\\). The prompt template is shown in Figure 2. We use \\(K=20\\) to maintain a tradeoff between the diversity of reasoning types and the model’s maximum input capacity. We use sampling-based decoding (temperature of 0.7) to generate different reasoning programs for multiple runs.",
        "page": 3,
        "parent_chapter": 39,
        "index": 41,
        "outline": [
          301.0,
          70.5,
          529.0,
          331.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "3.4 Sub-Task Functions",
        "page": 3,
        "parent_chapter": 25,
        "index": 42,
        "outline": [
          305.5,
          338.0,
          422.5,
          347.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We implement three sub-task functions for the model to call during the program execution.",
        "page": 3,
        "parent_chapter": 42,
        "index": 43,
        "outline": [
          304.0,
          354.5,
          525.0,
          381.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\\(\\bullet\\) QUESTION: This sub-task function is a questionanswering module that takes a question \\(Q\\) as the input argument and returns the answer \\(A\\) to the question. We use FLAN-T5 (Chung et al., 2022), an improved T5 model (Raffel et al., 2020) pretrained on more than 1.8K tasks with instruction tuning, which has achieved state-of-the-art zero/few-shot performance on many QA benchmarks. As shown in Figure 3, we prompt the model differently depending on the settings defined in Section 3.1. For the closed-book setting, the input prompt is",
        "page": 3,
        "parent_chapter": 42,
        "index": 44,
        "outline": [
          302.0,
          384.5,
          528.0,
          534.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Q: QUESTION ? The answer is:",
        "page": 3,
        "parent_chapter": 42,
        "index": 45,
        "outline": [
          325.5,
          537.0,
          484.5,
          551.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "For the other two settings, the input prompt is",
        "page": 3,
        "parent_chapter": 42,
        "index": 46,
        "outline": [
          304.0,
          551.5,
          507.0,
          569.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "EVIDENCE Q: QUESTION ?\nThe answer is:",
        "page": 3,
        "parent_chapter": 42,
        "index": 47,
        "outline": [
          327.5,
          571.5,
          454.5,
          597.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\\(\\bullet\\) VERIFY: This is a fact verification module that takes a claim \\(C\\) as the input argument and returns a label of either TRUE or FALSE. We also use FLAN-T5 for this module, by prompting the model with the following question-answering format.",
        "page": 3,
        "parent_chapter": 42,
        "index": 48,
        "outline": [
          304.0,
          604.0,
          525.0,
          671.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "EVIDENCE",
        "page": 3,
        "parent_chapter": 42,
        "index": 49,
        "outline": [
          329.0,
          676.5,
          375.5,
          686.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Q: Is it true that CLAIM ?\nTrue or False? The answer is:",
        "page": 3,
        "parent_chapter": 42,
        "index": 50,
        "outline": [
          325.5,
          684.5,
          485.5,
          714.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\\(\\bullet\\) PREDICT: This module takes as input a logical expression that performs AND, OR, NOT operations over the variables in the previous steps. Its output is returned as the predicted veracity label.",
        "page": 3,
        "parent_chapter": 42,
        "index": 51,
        "outline": [
          305.0,
          721.0,
          524.5,
          772.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6984",
        "page": 3,
        "parent_chapter": 42,
        "index": 52,
        "outline": [
          286.5,
          781.5,
          309.5,
          791.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "''' Generate a python - like program that describes the reasoning steps\nrequired to verify the claim step -by - step . You can call three functions\nin the program : 1. Question () to answer a question ; 2. Verify () to\nverify a simple claim ; 3. Predict () to predict the veracity label . '''\n# The claim is that Both James Cameron and the director of the film\nInterstellar were born in Canada .\ndef program () :\nfact_1 = Verify (\" James Cameron was born in Canada .\")\nAnswer_1 = Question (\" Who is the director of the film Interstellar ?\")\nfact_2 = Verify (\"{ Answer_1 } was born in Canada .\")\nlabel = Predict ( fact_1 and fact_2 )\n(· · · more in-context examples here · · ·)\n# The claim is that <input_claim>\ndef program () :",
        "page": 4,
        "parent_chapter": 42,
        "index": 53,
        "outline": [
          90.0,
          74.5,
          496.0,
          246.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Figure 2: The Codex prompt template used to generate reasoning programs, consisting of a task instruction, in-context examples, and a prompt for the <input_claim>. The full templates are given in Appendix D.",
        "page": 4,
        "parent_chapter": 42,
        "index": 54,
        "outline": [
          70.0,
          255.5,
          525.0,
          279.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "figure",
        "text": "<Gold Evidence>Gold EvidenceQ: <Question>The answer is: Open-book<Retrieved Evidence><Question> Q: <Question>FLAN-T5Ans The answer is: Closed-bookQ: <Question>The answer is:",
        "page": 4,
        "parent_chapter": 42,
        "index": 55,
        "outline": [
          72.5,
          294.5,
          287.5,
          382.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Figure 3: Implementation of the question-answering sub-task function for three different settings.",
        "page": 4,
        "parent_chapter": 42,
        "index": 56,
        "outline": [
          70.0,
          394.5,
          288.5,
          416.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "4 Experiments",
        "page": 4,
        "parent_chapter": -1,
        "index": 57,
        "outline": [
          69.0,
          436.0,
          154.0,
          450.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Datasets. Most fact-checking datasets consist primarily of simple claims that can be substantiated through a single piece of evidence. However, here we focus on complex claims that need multi-step reasoning. Given this context, we opt to evaluate our model on the only two datasets that, to the best of our knowledge, fulfill these criteria: HOVER (Jiang et al., 2020) and FEVEROUS (Aly et al., 2021). We use the validation sets for evaluation since the test sets are not publicly released. HOVER contains claims that require integration and reasoning over multiple Wikipedia articles. We divide its validation set into three subsets based on the number of “hops” required to verify the claim: 1,126 two-hop claims, 1,835 three-hop claims, and 1,039 four-hop claims. FEVEROUS focuses on fact-checking complex claims over unstructured and structured data, where each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia. Since we focus on textual fact-checking, we only selected claims that require exclusively sentence evidence, constituting 2,962 claims. We call this subset FEVEROUS-S.",
        "page": 4,
        "parent_chapter": 57,
        "index": 58,
        "outline": [
          67.0,
          461.5,
          293.0,
          772.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "For evaluation in the open-book setting, we use the corresponding Wikipedia corpus constructed for these two datasets as the knowledge sources. HOVER uses the October 2017 Wikipedia dump processed by Yang et al. (2018), consisting of the introductory sections of 5.2 million Wikipedia pages. FEVEROUS uses the December 2020 dump, including 5.4 million full Wikipedia articles.",
        "page": 4,
        "parent_chapter": 57,
        "index": 59,
        "outline": [
          303.0,
          295.5,
          527.0,
          404.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Baselines. We compare PROGRAMFC to seven baselines, categorized into three groups. (i) Pretrained models: BERT-FC (Soleimani et al., 2020) and LisT5 (Jiang et al., 2021) are two models that leverage BERT and T5 for fact verification, respectively. (ii) FC/NLI fine-tuned models: we choose three pretrained models that are fine-tuned on other fact-checking datasets or natural language inference (NLI) datasets. RoBERTa-NLI (Nie et al., 2020) uses fine-tuned RoBERTa-large on four NLI datasets; DeBERTaV3-NLI (He et al., 2021) finetunes the DeBERTaV3 model on 885,242 (claim, evidence, label) annotations from FEVER and four NLI datasets. MULTIVERS (Wadden et al., 2022b) is a LongFormer (Beltagy et al., 2020) model finetuned on FEVER. (iii) In-context learning models: one baseline is that we directly use the FLAN-T5 model in our VERIFY module for fact-checking. The other baseline uses the in-context learning of Codex for few-shot fact-checking. The implementation details are given in Appendix A.",
        "page": 4,
        "parent_chapter": 57,
        "index": 60,
        "outline": [
          302.0,
          411.5,
          528.0,
          698.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Few-Shot Learning. We study few-shot learning where only a few in-domain examples are available. Therefore, for a fair comparison, we restrict all models to have access to only 20 examples from HOVER or FEVEROUS-S.",
        "page": 4,
        "parent_chapter": 57,
        "index": 61,
        "outline": [
          305.0,
          708.0,
          525.0,
          771.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6985",
        "page": 4,
        "parent_chapter": 57,
        "index": 62,
        "outline": [
          286.5,
          781.5,
          309.5,
          791.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "table",
        "page": 5,
        "parent_chapter": 57,
        "index": 63,
        "outline": [
          75.5,
          71.5,
          522.5,
          233.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "title": "Table 1: Macro-F1 scores of PROGRAMFC (IV) and baselines (I-III) on the evaluation set of HOVER and FEVEROUS-S for few-shot fact-checking. Gold and Open represent the gold evidence setting and the open book setting, respectively. I: pretrained Transformers; II: FC/NLI fine-tuned models; III: in-context learning models.",
        "title_index": 64,
        "page_merged_table": null,
        "cells": {
          "0_0": {
            "text": "Few-shot learning models"
          },
          "0_2": {
            "text": "HOVER (2-hop)"
          },
          "0_4": {
            "text": " HOVER (3-hop)"
          },
          "0_6": {
            "text": " HOVER (4-hop)"
          },
          "0_8": {
            "text": " FEVEROUS-S"
          },
          "1_2": {
            "text": "Gold"
          },
          "1_3": {
            "text": " Open"
          },
          "1_4": {
            "text": "Gold"
          },
          "1_5": {
            "text": " Open"
          },
          "1_6": {
            "text": "Gold"
          },
          "1_7": {
            "text": " Open"
          },
          "1_8": {
            "text": "Gold"
          },
          "1_9": {
            "text": " Open"
          },
          "2_0": {
            "text": "I"
          },
          "2_1": {
            "text": "BERT-FC (Soleimani et al., 2020)"
          },
          "2_2": {
            "text": "53.40"
          },
          "2_3": {
            "text": "50.68"
          },
          "2_4": {
            "text": "50.90"
          },
          "2_5": {
            "text": "49.86"
          },
          "2_6": {
            "text": "50.86"
          },
          "2_7": {
            "text": "48.57"
          },
          "2_8": {
            "text": "74.71"
          },
          "2_9": {
            "text": "51.67"
          },
          "3_1": {
            "text": "LisT5 (Jiang et al., 2021)"
          },
          "3_2": {
            "text": "56.15"
          },
          "3_3": {
            "text": "52.56"
          },
          "3_4": {
            "text": "53.76"
          },
          "3_5": {
            "text": "51.89"
          },
          "3_6": {
            "text": "51.67"
          },
          "3_7": {
            "text": "50.46"
          },
          "3_8": {
            "text": "77.88"
          },
          "3_9": {
            "text": "54.15"
          },
          "4_0": {
            "text": "II"
          },
          "4_1": {
            "text": "RoBERTa-NLI (Nie et al., 2020)"
          },
          "4_2": {
            "text": "74.62"
          },
          "4_3": {
            "text": "63.62"
          },
          "4_4": {
            "text": "62.23"
          },
          "4_5": {
            "text": "53.99"
          },
          "4_6": {
            "text": "57.98"
          },
          "4_7": {
            "text": "52.40"
          },
          "4_8": {
            "text": "88.28"
          },
          "4_9": {
            "text": "57.80"
          },
          "5_1": {
            "text": "DeBERTaV3-NLI (He et al., 2021)"
          },
          "5_2": {
            "text": "77.22"
          },
          "5_3": {
            "text": "68.72"
          },
          "5_4": {
            "text": "65.98"
          },
          "5_5": {
            "text": "60.76"
          },
          "5_6": {
            "text": "60.49"
          },
          "5_7": {
            "text": "56.00"
          },
          "5_8": {
            "text": "91.98"
          },
          "5_9": {
            "text": "58.81"
          },
          "6_1": {
            "text": "MULTIVERS (Wadden et al., 2022b)"
          },
          "6_2": {
            "text": "68.86"
          },
          "6_3": {
            "text": "60.17"
          },
          "6_4": {
            "text": "59.87"
          },
          "6_5": {
            "text": "52.55"
          },
          "6_6": {
            "text": "55.67"
          },
          "6_7": {
            "text": "51.86"
          },
          "6_8": {
            "text": "86.03"
          },
          "6_9": {
            "text": "56.61"
          },
          "7_0": {
            "text": "III"
          },
          "7_1": {
            "text": "Codex (Chen et al., 2021)"
          },
          "7_2": {
            "text": "70.63"
          },
          "7_3": {
            "text": "65.07"
          },
          "7_4": {
            "text": "66.46"
          },
          "7_5": {
            "text": "56.63"
          },
          "7_6": {
            "text": "63.49"
          },
          "7_7": {
            "text": "57.27"
          },
          "7_8": {
            "text": "89.77"
          },
          "7_9": {
            "text": "62.58"
          },
          "8_1": {
            "text": "FLAN-T5 (Chung et al., 2022)"
          },
          "8_2": {
            "text": "73.69"
          },
          "8_3": {
            "text": "69.02"
          },
          "8_4": {
            "text": "65.66"
          },
          "8_5": {
            "text": "60.23"
          },
          "8_6": {
            "text": "58.08"
          },
          "8_7": {
            "text": "55.42"
          },
          "8_8": {
            "text": "90.81"
          },
          "8_9": {
            "text": "63.73"
          },
          "9_0": {
            "text": "IV"
          },
          "9_1": {
            "text": "ProgramFC (N=1)"
          },
          "9_2": {
            "text": "74.10"
          },
          "9_3": {
            "text": "69.36"
          },
          "9_4": {
            "text": "66.13"
          },
          "9_5": {
            "text": "60.63"
          },
          "9_6": {
            "text": "65.69"
          },
          "9_7": {
            "text": "59.16"
          },
          "9_8": {
            "text": "91.77"
          },
          "9_9": {
            "text": "67.80"
          },
          "10_1": {
            "text": "ProgramFC (N=5)"
          },
          "10_2": {
            "text": "75.65"
          },
          "10_3": {
            "text": "70.30"
          },
          "10_4": {
            "text": "68.48"
          },
          "10_5": {
            "text": "63.43"
          },
          "10_6": {
            "text": "66.75"
          },
          "10_7": {
            "text": "57.74"
          },
          "10_8": {
            "text": "92.69"
          },
          "10_9": {
            "text": "68.06"
          }
        },
        "merged": [
          [
            [
              0,
              0
            ],
            [
              0,
              1
            ],
            [
              1,
              0
            ],
            [
              1,
              1
            ]
          ],
          [
            [
              0,
              2
            ],
            [
              0,
              3
            ]
          ],
          [
            [
              0,
              4
            ],
            [
              0,
              5
            ]
          ],
          [
            [
              0,
              6
            ],
            [
              0,
              7
            ]
          ],
          [
            [
              0,
              8
            ],
            [
              0,
              9
            ]
          ],
          [
            [
              2,
              0
            ],
            [
              3,
              0
            ]
          ],
          [
            [
              4,
              0
            ],
            [
              5,
              0
            ],
            [
              6,
              0
            ]
          ],
          [
            [
              7,
              0
            ],
            [
              8,
              0
            ]
          ],
          [
            [
              9,
              0
            ],
            [
              10,
              0
            ]
          ]
        ],
        "grid": {
          "rows": [
            17.5,
            32.0,
            44.8,
            59.4,
            74.5,
            87.3,
            102.4,
            116.4,
            131.5,
            147.3
          ],
          "columns": [
            20.4,
            146.7,
            181.6,
            224.1,
            259.0,
            300.9,
            335.8,
            377.7,
            410.9
          ]
        }
      },
      {
        "type": "paragraph",
        "text": "Table 1: Macro-F1 scores of PROGRAMFC (IV) and baselines (I-III) on the evaluation set of HOVER and FEVEROUS-S for few-shot fact-checking. Gold and Open represent the gold evidence setting and the open book setting, respectively. I: pretrained Transformers; II: FC/NLI fine-tuned models; III: in-context learning models.",
        "page": 5,
        "parent_chapter": 57,
        "index": 64,
        "outline": [
          69.0,
          244.0,
          525.0,
          278.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We use these examples either for fine-tuning pre-trained models (BERT-FC and LisT5), for continuous fine-tuning the FC/NLI fine-tuned models, or as in-context examples for FLAN-T5 and Codex. For PROGRAMFC, we use them as in-context examples for reasoning program generation.",
        "page": 5,
        "parent_chapter": 57,
        "index": 65,
        "outline": [
          69.0,
          301.0,
          291.0,
          381.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We evaluate both the gold evidence setting and the open-book setting. The baseline models are the same for both settings. However, during testing in the open-book setting, the models are given the retrieved evidence rather than the ground-truth evidence. We use BM25 (Robertson and Zaragoza, 2009) implemented with the Pyserini toolkit (Lin et al., 2021) as the retriever for both PROGRAMFC and the baselines. We use as evidence the top-10 paragraphs retrieved from the knowledge corpus.",
        "page": 5,
        "parent_chapter": 57,
        "index": 66,
        "outline": [
          68.0,
          383.5,
          293.0,
          518.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "4.1 Main Results",
        "page": 5,
        "parent_chapter": 57,
        "index": 67,
        "outline": [
          69.0,
          533.0,
          157.0,
          543.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We report the overall results for PROGRAMFC and for the baselines for few-shot fact-checking in Table 1. PROGRAMFC achieves the best performance on 7 out of 8 evaluations, demonstrating its effectiveness. We have three more specific observations.",
        "page": 5,
        "parent_chapter": 67,
        "index": 68,
        "outline": [
          69.0,
          552.0,
          291.0,
          618.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "ProgramFC is more effective on deeper claims.",
        "page": 5,
        "parent_chapter": 67,
        "index": 69,
        "outline": [
          69.0,
          625.0,
          281.0,
          638.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "On the HOVER dataset, ProgramFC (N=5) outperforms the baselines on average by 10.38%, 11.37%, and 14.77% on two-hop, three-hop, and four-hop claims, respectively. This suggests that ProgramFC becomes increasingly effective as the required reasoning depth increases. Among the baselines, DeBERTaV3-NLI performs comparably to ProgramFC on two-hop claims, indicating that large-scale pre-training on simpler claims can help the model generalize to more complex claims.",
        "page": 5,
        "parent_chapter": 67,
        "index": 70,
        "outline": [
          68.0,
          638.5,
          292.0,
          774.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "However, this generalization becomes more challenging as the complexity of the claims increases. On HOVER, the F1 score of DeBERTaV3-NLI drops from 77.22 for 2-hop claims to 60.49 for 4-hop claims, which is a decrease of 21.7%. In contrast, the performance drop for ProgramFC, which uses the strategy of program-guided reasoning, is much smaller:j ust 11.7%.",
        "page": 5,
        "parent_chapter": 67,
        "index": 71,
        "outline": [
          304.0,
          300.0,
          527.0,
          408.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Decomposition is more effective than one-step prediction. The ProgramFC model, which uses the same FLAN-T5 model as the sub-task functions, outperforms the baseline of directly verifying claims with FLAN-T5 on all four datasets. On average, there is a 6.0% improvement in the gold evidence setting and a 4.5% improvement in the open-book setting. This suggests that decomposing a complex claim into simpler steps with a program can facilitate more accurate reasoning. This is especially evident when the required reasoning is complex: there is a 14.9% improvement in the gold evidence setting and a 6.7% improvement in the open-book setting for 4-hop claims.",
        "page": 5,
        "parent_chapter": 67,
        "index": 72,
        "outline": [
          303.0,
          418.0,
          528.0,
          609.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Aggregating reasoning programs is helpful.",
        "page": 5,
        "parent_chapter": 67,
        "index": 73,
        "outline": [
          305.0,
          618.5,
          509.0,
          632.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We find that aggregating the predictions of \\(N=5\\) reasoning programs improves the performance over using a single program by an average of 1.5%. This aligns with the findings of Wang et al. (2022), where the idea was applied for question answering: if multiple different ways of thinking lead to the same answer, we can have greater confidence that the final answer is correct. This intuition also applies to fact-checking, as each program represents a unique reasoning chain to verify the claim.",
        "page": 5,
        "parent_chapter": 67,
        "index": 74,
        "outline": [
          303.0,
          638.5,
          527.0,
          774.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6986",
        "page": 5,
        "parent_chapter": 67,
        "index": 75,
        "outline": [
          285.5,
          781.5,
          309.5,
          791.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "figure",
        "text": "FLAN-T5 ProgramFCHOVER (2-hop) FLAN-T5 ProgramFCHOVER (3-hop) FLAN-T5 ProgramFCHOVER (4-hop)  8076.11 75.6577.62808072.5677.077073.697067.88 68.55 68.48 69.567068.37 68.56 66.75 68.1864.3571.6968.2462.2366.89 62.4665.07 65.66 606063.056061.3663.3956.58 58.0850505047.7549.2948.59404040 80M 250M 780M 3B 11B80M 250M 780M 3B 11B80M 250M 780M 3B 11B",
        "page": 6,
        "parent_chapter": 67,
        "index": 76,
        "outline": [
          71.5,
          75.5,
          522.5,
          183.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Figure 4: F1 score for fact-checking with gold evidence using FLAN-T5 (blue line) and PROGRAMFC (green line) for language models of increasing sizes: FLAN-T5-small (80M), FLAN-T5-base (250M), FLAN-large (780M), FLAN-T5-XL (3B), and FLAN-T5-XXL (11B) on HOVER 2-hop (left), 3-hop (middle), and 4-hop (right).",
        "page": 6,
        "parent_chapter": 67,
        "index": 77,
        "outline": [
          69.0,
          196.0,
          526.0,
          231.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "figure",
        "text": "90 One-step Retrieval ProgramFC85.65 807077.1376.2573.186059.175051.3349.93403036.4320 HOVER (2-hop) HOVER (3-hop) HOVER (4-hop) FEVEROUS-S",
        "page": 6,
        "parent_chapter": 67,
        "index": 78,
        "outline": [
          71.5,
          250.5,
          287.5,
          359.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Figure 5: Retrieval recall@10 for the one-step retrieval and the iterative retrieval in PROGRAMFC.",
        "page": 6,
        "parent_chapter": 67,
        "index": 79,
        "outline": [
          70.0,
          368.0,
          288.5,
          389.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "4.2 How Does the Reasoning Program Help?",
        "page": 6,
        "parent_chapter": 57,
        "index": 80,
        "outline": [
          70.0,
          412.5,
          285.0,
          423.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "To further understand how reasoning programs facilitate fact-checking, we compare the performance of PROGRAMFC with FLAN-T5 using different language model sizes: small, base, large, XL, and XXL. The results are shown in Figure 4 and indicate that program-guided reasoning is particularly effective when the model size is small. As smaller models have less capacity for complex reasoning, the performance of the end-to-end FLAN-T5 model decreases significantly with decreasing model size. However, this trend is less notable for PROGRAMFC. The high-level reasoning plan offered by reasoning programs substantially alleviates the demands on the subsequent subtask solvers. Our results show that the programguided model using FLAN-T5-small (80M parameters) as sub-task solvers can achieve comparable performance to the 137x larger FLAN-T5-XXL (11B) model with end-to-end reasoning for 4-hop claims.",
        "page": 6,
        "parent_chapter": 80,
        "index": 81,
        "outline": [
          67.0,
          432.5,
          293.0,
          692.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "In the open-domain setting, we find that reasoning programs can enhance the retrieval of relevant evidence from the knowledge source. Figure 5 compares the retrieval performance of the one-step BM25 retriever used in the baselines to the iterative step-by-step BM25 retriever in PROGRAMFC.",
        "page": 6,
        "parent_chapter": 80,
        "index": 82,
        "outline": [
          69.0,
          693.5,
          291.0,
          773.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We measure the recall of the gold paragraphs for the top-10 retrieved paragraphs (recall@10).For PROGRAMFC, we combine the retrieved paragraphs of all steps and we consider the top-10 results. We can see in Figure 5 that PROGRAMFC outperforms one-step retrieval on all datasets, with the largest improvement of 37.1% on HOVER 4-hop. This is because some information may not be present in the original claim, but is only revealed during the reasoning process (e.g., “Christopher Nolan” in Figure 1). Thus, iterative retrieval guided by the reasoning program yields better results.",
        "page": 6,
        "parent_chapter": 80,
        "index": 83,
        "outline": [
          302.0,
          253.0,
          528.0,
          418.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "4.3 Interpretability of Reasoning Programs",
        "page": 6,
        "parent_chapter": 57,
        "index": 84,
        "outline": [
          305.0,
          429.0,
          515.0,
          441.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "An advantage of PROGRAMFC is that it improves the interpretability of fact-checking compared to end-to-end models, as the explicit program can aid human understanding and debugging. Examples of generated reasoning programs can be found in Figure 7 of Appendix B. To assess the quality of the generated reasoning programs, we sampled 300 claims where PROGRAMFC incorrectly predicted the final veracity labels from the HOVER 2-hop, 3-hop, and 4-hop datasets, with 100 examples per dataset. We asked human annotators to analyze the error types and we classified the results into three categories: (i) Syntactic errors, where the program does not conform to the defined grammar and cannot be parsed, (ii) Semantic errors, which include incorrect or missing arguments/variables (Token), incorrect program structure (Structure), and incorrect sub-task calls (Subtask), and (iii) Incorrect execution, where the program is correct, but where the incorrect prediction is a result of its execution.",
        "page": 6,
        "parent_chapter": 84,
        "index": 85,
        "outline": [
          302.0,
          447.0,
          528.0,
          720.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We show the error analysis in Table 2. First, no syntax errors were found in our samples, indicating that Codex effectively generates executable programs through few-shot in-context learning.",
        "page": 6,
        "parent_chapter": 84,
        "index": 86,
        "outline": [
          305.0,
          721.0,
          526.0,
          773.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6987",
        "page": 6,
        "parent_chapter": 84,
        "index": 87,
        "outline": [
          285.5,
          781.5,
          309.5,
          791.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Claim:\nEmery, located in the same state as Edison Local School District, is a ghost town. It is near the \ncity that lies close to the Ohio Turnpike, a 241.26 mi highway.\nPredicted Program: \nanswer_1 = Question(\"Which state is Emery located in?\")\nanswer_2 = Question(\"Which state is Edison Local School District located in?\")\nfact_1 = Verify(\"{answer_1} and {answer_2} are the same state.\")\nfact_2 = Verify(\"Emery is a ghost town.\")\nanswer_3 = Question(\"Which city is near Emery?\")\nanswer_4 = Question(\"Which city lies close to the Ohio Turnpike, a 241.26 mi highway?\")\nfact_3 = Verify(\"{answer_3} is near {answer_4}.\") fact_3 = Verify(“Emery is near {answer_4}.”)\nlabel = Predict(fact_1 and fact_2 and fact_3)",
        "page": 7,
        "parent_chapter": 84,
        "index": 88,
        "outline": [
          83.5,
          70.5,
          509.5,
          186.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Figure 6: An error case from the HOVER 4-hop dataset where the generated reasoning program has an incorrect program structure. The incorrect segment(s) are marked in red, and the correct revisions are marked in green.",
        "page": 7,
        "parent_chapter": 84,
        "index": 89,
        "outline": [
          69.0,
          197.0,
          526.0,
          221.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "table",
        "page": 7,
        "parent_chapter": 84,
        "index": 90,
        "outline": [
          72.5,
          241.0,
          285.5,
          341.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "title": "Table 2: Reasoning program evaluation for incorrectly-predicted examples from each hop length in HOVER.",
        "title_index": 91,
        "page_merged_table": null,
        "cells": {
          "0_0": {
            "text": "Error Type"
          },
          "0_1": {
            "text": "Proportion (%)"
          },
          "1_1": {
            "text": "2-hop"
          },
          "1_2": {
            "text": "3-hop"
          },
          "1_3": {
            "text": "4-hop"
          },
          "2_0": {
            "text": "Syntax error"
          },
          "2_1": {
            "text": "0%"
          },
          "2_2": {
            "text": "0%"
          },
          "2_3": {
            "text": "0%"
          },
          "3_0": {
            "text": "Semantic error"
          },
          "3_1": {
            "text": "29%"
          },
          "3_2": {
            "text": "38%"
          },
          "3_3": {
            "text": "77%"
          },
          "4_0": {
            "text": "Token"
          },
          "4_1": {
            "text": "8%"
          },
          "4_2": {
            "text": "20%"
          },
          "4_3": {
            "text": "18%"
          },
          "5_0": {
            "text": "Structure"
          },
          "5_1": {
            "text": "19%"
          },
          "5_2": {
            "text": "13%"
          },
          "5_3": {
            "text": "57%"
          },
          "6_0": {
            "text": "Subtask"
          },
          "6_1": {
            "text": "2%"
          },
          "6_2": {
            "text": "5%"
          },
          "6_3": {
            "text": "2%"
          },
          "7_0": {
            "text": "Incorrect execution"
          },
          "7_1": {
            "text": "71%"
          },
          "7_2": {
            "text": "62%"
          },
          "7_3": {
            "text": "23%"
          }
        },
        "merged": [
          [
            [
              0,
              0
            ],
            [
              1,
              0
            ]
          ],
          [
            [
              0,
              1
            ],
            [
              0,
              2
            ],
            [
              0,
              3
            ]
          ]
        ],
        "grid": {
          "rows": [
            14.0,
            28.0,
            41.0,
            52.0,
            63.0,
            74.0,
            85.0
          ],
          "columns": [
            83.5,
            128.0,
            168.5
          ]
        }
      },
      {
        "type": "paragraph",
        "text": "Table 2: Reasoning program evaluation for incorrectlypredicted examples from each hop length in HOVER.",
        "page": 7,
        "parent_chapter": 84,
        "index": 91,
        "outline": [
          69.0,
          351.0,
          291.0,
          373.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Second, for 2-hop claims, we find that 71% of the programs are correct. The majority of the errors are the result of incorrect program execution, where the question answering or the fact-checking modules failed to return the correct answer.",
        "page": 7,
        "parent_chapter": 84,
        "index": 92,
        "outline": [
          69.0,
          396.5,
          292.0,
          461.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Third, as the complexity of the claims increases, the proportion of semantic errors in the programs also increases, with structural errors becoming particularly prevalent. This highlights the difficulty of generating the appropriate step-by-step reasoning strategies for claims that require long-chain reasoning. An example structural error is shown in Figure 6, where the model fails to parse the second sentence of the claim into correct program instructions. Additional error examples can be found in Appendix C.",
        "page": 7,
        "parent_chapter": 84,
        "index": 93,
        "outline": [
          68.0,
          464.5,
          291.0,
          612.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "4.4 Closed-Book Fact-Checking",
        "page": 7,
        "parent_chapter": 57,
        "index": 94,
        "outline": [
          69.0,
          622.0,
          227.0,
          635.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Finally, we evaluate the closed-book setting, where the model does not have access to any knowledge source and needs to rely on its parametric knowledge only. The baseline models from groups I and II in Table 1 are trained with (evidence, claim) pairs and thus are not applicable in this setting. We compare our method to the baselines that use large language models for in-context learning, including Codex (code-davinci-002) and FLAN-T5 from Table 1.",
        "page": 7,
        "parent_chapter": 94,
        "index": 95,
        "outline": [
          67.0,
          639.5,
          292.0,
          772.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "table",
        "page": 7,
        "parent_chapter": 94,
        "index": 96,
        "outline": [
          307.5,
          240.5,
          524.5,
          381.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "title": "Table 3: Closed-book setting: macro-F1 scores for PRO-GRAMFC and for the baselines.",
        "title_index": 97,
        "page_merged_table": null,
        "cells": {
          "0_0": {
            "text": "Model"
          },
          "0_1": {
            "text": "HOVER"
          },
          "0_4": {
            "text": "FEVEROUS"
          },
          "1_1": {
            "text": "2-hop"
          },
          "1_2": {
            "text": " 3-hop"
          },
          "1_3": {
            "text": " 4-hop"
          },
          "2_0": {
            "text": "InstructGPT"
          },
          "2_1": {
            "text": ""
          },
          "2_2": {
            "text": ""
          },
          "2_3": {
            "text": ""
          },
          "2_4": {
            "text": ""
          },
          "3_0": {
            "text": "- Direct"
          },
          "3_1": {
            "text": "56.51"
          },
          "3_2": {
            "text": "51.75"
          },
          "3_3": {
            "text": "49.68"
          },
          "3_4": {
            "text": "60.13"
          },
          "4_0": {
            "text": "- ZS-CoT"
          },
          "4_1": {
            "text": "50.30"
          },
          "4_2": {
            "text": "52.30"
          },
          "4_3": {
            "text": "51.58"
          },
          "4_4": {
            "text": "54.78"
          },
          "5_0": {
            "text": "- CoT"
          },
          "5_1": {
            "text": "57.20"
          },
          "5_2": {
            "text": "53.66"
          },
          "5_3": {
            "text": "51.83"
          },
          "5_4": {
            "text": "61.05"
          },
          "6_0": {
            "text": "- Self-Ask"
          },
          "6_1": {
            "text": "51.54"
          },
          "6_2": {
            "text": "51.47"
          },
          "6_3": {
            "text": "52.45"
          },
          "6_4": {
            "text": "56.82"
          },
          "7_0": {
            "text": "Codex"
          },
          "7_1": {
            "text": "55.57"
          },
          "7_2": {
            "text": "53.42"
          },
          "7_3": {
            "text": "45.59"
          },
          "7_4": {
            "text": "57.85"
          },
          "8_0": {
            "text": "FLAN-T5"
          },
          "8_1": {
            "text": "48.27"
          },
          "8_2": {
            "text": "52.11"
          },
          "8_3": {
            "text": "51.13"
          },
          "8_4": {
            "text": "55.16"
          },
          "9_0": {
            "text": "ProgramFC"
          },
          "9_1": {
            "text": "54.27"
          },
          "9_2": {
            "text": "54.18"
          },
          "9_3": {
            "text": "52.88"
          },
          "9_4": {
            "text": "59.66"
          }
        },
        "merged": [
          [
            [
              0,
              0
            ],
            [
              1,
              0
            ]
          ],
          [
            [
              0,
              1
            ],
            [
              0,
              2
            ],
            [
              0,
              3
            ]
          ],
          [
            [
              0,
              4
            ],
            [
              1,
              4
            ]
          ]
        ],
        "grid": {
          "rows": [
            15.0,
            31.0,
            45.0,
            57.0,
            69.0,
            82.0,
            94.0,
            106.0,
            121.0
          ],
          "columns": [
            67.0,
            98.0,
            129.0,
            160.0
          ]
        }
      },
      {
        "type": "paragraph",
        "text": "Table 3: Closed-book setting: macro-F1 scores for PROGRAMFC and for the baselines.",
        "page": 7,
        "parent_chapter": 94,
        "index": 97,
        "outline": [
          305.0,
          390.0,
          526.0,
          411.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We also include the 175B-parameter InstructGPT (text-davinci-002) (Ouyang et al., 2022) with four different prompts: (i) direct prompting with the claim, (ii) CoT (Wei et al., 2022) or chain-of-thought prompting with demonstrations, (iii) ZS-CoT (Kojima et al., 2022) or zero-shot chain-of-thought with the prompt “let’s think step by step”, and (iv) Self-Ask (Press et al., 2022), which is a variant of CoT that guides the model reasoning by asking a series of questions. The detailed prompting templates are given in Appendix E.",
        "page": 7,
        "parent_chapter": 94,
        "index": 98,
        "outline": [
          302.0,
          433.5,
          529.0,
          584.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Our results, presented in Table 3, show that most models achieve a Macro-F1 score only slightly above random guessing on the HOVER dataset, indicating the difficulty of solely relying on parametric knowledge of large language models for fact-checking complex claims. Similar to the observations in Section 4.1, we see a trend of improved performance as the number of the required reasoning hops increases. Chain-of-thought prompting scores an average 2.7 points higher than direct prompting, highlighting the importance of stepby-step reasoning for complex fact-checking. It outperforms our PROGRAMFC on HOVER 2-hop and FEVEROUS but performs worse on HOVER 3-hop and 4-hop.",
        "page": 7,
        "parent_chapter": 94,
        "index": 99,
        "outline": [
          303.0,
          584.0,
          528.0,
          774.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": true,
        "page_merged_paragraph": {
          "paragraph_indices": [
            99,
            101
          ]
        }
      },
      {
        "type": "page_footer",
        "text": "6988",
        "page": 7,
        "parent_chapter": 94,
        "index": 100,
        "outline": [
          285.5,
          781.5,
          309.5,
          791.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "This can be due to CoT generating free-form explanations, which can lead to unpredictable errors in long reasoning chains. In contrast, our program generation-and-execution strategy is more stable for longer reasoning chains.",
        "page": 8,
        "parent_chapter": 94,
        "index": 102,
        "outline": [
          69.0,
          87.0,
          290.0,
          153.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "5 Conclusion and Future Work",
        "page": 8,
        "parent_chapter": -1,
        "index": 103,
        "outline": [
          70.0,
          166.0,
          237.5,
          176.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We proposed PROGRAMFC, a few-shot neurosymbolic model for fact-checking that learns to map input claims to a reasoning program consisting of a sequence of sub-task function calls for answering a question, for fact-checking a simple claim, and for computing a logical expression. Then factchecking is performed by executing that program. PROGRAMFC combines the advantages of symbolic programs, such as explainability, with the flexibility of end-to-end neural models. Using Codex as the program generator, PROGRAMFC demonstrates promising performance on HOVER and FEVEROUS with only a small number of incontext demonstrations and no additional training. We also investigated the impact of model size and the benefits of programs for retrieval, and we analyzed the errors. The results indicated that PROGRAMFC effectively balances model capability, learning efficiency, and interpretability.",
        "page": 8,
        "parent_chapter": 103,
        "index": 104,
        "outline": [
          66.0,
          186.0,
          294.0,
          447.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "In future work, we want to adapt PROGRAMFC to more real-world fact-checking scenarios, such as fake news detection and multi-modal fact-checking, with advanced reasoning program design and subtask functionalities.",
        "page": 8,
        "parent_chapter": 103,
        "index": 105,
        "outline": [
          69.0,
          446.0,
          291.0,
          510.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Limitations",
        "page": 8,
        "parent_chapter": -1,
        "index": 106,
        "outline": [
          70.0,
          525.0,
          130.5,
          536.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "We identify two main limitations of PROGRAMFC. First, despite being complex in their surface form, the claims in the HOVER and FEVEROUS datasets mostly require only explicit multi-step reasoning, i.e., the decomposition can be derived from the claim’s syntactic structure or how the claim is framed. This lowers the difficulty of generating reasoning programs. However, for many real-world complex claims, the reasoning is often implicit. For example, for the claim “Aristotle couldn’t have used a laptop”, the reasoning program is: answer_1 = Question(“When did Aristotle live?”);answer_2 = Question(“When was the laptop invented?”);fact_1 = Verify(“answer_1 is before answer_2.”);label = Predict(fact_1)",
        "page": 8,
        "parent_chapter": 106,
        "index": 107,
        "outline": [
          61.0,
          542.5,
          304.0,
          772.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Generating reasoning programs for such implicit complex claims requires a deeper understanding of the claim and also access to world and commonsense knowledge. We conducted preliminary experiments on these types of claims, but we found that our Codex-based generator struggled to produce a correct reasoning program. This highlights the gap in applying our PROGRAMFC to fact-check real-world claims. Addressing these challenges is an important direction for future work.",
        "page": 8,
        "parent_chapter": 106,
        "index": 108,
        "outline": [
          303.0,
          71.5,
          527.0,
          206.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Second, PROGRAMFC incurs a higher computational cost than baseline end-to-end fact-checking models. It requires calling large language models for program generation and further calling multiple sub-task models. This results in the actual computational time that is ∼4–5× higher than for an endto-end FLAN-T5 model. Developing more efficient methods for program generation and execution is an important direction for future work.",
        "page": 8,
        "parent_chapter": 106,
        "index": 109,
        "outline": [
          303.0,
          206.5,
          528.0,
          329.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Ethics Statement",
        "page": 8,
        "parent_chapter": -1,
        "index": 110,
        "outline": [
          305.0,
          340.0,
          392.5,
          351.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Biases. We note that there might be some biases in the data used to train the LLMs, as well as in factualityj udgments. Both are beyond our control.",
        "page": 8,
        "parent_chapter": 110,
        "index": 111,
        "outline": [
          305.0,
          362.5,
          526.0,
          401.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Intended Use and Misuse Potential. Our models can be of interest to the general public and could also save a lot of time to human fact-checkers. However, they could also be misused by malicious actors. We ask researchers to exercise caution.",
        "page": 8,
        "parent_chapter": 110,
        "index": 112,
        "outline": [
          304.0,
          410.0,
          526.0,
          474.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Environmental Impact. The use of large language models requires a significant amount of energy for computation for training, which contributes to global warming. Our work performs fewshot in-context learning instead of training models from scratch, so the energy footprint of our work is less. The large language model (Codex) whose API we use for inference consumes significant energy.",
        "page": 8,
        "parent_chapter": 110,
        "index": 113,
        "outline": [
          303.0,
          485.0,
          527.0,
          593.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Acknowledgements",
        "page": 8,
        "parent_chapter": -1,
        "index": 114,
        "outline": [
          305.0,
          604.0,
          405.5,
          616.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "This work was supported in part by the National Science Foundation award #2048122 and by Singapore’s Ministry of Education Tier 3 grant “Digital Information Resilience: Restoring Trust and Nudging Behaviours in Digitalisation”. The views expressed are those of the authors and do not reflect the official policy or position of the US government. We thank Alex Mei, Xinyi Wang, Danqing Wang, Sharon Levy, Gyuwan Kim, and other members of the UCSB NLP group for their valuable feedback.",
        "page": 8,
        "parent_chapter": 114,
        "index": 115,
        "outline": [
          304.0,
          625.5,
          527.0,
          761.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6989",
        "page": 8,
        "parent_chapter": 114,
        "index": 116,
        "outline": [
          285.5,
          781.5,
          310.0,
          791.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "References",
        "page": 9,
        "parent_chapter": -1,
        "index": 117,
        "outline": [
          70.0,
          71.5,
          126.0,
          82.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Naser Ahmadi, Joohyung Lee, Paolo Papotti, and Mohammed Saeed. 2019. Explainable fact checking with probabilistic answer set programming. In Proceedings of the Truth and Trust Online Conference (TTO), London, UK.",
        "page": 9,
        "parent_chapter": 117,
        "index": 118,
        "outline": [
          69.0,
          90.5,
          291.0,
          146.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. In Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, Online.",
        "page": 9,
        "parent_chapter": 117,
        "index": 119,
        "outline": [
          70.0,
          156.0,
          291.0,
          244.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Rami Aly and Andreas Vlachos. 2022. Natural logicguided autoregressive multi-hop document retrieval for fact verification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6123–6135, Abu Dhabi, United Arab Emirates.",
        "page": 9,
        "parent_chapter": 117,
        "index": 120,
        "outline": [
          69.0,
          253.0,
          291.0,
          318.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating fact checking explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 7352–7364, Online.",
        "page": 9,
        "parent_chapter": 117,
        "index": 121,
        "outline": [
          69.0,
          328.0,
          291.0,
          383.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. 2019. MultiFC: A real-world multi-domain dataset for evidencebased fact checking of claims. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4685–4697, Hong Kong, China.",
        "page": 9,
        "parent_chapter": 117,
        "index": 122,
        "outline": [
          69.0,
          392.5,
          291.0,
          502.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Giorgio Barnabò, Federico Siciliano, Carlos Castillo, Stefano Leonardi, Preslav Nakov, Giovanni Da San Martino, and Fabrizio Silvestri. 2022. FbMultiLingMisinfo: Challenging large-scale multilingual benchmark for misinformation detection. In Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8, Padova, Italy.",
        "page": 9,
        "parent_chapter": 117,
        "index": 123,
        "outline": [
          70.0,
          513.0,
          291.0,
          601.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Giorgio Barnabò, Federico Siciliano, Carlos Castillo, Stefano Leonardi, Preslav Nakov, Giovanni Da San Martino, and Fabrizio Silvestri. 2023. Deep active learning for misinformation detection using geometric deep learning. Online Social Networks and Media, 33:100244.",
        "page": 9,
        "parent_chapter": 117,
        "index": 124,
        "outline": [
          69.0,
          611.0,
          290.0,
          675.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. ArXiv preprint, abs/2004.05150.",
        "page": 9,
        "parent_chapter": 117,
        "index": 125,
        "outline": [
          70.0,
          685.0,
          291.0,
          718.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical",
        "page": 9,
        "parent_chapter": 117,
        "index": 126,
        "outline": [
          70.0,
          729.5,
          290.0,
          772.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Methods in Natural Language Processing (EMNLP), pages 632–642, Lisbon, Portugal.",
        "page": 9,
        "parent_chapter": 117,
        "index": 127,
        "outline": [
          314.5,
          72.5,
          525.0,
          94.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS), Online.",
        "page": 9,
        "parent_chapter": 117,
        "index": 128,
        "outline": [
          302.0,
          102.5,
          528.0,
          248.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Jifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. 2022a. Generating literal and implied subquestions to fact-check complex claims. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3495–3516, Abu Dhabi, United Arab Emirates.",
        "page": 9,
        "parent_chapter": 117,
        "index": 129,
        "outline": [
          304.0,
          254.5,
          526.0,
          320.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.",
        "page": 9,
        "parent_chapter": 117,
        "index": 130,
        "outline": [
          302.0,
          327.0,
          528.0,
          563.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022b. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588.",
        "page": 9,
        "parent_chapter": 117,
        "index": 131,
        "outline": [
          305.0,
          569.0,
          526.0,
          621.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022. Binding language models in symbolic languages. CoRR, abs/2210.02875.",
        "page": 9,
        "parent_chapter": 117,
        "index": 132,
        "outline": [
          305.5,
          633.5,
          526.0,
          696.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,",
        "page": 9,
        "parent_chapter": 117,
        "index": 133,
        "outline": [
          304.0,
          707.0,
          526.0,
          772.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6990",
        "page": 9,
        "parent_chapter": 117,
        "index": 134,
        "outline": [
          286.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.",
        "page": 10,
        "parent_chapter": 117,
        "index": 135,
        "outline": [
          80.0,
          73.5,
          291.0,
          127.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Limeng Cui, Kai Shu, Suhang Wang, Dongwon Lee, and Huan Liu. 2019. dEFEND: A system for explainable fake news detection. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM), pages 2961–2964, Beijing, China.",
        "page": 10,
        "parent_chapter": 117,
        "index": 136,
        "outline": [
          70.0,
          137.0,
          290.0,
          202.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171–4186, Minneapolis, Minnesota, USA.",
        "page": 10,
        "parent_chapter": 117,
        "index": 137,
        "outline": [
          69.0,
          210.5,
          291.0,
          298.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Mohamed H. Gad-Elrab, Daria Stepanova, Jacopo Urbani, and Gerhard Weikum. 2019. Exfakt: A framework for explaining facts over knowledge graphs and text. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (WSDM), pages 87–95, Melbourne, Australia.",
        "page": 10,
        "parent_chapter": 117,
        "index": 138,
        "outline": [
          69.0,
          306.5,
          290.0,
          371.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. PAL: program-aided language models. CoRR, abs/2211.10435.",
        "page": 10,
        "parent_chapter": 117,
        "index": 139,
        "outline": [
          69.0,
          379.5,
          291.0,
          423.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Max Glockner, Yufang Hou, and Iryna Gurevych. 2022. Missing counter-evidence renders NLP fact-checking unrealistic for misinformation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5916–5936, Abu Dhabi, United Arab Emirates.",
        "page": 10,
        "parent_chapter": 117,
        "index": 140,
        "outline": [
          69.0,
          432.5,
          291.0,
          498.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10:178–206.",
        "page": 10,
        "parent_chapter": 117,
        "index": 141,
        "outline": [
          69.0,
          505.5,
          292.0,
          550.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Ashim Gupta and Vivek Srikumar. 2021. X-Fact: A new benchmark dataset for multilingual fact checking. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 675–682, Online.",
        "page": 10,
        "parent_chapter": 117,
        "index": 142,
        "outline": [
          69.0,
          558.5,
          291.0,
          625.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-training with gradientdisentangled embedding sharing. ArXiv preprint, abs/2111.09543.",
        "page": 10,
        "parent_chapter": 117,
        "index": 143,
        "outline": [
          70.0,
          633.5,
          291.0,
          686.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Kelvin Jiang, Ronak Pradeep, and Jimmy Lin. 2021. Exploring listwise evidence reasoning with T5 for fact verification. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 402–410, Online.",
        "page": 10,
        "parent_chapter": 117,
        "index": 144,
        "outline": [
          70.0,
          695.0,
          291.0,
          771.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3441–3460, Online.",
        "page": 10,
        "parent_chapter": 117,
        "index": 145,
        "outline": [
          305.0,
          72.5,
          526.0,
          138.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Shailza Jolly, Pepa Atanasova, and Isabelle Augenstein. 2022. Generating fluent fact checking explanations with unsupervised post-editing. Information, 13(10):500.",
        "page": 10,
        "parent_chapter": 117,
        "index": 146,
        "outline": [
          305.0,
          147.0,
          526.0,
          191.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. CoRR, abs/2205.11916.",
        "page": 10,
        "parent_chapter": 117,
        "index": 147,
        "outline": [
          305.0,
          199.5,
          526.0,
          241.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Neema Kotonya and Francesca Toni. 2020. Explainable automated fact-checking for public health claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7740–7754, Online.",
        "page": 10,
        "parent_chapter": 117,
        "index": 148,
        "outline": [
          305.0,
          253.0,
          525.0,
          308.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Amrith Krishna, Sebastian Riedel, and Andreas Vlachos. 2022. ProoFVer: Natural logic theorem proving for fact verification. Transactions of the Association for Computational Linguistics (TACL), 10:1013–1030.",
        "page": 10,
        "parent_chapter": 117,
        "index": 149,
        "outline": [
          305.5,
          316.5,
          525.0,
          359.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021. Towards few-shot fact-checking via perplexity. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1971–1981, Online.",
        "page": 10,
        "parent_chapter": 117,
        "index": 150,
        "outline": [
          305.0,
          368.0,
          526.0,
          434.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Nayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau Yih, Hao Ma, and Madian Khabsa. 2020. Language models as fact checkers? In Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER), pages 36–41, Online.",
        "page": 10,
        "parent_chapter": 117,
        "index": 151,
        "outline": [
          305.0,
          443.0,
          526.0,
          498.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 2356–2362, Online.",
        "page": 10,
        "parent_chapter": 117,
        "index": 152,
        "outline": [
          304.0,
          505.5,
          526.0,
          592.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. 2022. WANLI: Worker and AI collaboration for natural language inference dataset creation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6826–6847, Abu Dhabi, United Arab Emirates.",
        "page": 10,
        "parent_chapter": 117,
        "index": 153,
        "outline": [
          305.0,
          602.5,
          526.0,
          668.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. ArXiv preprint, abs/1907.11692.",
        "page": 10,
        "parent_chapter": 117,
        "index": 154,
        "outline": [
          305.5,
          677.0,
          525.0,
          730.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2020. Fine-grained fact verification with kernel graph attention network. In Proceedings",
        "page": 10,
        "parent_chapter": 117,
        "index": 155,
        "outline": [
          305.5,
          740.5,
          525.0,
          772.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6991",
        "page": 10,
        "parent_chapter": 117,
        "index": 156,
        "outline": [
          285.5,
          781.0,
          308.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 7342–7351, Online.",
        "page": 11,
        "parent_chapter": 117,
        "index": 157,
        "outline": [
          80.0,
          73.5,
          292.0,
          104.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Yi-Ju Lu and Cheng-Te Li. 2020. GCAN: Graph-aware co-attention networks for explainable fake news detection on social media. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 505–514, Online.",
        "page": 11,
        "parent_chapter": 117,
        "index": 158,
        "outline": [
          70.0,
          117.0,
          290.0,
          172.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey. CoRR, abs/2302.07842.",
        "page": 11,
        "parent_chapter": 117,
        "index": 159,
        "outline": [
          70.0,
          182.0,
          291.0,
          248.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Preslav Nakov, Alberto Barrón-Cedeño, Giovanni Da San Martino, Firoj Alam, Julia Maria Struß, Thomas Mandl, Rubén Míguez, Tommaso Caselli, Mucahid Kutlu, Wajdi Zaghouani, Chengkai Li, Shaden Shaar, Gautam Kishore Shahi, Hamdy Mubarak, Alex Nikolov, Nikolay Babulkov, Yavuz Selim Kartal, and Javier Beltrán. 2022. The CLEF-2022 CheckThat! lab on fighting the COVID-19 infodemic and fake news detection. In Proceedings of the 44th European Conference on IR Research: Advances in Information Retrieval (ECIR), pages 416–428, Berlin, Heidelberg.",
        "page": 11,
        "parent_chapter": 117,
        "index": 160,
        "outline": [
          68.0,
          256.5,
          293.0,
          392.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barrón-Cedeño, Paolo Papotti, Shaden Shaar, and Giovanni Da San Martino. 2021a. Automated fact-checking for assisting human fact-checkers. In Proceedings of the Joint Conference on Artificial Intelligence (IJCAI), pages 4551–4558, Online.",
        "page": 11,
        "parent_chapter": 117,
        "index": 161,
        "outline": [
          69.0,
          400.5,
          291.0,
          478.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Preslav Nakov, Giovanni Da San Martino, Tamer Elsayed, Alberto Barrón-Cedeño, Rubén Míguez, Shaden Shaar, Firoj Alam, Fatima Haouari, Maram Hasanain, Nikolay Babulkov, Alex Nikolov, Gautam Kishore Shahi, Julia Maria Struß, and Thomas Mandl. 2021b. The CLEF-2021 CheckThat! lab on detecting check-worthy claims, previously factchecked claims, and fake news. In Proceedings of the 43rd European Conference on Information Retrieval (ECIR), pages 639–649, Lucca, Italy.",
        "page": 11,
        "parent_chapter": 117,
        "index": 162,
        "outline": [
          69.0,
          487.5,
          292.0,
          598.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. 2020. FANG: leveraging social context for fake news detection using graph representation. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM), pages 1165–1174.",
        "page": 11,
        "parent_chapter": 117,
        "index": 163,
        "outline": [
          69.0,
          608.0,
          291.0,
          675.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Yixin Nie, Haonan Chen, and Mohit Bansal. 2019. Combining fact extraction and verification with neural semantic matching networks. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI), pages 6859–6866, Honolulu, Hawaii, USA.",
        "page": 11,
        "parent_chapter": 117,
        "index": 164,
        "outline": [
          70.0,
          685.0,
          292.0,
          739.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial",
        "page": 11,
        "parent_chapter": 117,
        "index": 165,
        "outline": [
          70.5,
          751.5,
          291.0,
          772.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 4885–4901, Online.",
        "page": 11,
        "parent_chapter": 117,
        "index": 166,
        "outline": [
          315.5,
          73.5,
          525.0,
          116.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155.",
        "page": 11,
        "parent_chapter": 117,
        "index": 167,
        "outline": [
          304.0,
          126.0,
          526.0,
          214.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Liangming Pan, Wenhu Chen, Wenhan Xiong, MinYen Kan, and William Yang Wang. 2021. Zero-shot fact verification by claim generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 476–483, Online.",
        "page": 11,
        "parent_chapter": 117,
        "index": 168,
        "outline": [
          304.0,
          223.0,
          526.0,
          301.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Alicia Parrish, William Huang, Omar Agha, Soo-Hwan Lee, Nikita Nangia, Alexia Warstadt, Karmanya Aggarwal, Emily Allaway, Tal Linzen, and Samuel R. Bowman. 2021. Does putting a linguist in the loop improve NLU data collection? In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4886–4901, Punta Cana, Dominican Republic.",
        "page": 11,
        "parent_chapter": 117,
        "index": 169,
        "outline": [
          305.0,
          309.0,
          526.0,
          398.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, and Gerhard Weikum. 2017. Where the truth lies: Explaining the credibility of emerging claims on the web and social media. In Proceedngs of the International World Wide Web Conference (WWW), pages 1003–1012.",
        "page": 11,
        "parent_chapter": 117,
        "index": 170,
        "outline": [
          305.0,
          407.0,
          526.0,
          474.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. CoRR, abs/2210.03350.",
        "page": 11,
        "parent_chapter": 117,
        "index": 171,
        "outline": [
          305.0,
          483.0,
          525.0,
          525.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.",
        "page": 11,
        "parent_chapter": 117,
        "index": 172,
        "outline": [
          305.0,
          536.5,
          526.0,
          590.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333–389.",
        "page": 11,
        "parent_chapter": 117,
        "index": 173,
        "outline": [
          305.5,
          601.0,
          526.0,
          643.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan. 2021. COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 2116–2129, Online.",
        "page": 11,
        "parent_chapter": 117,
        "index": 174,
        "outline": [
          304.0,
          653.5,
          527.0,
          741.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Aalok Sathe, Salar Ather, Tuan Manh Le, Nathan Perry, and Joonsuk Park. 2020. Automated fact-checking",
        "page": 11,
        "parent_chapter": 117,
        "index": 175,
        "outline": [
          304.0,
          751.5,
          525.0,
          773.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6992",
        "page": 11,
        "parent_chapter": 117,
        "index": 176,
        "outline": [
          286.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "of claims from Wikipedia. In Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC), pages 6874–6882, Marseille, France.",
        "page": 12,
        "parent_chapter": 117,
        "index": 177,
        "outline": [
          80.0,
          73.5,
          291.0,
          107.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761.",
        "page": 12,
        "parent_chapter": 117,
        "index": 178,
        "outline": [
          70.0,
          113.5,
          290.0,
          167.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 624–643, Online.",
        "page": 12,
        "parent_chapter": 117,
        "index": 179,
        "outline": [
          69.0,
          174.0,
          292.0,
          252.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Amir Soleimani, Christof Monz, and Marcel Worring. 2020. BERT for evidence retrieval and claim verification. In Advances in Information Retrieval (ECIR), volume 12036, pages 359–366.",
        "page": 12,
        "parent_chapter": 117,
        "index": 180,
        "outline": [
          70.0,
          259.0,
          290.0,
          302.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "James Thorne and Andreas Vlachos. 2018. Automated fact checking: Task formulations, methods and future directions. In Proceedings of the 27th International Conference on Computational Linguistics (COLING), pages 3346–3359, Santa Fe, New Mexico, USA.",
        "page": 12,
        "parent_chapter": 117,
        "index": 181,
        "outline": [
          70.0,
          310.0,
          290.0,
          365.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 809–819, New Orleans, Louisiana.",
        "page": 12,
        "parent_chapter": 117,
        "index": 182,
        "outline": [
          69.0,
          371.5,
          291.0,
          459.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems (NeurIPS), pages 5998–6008, Long Beach, California, USA.",
        "page": 12,
        "parent_chapter": 117,
        "index": 183,
        "outline": [
          69.0,
          466.0,
          291.0,
          544.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534–7550, Online.",
        "page": 12,
        "parent_chapter": 117,
        "index": 184,
        "outline": [
          69.0,
          551.0,
          292.0,
          616.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. 2022a. SciFact-open: Towards open-domain scientific claim verification. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4719–4734, Abu Dhabi, United Arab Emirates.",
        "page": 12,
        "parent_chapter": 117,
        "index": 185,
        "outline": [
          69.0,
          622.5,
          291.0,
          689.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "David Wadden, Kyle Lo, Lucy Wang, Arman Cohan, Iz Beltagy, and Hannaneh Hajishirzi. 2022b. MultiVerS: Improving scientific claim verification with weak supervision and full-document context. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 61–76, Seattle, Washington, USA.",
        "page": 12,
        "parent_chapter": 117,
        "index": 186,
        "outline": [
          70.0,
          696.0,
          291.0,
          770.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "William Yang Wang. 2017. “Liar, liar pants on fire”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), pages 422–426, Vancouver, Canada.",
        "page": 12,
        "parent_chapter": 117,
        "index": 187,
        "outline": [
          305.0,
          72.5,
          526.0,
          127.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2022. Selfconsistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171.",
        "page": 12,
        "parent_chapter": 117,
        "index": 188,
        "outline": [
          305.0,
          138.0,
          527.0,
          180.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903.",
        "page": 12,
        "parent_chapter": 117,
        "index": 189,
        "outline": [
          305.5,
          190.5,
          526.0,
          233.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT), pages 1112–1122, New Orleans, Louisiana, USA.",
        "page": 12,
        "parent_chapter": 117,
        "index": 190,
        "outline": [
          304.0,
          242.0,
          527.0,
          329.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, and Lucy Wang. 2022. Generating scientific claims for zero-shot scientific fact checking. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 2448–2460, Dublin, Ireland.",
        "page": 12,
        "parent_chapter": 117,
        "index": 191,
        "outline": [
          305.0,
          339.0,
          526.0,
          405.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Fan Yang, Shiva K. Pentyala, Sina Mohseni, Mengnan Du, Hao Yuan, Rhema Linder, Eric D. Ragan, Shuiwang Ji, and Xia (Ben) Hu. 2019. XFake: Explainable fake news detector with visualizations. In Proceedings of the The World Wide Web Conference (WWW), pages 3600–3604, San Francisco, California, USA.",
        "page": 12,
        "parent_chapter": 117,
        "index": 192,
        "outline": [
          305.0,
          414.0,
          526.0,
          488.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2369–2380, Brussels, Belgium.",
        "page": 12,
        "parent_chapter": 117,
        "index": 193,
        "outline": [
          304.0,
          498.5,
          526.0,
          577.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020. Reasoning over semantic-level graph for fact checking. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 6170–6180, Online.",
        "page": 12,
        "parent_chapter": 117,
        "index": 194,
        "outline": [
          305.0,
          585.5,
          526.0,
          650.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2019. GEAR: Graph-based evidence aggregating and reasoning for fact verification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 892–901, Florence, Italy.",
        "page": 12,
        "parent_chapter": 117,
        "index": 195,
        "outline": [
          304.0,
          659.0,
          526.0,
          737.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6993",
        "page": 12,
        "parent_chapter": 117,
        "index": 196,
        "outline": [
          285.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "A Implementation Details about the Baselines",
        "page": 13,
        "parent_chapter": -1,
        "index": 197,
        "outline": [
          69.0,
          71.5,
          263.0,
          96.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "In this section, we give the implementation details for the seven baselines we used in our work. Typical ways to perform few-shot fact-checking using large language models are fine-tuning and incontext learning. Thus, we categorize the baselines into three categories.",
        "page": 13,
        "parent_chapter": 197,
        "index": 198,
        "outline": [
          69.0,
          107.0,
          291.0,
          186.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "A.1 Pre-trained Models",
        "page": 13,
        "parent_chapter": 197,
        "index": 199,
        "outline": [
          70.0,
          196.0,
          188.0,
          205.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Pre-trained models use pretrained Transformers (Vaswani et al., 2017) such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) for factchecking. For few-shot learning, we fine-tune them using 20 randomly sampled training examples from HOVER or FEVEROUS. We ran the training 10 times with different random seeds and report the average performance on the validation set. We chose two models:",
        "page": 13,
        "parent_chapter": 199,
        "index": 200,
        "outline": [
          69.0,
          213.0,
          291.0,
          331.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "• BERT-FC (Soleimani et al., 2020): It uses BERT for claim verification. The claim and the evidence are concatenated ([CLS]claim [SEP] evidence) and used as input for a binary classification task to predict the veracity label of the claim. We use the bert-large-uncased (345M parameters) model provided in HuggingFace.2",
        "page": 13,
        "parent_chapter": 199,
        "index": 201,
        "outline": [
          80.5,
          340.0,
          292.0,
          448.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "• LisT5 (Jiang et al., 2021): This is a factchecking framework built with a pretrained sequence-to-sequence transformer, namely T5 (Raffel et al., 2020), as its backbone. We adopt the “listwise concatenation” proposed in the paper for label prediction, which concatenates all candidate evidence sentences into a single input and we train the t5-large model to directly classify the claim as Supported or Refuted. We use the original implementation of this model.3",
        "page": 13,
        "parent_chapter": 199,
        "index": 202,
        "outline": [
          80.5,
          455.0,
          292.0,
          603.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "A.2 FC/NLI Fine-Tuned Models",
        "page": 13,
        "parent_chapter": 197,
        "index": 203,
        "outline": [
          70.0,
          611.0,
          227.5,
          620.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "These models are pretrained Transformer models that have been specifically fine-tuned on singlehop fact-checking datasets (e.g., FEVER) or natural language inference (NLI) datasets. This additional training allows these models to excel at fact-checking simple claims, and thus they can generalize better to complex claims that require multihop reasoning during further few-shot fine-tuning.",
        "page": 13,
        "parent_chapter": 203,
        "index": 204,
        "outline": [
          68.0,
          627.0,
          292.0,
          736.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "In this category, we selected the following three fine-tuned models:",
        "page": 13,
        "parent_chapter": 203,
        "index": 206,
        "outline": [
          305.5,
          73.5,
          524.5,
          96.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "• RoBERTa-NLI (Nie et al., 2020) fine-tunes RoBERTa-large (Liu et al., 2019) on a combination of four well-known NLI datasets: SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), FEVER-NLI (Nie et al., 2019), ANLI (R1, R2, R3) (Nie et al., 2020). We used the public model checkpoint available at HuggingFace4and we further fine-tuned it with 20 random examples from HOVER/FEVEROUS.",
        "page": 13,
        "parent_chapter": 203,
        "index": 207,
        "outline": [
          317.5,
          108.0,
          527.0,
          240.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "• DeBERTaV3-NLI (He et al., 2021) finetunes the DeBERTaV3-large model on 885,242 NLI hypothesis–premise pairs from FEVER and on four NLI datasets: MNLI, ANLI, LingNLI (Parrish et al., 2021), and WANLI (Liu et al., 2022). This is the bestperforming NLI model on HuggingFace as of 06/06/2022.5",
        "page": 13,
        "parent_chapter": 203,
        "index": 208,
        "outline": [
          317.5,
          252.0,
          526.0,
          358.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "• MULTIVERS (Wadden et al., 2022b), formerly known as LongChecker, uses the LongFormer (Beltagy et al., 2020) for claim verification to address the long input evidence problem. We use a model checkpoint finetuned on FEVER.6",
        "page": 13,
        "parent_chapter": 203,
        "index": 209,
        "outline": [
          316.5,
          369.0,
          527.0,
          447.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "A.3 In-Context Learning Models",
        "page": 13,
        "parent_chapter": 197,
        "index": 210,
        "outline": [
          304.0,
          457.5,
          466.0,
          471.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "These models have recently shown strong few-shot learning ability in various NLP tasks. By prompting a large language model with a few in-context examples, the model can quickly learn a task from demonstrations. To make a fair comparison to our model, we choose two in-context learning baselines as follows.",
        "page": 13,
        "parent_chapter": 210,
        "index": 211,
        "outline": [
          304.0,
          475.0,
          526.0,
          567.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "• Codex (Chen et al., 2021) is used in our model to generate reasoning programs. One straightforward baseline directly uses it for fact-checking. To this end, we prompt Codex (code-davinci-002) as follows: “<Evidence> Based on the above information, is it true that <Claim>?True or False? The answer is:”. We prefix the same 20 in-context examples for our model before the prompt as demonstrations.",
        "page": 13,
        "parent_chapter": 210,
        "index": 212,
        "outline": [
          315.5,
          577.5,
          527.0,
          714.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6994",
        "page": 13,
        "parent_chapter": 210,
        "index": 214,
        "outline": [
          286.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "• FLAN-T5 (Chung et al., 2022) is an improved version of T5, which is fine-tuned on 1.8K tasks phrased as instructions, with and without exemplars, i.e., zero-shot and few-shot. The model has shown strong performance in various in-context few-shot learning NLP tasks, such as reasoning, and question-answering. We prompt the model with the same format as we used in Section 3.4: “<Evidence> Q:<Claim> Is it true that <Claim>? True or False? The answer is:”, prefixing with the same 20 in-context examples. We also use the same model size (FLAN-T5-XXL 3B) with our model for fair comparison.",
        "page": 14,
        "parent_chapter": 210,
        "index": 215,
        "outline": [
          80.5,
          70.5,
          293.0,
          263.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "B Examples of Generated Reasoning Programs",
        "page": 14,
        "parent_chapter": -1,
        "index": 216,
        "outline": [
          69.0,
          273.0,
          267.5,
          298.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Figure 7 shows six examples of generated reasoning programs by PROGRAMFC that cover diverse reasoning chains.",
        "page": 14,
        "parent_chapter": 216,
        "index": 217,
        "outline": [
          69.0,
          309.0,
          291.0,
          347.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "C Error Analysis for Reasoning Programs",
        "page": 14,
        "parent_chapter": -1,
        "index": 218,
        "outline": [
          70.0,
          359.0,
          242.0,
          385.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Figure 8 shows five examples of erroneous cases where the generated reasoning programs are incorrect. We provide explanations for each of the error cases below:",
        "page": 14,
        "parent_chapter": 218,
        "index": 219,
        "outline": [
          69.0,
          397.0,
          291.0,
          447.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Example 1 It generates a wrong logical reasoning operator for the final step. The correct logic should be “not (fact_1 and fact_2)” instead of “fact_1 and fact_2”.",
        "page": 14,
        "parent_chapter": 218,
        "index": 220,
        "outline": [
          69.0,
          457.5,
          291.0,
          508.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Example 2 It fails to perform co-reference resolution for the arguments in the third and the fourth reasoning steps. “This album” should be replaced with “The bluegrass” to make the sub-task contextindependent. “This musical” should be replaced with the variable “answer_1” from the first step.",
        "page": 14,
        "parent_chapter": 218,
        "index": 221,
        "outline": [
          69.0,
          520.0,
          291.0,
          600.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Example 3 It fails to create a meaningful problem decomposition for the claim. It generates a trivial program that simply repeats the original claim.",
        "page": 14,
        "parent_chapter": 218,
        "index": 222,
        "outline": [
          69.0,
          609.0,
          292.0,
          649.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Example 4 It fails to generate a fine-grained reasoning structure for the input claim. It also generates a trivial program that simply separates the claim into sentences.",
        "page": 14,
        "parent_chapter": 218,
        "index": 223,
        "outline": [
          69.0,
          658.0,
          292.0,
          708.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Example 5 It generates a redundant reasoning step “Question(\"When was the musician born?\")”, which does not add any new information to the reasoning chain.",
        "page": 14,
        "parent_chapter": 218,
        "index": 224,
        "outline": [
          69.0,
          720.5,
          291.0,
          772.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "D Program Generation Prompts",
        "page": 14,
        "parent_chapter": -1,
        "index": 225,
        "outline": [
          305.0,
          71.5,
          480.0,
          84.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Our manually written prompts for the HOVER and the FEVEROUS-S datasets are given in Listings 1 and 2, respectively.",
        "page": 14,
        "parent_chapter": 225,
        "index": 226,
        "outline": [
          305.0,
          94.5,
          526.0,
          131.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "E Prompts for Closed-Book Fact-Checking",
        "page": 14,
        "parent_chapter": -1,
        "index": 227,
        "outline": [
          305.5,
          144.0,
          456.0,
          170.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Below we show the templates for the four prompting methods used for InstructGPT for the closedbook fact-checking setting in Section 4.4.",
        "page": 14,
        "parent_chapter": 227,
        "index": 228,
        "outline": [
          304.0,
          178.5,
          527.0,
          218.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Direct Prompting",
        "page": 14,
        "parent_chapter": 227,
        "index": 229,
        "outline": [
          315.5,
          223.0,
          399.0,
          234.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "# Answer the following true / false questions :\nIs it true that The woman the story behind Girl Crazy\nis credited to is older than Ted Kotcheff ?\nThe answer is: False\n(· · · more in-context examples here · · ·)\nIs it true that <input_claim>?\nThe answer is:",
        "page": 14,
        "parent_chapter": 227,
        "index": 230,
        "outline": [
          302.0,
          238.5,
          527.0,
          323.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "ZS-CoT Prompting",
        "page": 14,
        "parent_chapter": -1,
        "index": 231,
        "outline": [
          314.5,
          332.5,
          407.0,
          343.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "# Answer the following true / false question :\nIs it true that <input_claim>? True or False ?\nLet us think step - by - step . The answer is:",
        "page": 14,
        "parent_chapter": 231,
        "index": 232,
        "outline": [
          304.0,
          349.0,
          526.0,
          385.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "CoT Prompting",
        "page": 14,
        "parent_chapter": -1,
        "index": 233,
        "outline": [
          315.5,
          394.0,
          391.0,
          405.0
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "# Answer the following true / false questions :\nIs it true that The woman the story behind Girl Crazy\nis credited to is older than Ted Kotcheff ?\nLet 's think step by step .\nGirl Crazy 's story is credited to Hampton Del Ruth .\nHampton Del Ruth was born on September 7 , 1879.\nTed Kotcheff was born on April 7 , 1931.\nTherefore , the answer is: False .\n(· · · more in-context examples here · · ·)\nIs it true that <input_claim>?\nLet 's think step by step .",
        "page": 14,
        "parent_chapter": 233,
        "index": 234,
        "outline": [
          302.0,
          410.5,
          528.0,
          527.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Self-Ask Prompting",
        "page": 14,
        "parent_chapter": -1,
        "index": 235,
        "outline": [
          314.5,
          535.5,
          409.0,
          546.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "# Answer the following true / false questions :\nIs it true that The woman the story behind Girl Crazy\nis credited to is older than Ted Kotcheff ?\nQ : The story behind Girl Crazy is credited to whom ?\nA : Hampton Del Ruth\nQ : Is Hampton Del Ruth older than Ted Kotcheff ?\nA : No\nSo the final answer is: False .\n(· · · more in-context examples here · · ·)\nIs it true that <input_claim>?",
        "page": 14,
        "parent_chapter": 235,
        "index": 236,
        "outline": [
          302.0,
          551.0,
          528.0,
          661.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6995",
        "page": 14,
        "parent_chapter": 235,
        "index": 237,
        "outline": [
          285.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Claim:",
        "page": 15,
        "parent_chapter": 235,
        "index": 238,
        "outline": [
          90.2,
          133.2,
          113.7,
          139.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Tritonia and Phyteuma are both names for a plant genus.",
        "page": 15,
        "parent_chapter": 235,
        "index": 239,
        "outline": [
          89.8,
          142.1,
          322.7,
          149.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Predicted Program:",
        "page": 15,
        "parent_chapter": 235,
        "index": 240,
        "outline": [
          90.2,
          161.1,
          164.9,
          168.3
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_1 = Verify(\"Tritonia is a name for a plant genus.\")",
        "page": 15,
        "parent_chapter": 235,
        "index": 241,
        "outline": [
          90.4,
          169.6,
          327.2,
          176.9
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_2 = Verify(\"Phyteuma is a name for a plant genus.\")",
        "page": 15,
        "parent_chapter": 235,
        "index": 242,
        "outline": [
          90.4,
          178.2,
          327.2,
          185.4
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "label = Predict(fact_1 and fact_2)",
        "page": 15,
        "parent_chapter": 235,
        "index": 243,
        "outline": [
          90.5,
          186.7,
          233.3,
          193.7
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Claim:",
        "page": 15,
        "parent_chapter": 235,
        "index": 244,
        "outline": [
          89.0,
          203.0,
          114.5,
          210.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The country that Fujairah College is located in had a 2013 population of 9.2 million until it was  hit by the plague in 1483 when the population was halved.",
        "page": 15,
        "parent_chapter": 235,
        "index": 245,
        "outline": [
          88.0,
          211.0,
          505.0,
          228.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The coun hit by t",
        "page": 15,
        "parent_chapter": 235,
        "index": 246,
        "outline": [
          90.5,
          214.0,
          124.5,
          226.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Predicted Program:",
        "page": 15,
        "parent_chapter": 235,
        "index": 247,
        "outline": [
          90.2,
          240.5,
          164.9,
          247.7
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer_1 = Question(\"Which country is Fujairah College located in?\")",
        "page": 15,
        "parent_chapter": 235,
        "index": 248,
        "outline": [
          90.1,
          249.0,
          378.4,
          256.3
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_1 = Verify(\"{answer_1} had a 2013 population of 9.2 million.\")",
        "page": 15,
        "parent_chapter": 235,
        "index": 249,
        "outline": [
          90.4,
          257.5,
          374.1,
          264.8
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_2 = Verify(\"{answer_1} was hit by the plague in 1483.\")",
        "page": 15,
        "parent_chapter": 235,
        "index": 250,
        "outline": [
          90.4,
          266.0,
          344.3,
          273.4
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_3 = Verify(\"The population of {answer_1} was halved in 1483.\")",
        "page": 15,
        "parent_chapter": 235,
        "index": 251,
        "outline": [
          90.4,
          274.6,
          374.1,
          281.9
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "label = Predict(fact_1 and fact_2 and fact_3)",
        "page": 15,
        "parent_chapter": 235,
        "index": 252,
        "outline": [
          90.5,
          283.2,
          280.3,
          290.2
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Claim:",
        "page": 15,
        "parent_chapter": -1,
        "index": 253,
        "outline": [
          90.2,
          301.8,
          113.7,
          307.7
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The first female board member for the Liberal Party, she was born in Vestfold county in Norway.",
        "page": 15,
        "parent_chapter": 253,
        "index": 254,
        "outline": [
          88.0,
          309.0,
          494.5,
          318.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Predicted Program:",
        "page": 15,
        "parent_chapter": -1,
        "index": 255,
        "outline": [
          90.2,
          329.6,
          164.9,
          336.9
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer_1 = Question(\"Who was the first female board member for the Liberal Party?\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 256,
        "outline": [
          90.1,
          338.2,
          442.4,
          345.4
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_1 = Verify(\"{answer_1} was born in Vestfold county in Norway.\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 257,
        "outline": [
          90.4,
          346.6,
          378.4,
          353.9
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "label = Predict(fact_1)",
        "page": 15,
        "parent_chapter": 255,
        "index": 258,
        "outline": [
          90.5,
          355.2,
          186.4,
          362.2
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Claim:",
        "page": 15,
        "parent_chapter": 255,
        "index": 259,
        "outline": [
          90.2,
          375.9,
          113.7,
          381.8
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The solicitor who won the show Back to Reality ahead of Maureen Rees and Craig Phillips is English. The solicitor that was a chair of Global Witness is also English.",
        "page": 15,
        "parent_chapter": 255,
        "index": 260,
        "outline": [
          89.0,
          382.5,
          479.0,
          402.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Program:",
        "page": 15,
        "parent_chapter": 255,
        "index": 261,
        "outline": [
          135.0,
          412.5,
          163.5,
          418.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer_1 = Question(\"Which solicitor won the show Back to Reality ahead of Maureen Rees and Craig Phillips?\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 262,
        "outline": [
          89.0,
          419.5,
          504.5,
          435.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer_2 = Question(\"Which solicitor was a chair of Global Witness?\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 263,
        "outline": [
          90.1,
          438.0,
          382.7,
          445.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_1 = Verify(\"{answer_1} is English.\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 264,
        "outline": [
          90.4,
          446.4,
          263.2,
          453.8
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_2 = Verify(\"{answer_2} is English.\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 265,
        "outline": [
          90.4,
          455.0,
          263.2,
          462.3
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "label = Predict(fact_1 and fact_2)",
        "page": 15,
        "parent_chapter": 255,
        "index": 266,
        "outline": [
          90.5,
          463.6,
          233.3,
          470.6
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Claim:",
        "page": 15,
        "parent_chapter": 255,
        "index": 267,
        "outline": [
          90.2,
          483.2,
          113.7,
          489.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Anthony Burgess addressed the novelist and essayist, the author of Grimus, in a lengthy love  letter. The author is of the same nationality as Raj Koothrappali.",
        "page": 15,
        "parent_chapter": 255,
        "index": 268,
        "outline": [
          89.0,
          490.5,
          483.5,
          507.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": " Program:",
        "page": 15,
        "parent_chapter": 255,
        "index": 269,
        "outline": [
          130.5,
          519.5,
          164.0,
          525.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer_1 = Question(\"Which novelist and essayist did Anthony Burgess address in a lengthy love letter?\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 270,
        "outline": [
          89.0,
          526.5,
          491.5,
          542.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_1 = Verify(\"{answer",
        "page": 15,
        "parent_chapter": 255,
        "index": 271,
        "outline": [
          90.3,
          545.0,
          191.8,
          552.3
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "is the author of Grimus.\"",
        "page": 15,
        "parent_chapter": 255,
        "index": 272,
        "outline": [
          209.5,
          544.5,
          315.5,
          551.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer",
        "page": 15,
        "parent_chapter": 255,
        "index": 273,
        "outline": [
          90.1,
          555.4,
          115.0,
          559.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_2 = Verify(\"{answer_1} is of the same nationality as {answer_2}.\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 274,
        "outline": [
          89.0,
          560.0,
          392.5,
          570.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "label = Predict(fact_1 and fact_2)",
        "page": 15,
        "parent_chapter": 255,
        "index": 275,
        "outline": [
          90.5,
          570.7,
          233.3,
          577.7
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Claim:",
        "page": 15,
        "parent_chapter": 255,
        "index": 276,
        "outline": [
          90.2,
          595.0,
          113.7,
          600.9
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "The critically acclaimed film, that Buddy Baker scored in 1975, is a Walt Disney film. It was produced first before the film that featured Bruce M. Fischer as Mr. Coogar.",
        "page": 15,
        "parent_chapter": 255,
        "index": 277,
        "outline": [
          88.0,
          602.0,
          487.0,
          621.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Predicted Program:",
        "page": 15,
        "parent_chapter": 255,
        "index": 278,
        "outline": [
          90.2,
          631.4,
          164.9,
          638.6
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer_1",
        "page": 15,
        "parent_chapter": 255,
        "index": 279,
        "outline": [
          90.1,
          640.5,
          123.3,
          647.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "?\"",
        "page": 15,
        "parent_chapter": 255,
        "index": 280,
        "outline": [
          441.0,
          640.5,
          447.0,
          646.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_1 = Verify(\"{answer_1} is a Walt Disney film.\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 281,
        "outline": [
          90.3,
          648.4,
          310.1,
          655.7
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "answer_2 = Question(\"Which film featured Bruce M. Fischer as Mr. Coogar?\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 282,
        "outline": [
          90.1,
          657.0,
          404.0,
          664.3
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "fact_2 = Verify(\"{answer_1} was produced first before {answer_2}.\")",
        "page": 15,
        "parent_chapter": 255,
        "index": 283,
        "outline": [
          90.3,
          665.5,
          374.1,
          672.8
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "label = Predict(fact_1 and fact_2)",
        "page": 15,
        "parent_chapter": 255,
        "index": 284,
        "outline": [
          90.5,
          674.1,
          233.3,
          681.1
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Figure 7: Examples of generated reasoning programs by PROGRAMFC.",
        "page": 15,
        "parent_chapter": 255,
        "index": 285,
        "outline": [
          153.5,
          700.5,
          441.0,
          711.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6996",
        "page": 15,
        "parent_chapter": 255,
        "index": 286,
        "outline": [
          285.5,
          781.0,
          310.0,
          792.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": " Baker score in 1975",
        "page": 15,
        "parent_chapter": 255,
        "index": 287,
        "outline": [
          353.5,
          640.0,
          439.0,
          645.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "figure",
        "text": "Semantic Error — Token: incorrect or missing arguments/variables  Example 1: Bitter Jester and The Future of Food are not both documentaries about food. Predicted Program:  fact_1 = Verify(\"Bitter Jester is a documentary about food.\") fact_2 = Verify(\"The Future of Food is a documentary about food.\") label = Predict(fact_1 and fact_2) label = Predict(not (fact_1 and fact_2))Example 2: The record producer that produced the bluegrass album was born on 22 June, 1944. This album  inspired a Tony award winning musical. This musical had a character that was originated by Carmen  Cusack. Predicted Program:  answer_1 = Question(\"Which record producer produced the bluegrass album?\")fact_1 = Verify(\"{answer_1} was born on 22 June, 1944.\") fact_2 = Verify(\"This album inspired a Tony award winning musical.\") fact_3 = Verify(\"This musical had a character that was originated by Carmen Cusack.\") label = Predict(fact_1 and fact_2 and fact_3) answer_1 = Question(\"Which record producer produced the bluegrass album?\")fact_1 = Verify(\"{answer_1} was born on 22 June, 1944.\") fact_2 = Verify(\"The bluegrass album inspired a Tony award winning musical.\") fact_3 = Verify(\"{answer_1} had a character that was originated by Carmen Cusack.\") label = Predict(fact_1 and fact_2 and fact_3) Semantic Error — Structure: incorrect program structure Example 3: Richard Strauss, German composer of Die Nacht, composed another well-established opera called Der  Rosenkavalier. Predicted Program:  fact_1 = Verify(\"Richard Strauss, German composer of Die Nacht, composed another well-established  opera called Der Rosenkavalier.\") label = Predict(fact_1) fact_1 = Verify(\"Richard Strauss is a German composer of Die Nacht.”) fact_2 = Verify(\"Richard Strauss composed a well-established opera called Der Rosenkavalier.\") label = Predict(fact_1 and fact_2) Example 4: The film Deanna Oliver produced in 1999 grossed $36.8 million domestically. The musical film  based on coach Herman Boone, did not. Predicted Program:  fact_1 = Verify(\"The film Deanna Oliver produced in 1999 grossed $36.8 million domestically.\") fact_2 = Verify(\"The musical film based on coach Herman Boone, did not.\") label = Predict(fact_1 and fact_2) answer_1 = Question(\"What film did Deanna Oliver produce in 1999?\") fact_1 = Verify(\"{answer_1} grossed $36.8 million domestically.\") answer_2 = Question(\"What musical film was based on coach Herman Boone?\")fact_2 = Verify(\"{answer_2} grossed less than $36.8 million domestically.\") label = Predict(fact_1 and fact_2) Semantic Error — Subtask: missing / redundant / incorrect sub-task calls Example 5: The musician, who founded Morningwood with Max Green, is older than Max Green. Predicted Program:  answer_1 = Question(\"Who founded Morningwood with Max Green?\")answer_2 = Question(\"When was Max Green born?\")answer_3 = Question(\"When was the musician born?\")fact_1 = Verify(\"{answer_3} is older than {answer_2}.\") {answer_1} is older than {answer_2}.label = Verify(fact_1)",
        "page": 16,
        "parent_chapter": 255,
        "index": 288,
        "outline": [
          80.0,
          95.0,
          515.0,
          709.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "Figure 8: Example error cases where the generated reasoning programs are incorrect. The incorrect segment(s) are marked in red, and the correct revisions are marked in green.",
        "page": 16,
        "parent_chapter": 255,
        "index": 289,
        "outline": [
          70.0,
          718.0,
          524.5,
          739.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6997",
        "page": 16,
        "parent_chapter": 255,
        "index": 290,
        "outline": [
          285.5,
          781.0,
          310.0,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "''' Generate a python - like program that describes the reasoning steps required to\nverify the claim step -by - step . You can call three functions in the program : 1.\nQuestion () to answer a question ; 2. Verify () to verify a simple claim ; 3.\nPredict () to predict the veracity label . '''\n# The claim is that Howard University Hospital and Providence Hospital are both\nlocated in Washington , D.C.\ndef program () :\nfact_1 = Verify (\" Howard University Hospital is located in Washington , D.C.\")\nfact_2 = Verify (\" Providence Hospital is located in Washington , D.C.\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that WWE Super Tuesday took place at an arena that currently goes by\nthe name TD Garden .\ndef program () :\nanswer_1 = Question (\" Which arena the WWE Super Tuesday took place ?\")\nfact_1 = Verify ( f\"{ answer_1 } currently goes by the name TD Garden .\")\nlabel = Predict ( fact_1 )\n# The claim is that Talking Heads , an American rock band that was \"one of the most\ncritically acclaimed bands of the 80's\" is featured in KSPN 's AAA format .\ndef program () :\nfact_1 = Verify (\" Talking Heads is an American rock band that was 'one of the\nmost critically acclaimed bands of the 80's '.\")\nfact_2 = Verify (\" Talking Heads is featured in KSPN 's AAA format .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that An IndyCar race driver drove a Formula 1 car designed by Peter\nMcCool during the 2007 Formula One season .\ndef program () :\nanswer_1 = Question (\" Which Formula 1 car was designed by Peter McCool during the\n2007 Formula One season ?\")\nfact_1 = Verify ( f\"An IndyCar race driver drove the car { answer_1 }.\")\nlabel = Predict ( fact_1 )\n# The claim is that Gina Bramhill was born in a village . The 2011 population of the\narea that includes this village was 167 ,446.\ndef program () :\nanswer_1 = Question (\" Which village was Gina Bramhill born in?\")\nfact_1 = Verify ( f\" The 2011 population of the area that includes { answer_1 } was\n167 ,446. \")\nlabel = Predict ( fact_1 )\n# The claim is that Don Ashley Turlington graduated from Saint Joseph 's College , a\nprivate Catholic liberal arts college in Standish .\ndef program () :\nfact_1 = Verify (\" Saint Joseph 's College is a private Catholic liberal arts\ncollege is located in Standish .\")\nfact_2 = Verify ( f\" Don Ashley Turlington graduated from Saint Joseph 's College .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Gael and Fitness are not published in the same country .\ndef program () :\nanswer_1 = Question (\" Which country was Gael published in?\")\nanswer_2 = Question (\" Which country was Fitness published in?\")\nfact_1 = Verify ( f\"{ answer_1 } and { answer_2 } are not the same country .\")\nlabel = Predict ( fact_1 )\n# The claim is that Blackstar is the name of the album released by David Bowie that\nwas recorded in secret .\ndef program () :\nfact_1 = Verify (\" David Bowie released an album called Blackstar .\")\nfact_2 = Verify (\" David Bowie recorded an album in secret .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that In the 2004 Hockey film produced by a former major league\nbaseball pitcher Kurt Russell played the USA coach .\ndef program () :\nanswer_1 = Question (\" Which 2004 Hockey film was produced a former major league",
        "page": 17,
        "parent_chapter": 255,
        "index": 291,
        "outline": [
          61.5,
          72.5,
          532.5,
          774.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6998",
        "page": 17,
        "parent_chapter": 255,
        "index": 292,
        "outline": [
          285.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "baseball pitcher ?\")\nfact_1 = Verify (\" Kurt Russell played the USA coach in the film { answer_1 }.\")\nlabel = Predict ( fact_1 )\n# The claim is that Along with the New York Islanders and the New York Rangers , the\nNew Jersey Devils NFL franchise is popular in the New York metropolitan area .\ndef program () :\nfact_1 = Verify (\"The New York Islanders and the New York Rangers are popular in\nthe New York metropolitan area .\")\nfact_2 = Verify (\"The New Jersey Devils NFL franchise is popular in the New York\nmetropolitan area .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Jack McFarland is the best known role of the host of the 64 th\nAnnual Tony Awards .\ndef program () :\nanswer_1 = Question (\" Who is the host of the 64 th Annual Tony Awards ?\")\nfact_1 = Verify ( f \\\" Jack McFarland is the best known role of { answer_1 }.\")\nlabel = Predict ( fact_1 )\n# The claim is that The song recorded by Fergie that was produced by Polow da Don\nand was followed by Life Goes On was M.I.L.F.$.\ndef program () :\nfact_1 = Verify (\"M.I.L.F.$ was recorded by Fergie that was produced by Polow da\nDon.\")\nfact_2 = Verify (\"M.I.L.F.$ was was followed by Life Goes On.\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Eatza Pizza and Your Pie were not founded in the same state .\ndef program () :\nanswer_1 = Question (\" Which state was Eatza Pizza founded in?\")\nanswer_2 = Question (\" Which state was Your Pie founded in?\")\nfact_1 = Verify ( f\"{ answer_1 } and { answer_2 } are not the same state .\")\nlabel = Predict ( fact_1 )\n# The claim is that Gregg Rolie and Rob Tyner , are not a keyboardist .\ndef program () :\nfact_1 = Verify (\" Gregg Rolie is not a keyboardist .\")\nfact_2 = Verify (\"Rob Tyner is not a keyboardist .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Maria Esther Andion Bueno , not Jimmy Connors , is the player that\nis from Brazil .\ndef program () :\nfact_1 = Verify (\" Maria Esther Andion Bueno is from Brazil .\")\nfact_2 = Verify (\" Jimmy Connors is not from Brazil .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Vladimir Igorevich Arnold died after Georg Cantor .\ndef program () :\nanswer_1 = Question (\" When did Vladimir Igorevich Arnold die ?\")\nanswer_2 = Question (\" When did Georg Cantor die?\")\nfact_1 = Verify ( f\"{ answer_1 } is after { answer_2 }.\")\nlabel = Predict ( fact_1 )\n# The claim is that Barton Mine was halted by a natural disaster not Camlaren Mine .\ndef program () :\nfact_1 = Verify (\" Barton Mine was halted by a natural disaster .\")\nfact_2 = Verify (\" Camlaren Mine was not halted by a natural disaster .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that John O'Hara and Rabindranath Tagore are not the same nationality\n.\ndef program () :\nanswer_1 = Question (\" What is the nationality of John O'Hara ?\")\nanswer_2 = Question (\" What is the nationality of Rabindranath Tagore ?\")\nfact_1 = Verify ( f\"{ answer_1 } and { answer_2 } are not the same nationality .\")\nlabel = Predict ( fact_1 )",
        "page": 18,
        "parent_chapter": 255,
        "index": 293,
        "outline": [
          61.5,
          69.0,
          532.5,
          776.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "6999",
        "page": 18,
        "parent_chapter": 255,
        "index": 294,
        "outline": [
          285.5,
          781.0,
          310.0,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "# The claim is that Thomas Loren Friedman has won more Pulitzer Prizes than Colson\nWhitehead .\ndef program () :\nanswer_1 = Question (\" How many Pulitzer Prizes has Thomas Loren Friedman won ?\")\nanswer_2 = Question (\" How many Pulitzer Prizes has Colson Whitehead won?\")\nfact_1 = Verify ( f\"{ answer_1 } is more than { answer_2 }.\")\nlabel = Predict ( fact_1 )\n# The claim is that The model of car Trevor Bayne drives was introduced for model\nyear 2006. The Rookie of The Year in the 1997 CART season drives it in the\nNASCAR Sprint Cup .\ndef program () :\nanswer_1 = Question (\" Which model of car is drived by Trevor Bayne ?\")\nfact_1 = Verify ( f\"{ answer_1 } was introduced for model year 2006. \")\nanswer_2 = Question (\" Who is the Rookie of The Year in the 1997 CART season ?\")\nfact_2 = Verify ( f\"{ answer_2 } drives the model of car Trevor Bayne drives in the\nNASCAR Sprint Cup .\")\nlabel = predict ( fact_1 and fact_2 )\n# The claim is that <input_claim>\ndef program () :",
        "page": 19,
        "parent_chapter": 255,
        "index": 295,
        "outline": [
          65.5,
          72.5,
          528.0,
          288.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Listing 1: The prompt used for Program Generation for HOVER.",
        "page": 19,
        "parent_chapter": 255,
        "index": 296,
        "outline": [
          166.0,
          292.5,
          426.5,
          302.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "7000",
        "page": 19,
        "parent_chapter": 255,
        "index": 297,
        "outline": [
          285.5,
          781.0,
          310.0,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "''' Generate a python - like program that describes the reasoning steps required to\nverify the claim step -by - step . You can call three functions in the program : 1.\nQuestion () to answer a question ; 2. Verify () to verify a simple claim ; 3.\nPredict () to predict the veracity label . '''\n# The claim is that In 1959 , former Chilean boxer Alfredo Cornejo Cuevas ( born June\n6 , 1933) won the gold medal in the welterweight division at the Pan American\nGames ( held in Chicago , United States , from August 27 to September 7) in Chicago\n, United States , and the world amateur welterweight title in Mexico City .\ndef program () :\nfact_1 = Verify (\" Alfredo Cornejo Cuevas was born in June 6 , 1933. \")\nfact_2 = Verify (\" Alfredo Cornejo Cuevas won the gold medal in the welterweight\ndivision at the Pan American Games in 1959. \")\nfact_3 = Verify (\"The Pan American Games in 1959 was held in Chicago , United\nStates , from August 27 to September 7.\")\nfact_4 = Verify (\" Alfredo Cornejo Cuevas won the world amateur welterweight title\nin Mexico City .\")\nlabel = Predict ( fact_1 and fact_2 and fact_3 and fact_4 )\n# The claim is that The Footwork FA12 , which was intended to start the season ,\nfinally debuted at the San Marino Grand Prix , a Formula One motor race held at\nImola on 28 April 1991.\ndef program () :\nfact_1 = Verify (\"The Footwork FA12 , which was intended to start the season .\")\nfact_2 = Verify (\"The Footwork FA12 finally debuted at the San Marino Grand Prix .\n\")\nfact_3 = Verify (\"The San Marino Grand Prix was a Formula One motor race held at\nImola on 28 April 1991. \")\nlabel = Predict ( fact_1 and fact_2 and fact_3 )\n# The claim is that SkyHigh Mount Dandenong ( formerly Mount Dandenong Observatory )\nis a restaurant located on top of Mount Dandenong , Victoria , Australia .\ndef program () :\nfact_1 = Verify (\" SkyHigh Mount Dandenong is a restaurant located on top of Mount\nDandenong , Victoria , Australia .\")\nfact_2 = Verify (\" SkyHigh Mount Dandenong is formerly known as Mount Dandenong\nObservatory .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Before the first Europeans arrived or copra companies leased it ,\nMaupihaa was home to Inca 's in ancient times .\ndef program () :\nfact_1 = Verify (\" Maupihaa was home to Inca 's in ancient times .\")\nfact_2 = Verify (\" Maupihaa was home to Inca 's before the first Europeans arrived\nor copra companies leased it.\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Shulin , a 33.1288 km (12.7911 sq mi) land located in New Taipei\nCity , China , a country in East Asia , has a total population of 183 ,946 in\nDecember 2018.\ndef program () :\nfact_1 = Verify (\" Shulin is a 33.1288 km (12.7911 sq mi) land located in New\nTaipei City , China .\")\nfact_2 = Verify (\" Shulin has a total population of 183 ,946 in December 2018. \")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Sumo wrestler Toyozakura Toshiaki committed match - fixing , ending\nhis career in 2011 that started in 1989.\ndef program () :\nfact_1 = Verify (\" Toyozakura Toshiaki ended his career in 2011 that started in\n1989. \")\nfact_2 = Verify (\" Toyozakura Toshiaki is a Sumo wrestler .\")\nfact_3 = Verify (\" Toyozakura Toshiaki committed match - fixing .\")\nlabel = Predict ( fact_1 and fact_2 and fact_3 )\n# The claim is that In 1959 , former Chilean boxer Alfredo Cornejo Cuevas ( born June\n6 , 1933) won the gold medal in the welterweight division at the Pan American\nGames ( held in Chicago , United States , from August 27 to September 7) in Chicago",
        "page": 20,
        "parent_chapter": 255,
        "index": 298,
        "outline": [
          60.0,
          70.0,
          534.0,
          774.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "7001",
        "page": 20,
        "parent_chapter": 255,
        "index": 299,
        "outline": [
          285.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": ", United States , and the world amateur welterweight title in Mexico City .\ndef program () :\nfact_1 = Verify (\" Alfredo Cornejo Cuevas is a former Chilean boxer .\")\nfact_2 = Verify (\" Alfredo Cornejo won the gold medal in the welterweight division\nat the Pan American Games .\")\nfact_3 = Verify (\"The Pan American Games was held in Chicago , United States , from\nAugust 27 to September 7.\")\nfact_4 = Verify (\" Alfredo Cornejo won the world amateur welterweight title in\nMexico City .\")\nlabel = Predict ( fact_1 and fact_2 and fact_3 and fact_4 )\n# The claim is that Adductor hiatus is associated with nine structures , seven of\nwhich enter and leave through hiatus .\ndef program () :\nfact_1 = Verify (\" Adductor hiatus is associated with nine structures .\")\nfact_2 = Verify (\" Seven of the nine structures associated with Adductor hiatus\nenter and leave through hiatus .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Ifor Bowen Lloyd was educated at Winchester (an independent\nboarding school for boys in the British public school tradition ) and Exeter\nCollege , Oxford where he was a member of the Library Committee of the Oxford\nUnion Society , as well as , received a BA in Modern History in 1924.\ndef program () :\nfact_1 = Verify (\" Ifor Bowen Lloyd was educated at Winchester and Exeter College ,\nOxford .\")\nfact_2 = Verify (\" Winchester is an independent boarding school for boys in the\nBritish public school tradition .\")\nfact_3 = Verify (\" While at Oxford , Ifor Bowen Lloyd was a member of the Library\nCommittee of the Oxford Union Society .\")\nfact_4 = Verify (\" Ifor Bowen Lloyd received a BA in Modern History in 1924 at\nOxford .\")\nlabel = Predict ( fact_1 and fact_2 and fact_3 and fact_4 )\n# The claim is that In the 2001 Stanley Cup playoffs Eastern Conference Semifinals\nDevils ' Elias scored and Maple Leafs ' left Devils player Scott Neidermayer hurt .\ndef program () :\nfact_1 = Verify (\"In the 2001 Stanley Cup playoffs Eastern Conference Semifinals\nDevils ' Elias scored .\")\nfact_2 = Verify (\" Maple Leafs ' left Devils player Scott Neidermayer hurt .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Teldenia helena is a moth first described in 1967 by Wilkinson .\ndef program () :\nfact_1 = Verify (\" Teldenia helena is a moth .\")\nfact_2 = Verify (\" Teldenia helena was first described by Wilkinson in 1967. \")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that Born December 30 , 1974 , William Frick was a dark horse candidate\nin the Maryland House of Delegates appointment process .\ndef program () :\nfact_1 = Verify (\" William Frick was born in December 30 , 1974. \")\nfact_2 = Verify (\" William Frick was a dark horse candidate in the Maryland House\nof Delegates appointment process .\")\nlabel = Predict ( fact_1 and fact_2 )\n# The claim is that <input_claim>\ndef program () :",
        "page": 21,
        "parent_chapter": 255,
        "index": 300,
        "outline": [
          61.5,
          67.0,
          533.5,
          661.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Listing 2: The prompt used for Program Generation for FEVEROUS-S.",
        "page": 21,
        "parent_chapter": 255,
        "index": 301,
        "outline": [
          153.5,
          660.0,
          441.0,
          670.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "7002",
        "page": 21,
        "parent_chapter": 255,
        "index": 302,
        "outline": [
          285.5,
          781.0,
          310.0,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "ACL 2023 Responsible NLP Checklist",
        "page": 22,
        "parent_chapter": -1,
        "index": 303,
        "outline": [
          80.0,
          73.5,
          260.5,
          84.5
        ],
        "is_chapter_title": true,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "A For every submission:",
        "page": 22,
        "parent_chapter": 303,
        "index": 304,
        "outline": [
          69.0,
          90.5,
          192.5,
          104.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ A1. Did you describe the limitations of your work?Line 587 - 620",
        "page": 22,
        "parent_chapter": 303,
        "index": 305,
        "outline": [
          78.0,
          105.0,
          318.5,
          132.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ A2. Did you discuss any potential risks of your work?Line 626 - 630",
        "page": 22,
        "parent_chapter": 303,
        "index": 306,
        "outline": [
          79.0,
          142.5,
          329.0,
          168.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ A3. Do the abstract and introduction summarize the paper’s main claims?Line 67 - 86",
        "page": 22,
        "parent_chapter": 303,
        "index": 307,
        "outline": [
          79.0,
          178.5,
          414.5,
          205.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✗ A4. Have you used AI writing assistants when working on this paper?Left blank.",
        "page": 22,
        "parent_chapter": 303,
        "index": 308,
        "outline": [
          78.0,
          215.0,
          400.0,
          244.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "B \u0003✓ Did you use or create scientific artifacts?",
        "page": 22,
        "parent_chapter": 303,
        "index": 309,
        "outline": [
          69.0,
          252.0,
          290.0,
          266.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Line 327 - 352",
        "page": 22,
        "parent_chapter": 303,
        "index": 310,
        "outline": [
          79.0,
          270.0,
          147.0,
          283.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ B1. Did you cite the creators of artifacts you used?Line 328 - 329",
        "page": 22,
        "parent_chapter": 303,
        "index": 311,
        "outline": [
          78.0,
          290.0,
          317.5,
          320.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?Not applicable. The datasets used in this paper are publicly available datasets from existing works.",
        "page": 22,
        "parent_chapter": 303,
        "index": 312,
        "outline": [
          77.0,
          328.0,
          526.0,
          357.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?",
        "page": 22,
        "parent_chapter": 303,
        "index": 313,
        "outline": [
          80.5,
          363.5,
          526.0,
          422.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Line 327 - 344",
        "page": 22,
        "parent_chapter": 303,
        "index": 314,
        "outline": [
          90.0,
          419.5,
          156.0,
          430.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?Not applicable. Left blank.",
        "page": 22,
        "parent_chapter": 303,
        "index": 315,
        "outline": [
          77.0,
          442.5,
          526.0,
          496.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?Not applicable. Left blank.",
        "page": 22,
        "parent_chapter": 303,
        "index": 316,
        "outline": [
          78.0,
          504.0,
          526.0,
          545.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.",
        "page": 22,
        "parent_chapter": 303,
        "index": 317,
        "outline": [
          80.0,
          553.0,
          526.0,
          624.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Line 327 - 344",
        "page": 22,
        "parent_chapter": 303,
        "index": 318,
        "outline": [
          91.5,
          623.5,
          156.0,
          632.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "C \u0003✓ Did you run computational experiments?",
        "page": 22,
        "parent_chapter": 303,
        "index": 319,
        "outline": [
          70.0,
          642.5,
          293.0,
          657.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Section 4",
        "page": 22,
        "parent_chapter": 303,
        "index": 320,
        "outline": [
          80.5,
          663.5,
          123.5,
          673.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?Figure 4; Appendix A",
        "page": 22,
        "parent_chapter": 303,
        "index": 321,
        "outline": [
          76.0,
          680.5,
          527.0,
          725.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "7003",
        "page": 22,
        "parent_chapter": 303,
        "index": 323,
        "outline": [
          286.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?Appendix A",
        "page": 23,
        "parent_chapter": 303,
        "index": 324,
        "outline": [
          78.0,
          69.0,
          526.0,
          110.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003✓ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. orj ust a single run?Appendix A",
        "page": 23,
        "parent_chapter": 303,
        "index": 325,
        "outline": [
          78.0,
          118.0,
          528.0,
          180.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?",
        "page": 23,
        "parent_chapter": 303,
        "index": 326,
        "outline": [
          78.0,
          185.0,
          525.0,
          225.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Not applicable. Left blank.",
        "page": 23,
        "parent_chapter": 303,
        "index": 327,
        "outline": [
          90.0,
          225.5,
          208.5,
          238.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "D \u0003✗ Did you use human annotators (e.g., crowdworkers) or research with human participants?",
        "page": 23,
        "parent_chapter": 303,
        "index": 328,
        "outline": [
          70.0,
          249.0,
          520.5,
          260.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Left blank.",
        "page": 23,
        "parent_chapter": 303,
        "index": 329,
        "outline": [
          80.5,
          266.5,
          127.0,
          277.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?",
        "page": 23,
        "parent_chapter": 303,
        "index": 330,
        "outline": [
          79.0,
          288.0,
          525.0,
          314.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Not applicable. Left blank.",
        "page": 23,
        "parent_chapter": 303,
        "index": 331,
        "outline": [
          89.0,
          315.5,
          209.5,
          326.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants’ demographic (e.g., country of residence)?",
        "page": 23,
        "parent_chapter": 303,
        "index": 332,
        "outline": [
          79.0,
          337.0,
          525.0,
          378.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "Not applicable. Left blank.",
        "page": 23,
        "parent_chapter": 303,
        "index": 333,
        "outline": [
          90.0,
          378.0,
          209.5,
          390.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 D3. Did you discuss whether and how consent was obtained from people whose data you’re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?",
        "page": 23,
        "parent_chapter": 303,
        "index": 334,
        "outline": [
          78.0,
          400.5,
          525.0,
          447.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?Not applicable. Left blank.",
        "page": 23,
        "parent_chapter": 303,
        "index": 335,
        "outline": [
          78.0,
          463.0,
          520.5,
          489.5
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "paragraph",
        "text": "\u0003 D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?Not applicable. Left blank.",
        "page": 23,
        "parent_chapter": 303,
        "index": 336,
        "outline": [
          77.0,
          499.5,
          526.0,
          541.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0,
        "continued": false,
        "page_merged_paragraph": null
      },
      {
        "type": "page_footer",
        "text": "7004",
        "page": 23,
        "parent_chapter": -1,
        "index": 337,
        "outline": [
          286.5,
          781.0,
          309.5,
          791.0
        ],
        "is_chapter_title": false,
        "rotation": 0.0
      }
    ]
  }
}